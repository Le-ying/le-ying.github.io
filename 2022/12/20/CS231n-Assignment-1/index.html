<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>CS231n Assignment 1 (Updating) | Kung's Blog</title><meta name="author" content="龚泽颖"><meta name="copyright" content="龚泽颖"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="CS231n Assignment 1 作业1分为五个部分：KNN、SVM、Softmax classifier、2层神经网络、Higher Level Representations: Image Features. 建议作业完成顺序：  k近邻分类：knn.ipynb &amp; k_nearest_neighbor.py svm线性分类：svm.ipynb &amp; linea">
<meta property="og:type" content="article">
<meta property="og:title" content="CS231n Assignment 1 (Updating)">
<meta property="og:url" content="https://serika-onoe.github.io/2022/12/20/CS231n-Assignment-1/index.html">
<meta property="og:site_name" content="Kung&#39;s Blog">
<meta property="og:description" content="CS231n Assignment 1 作业1分为五个部分：KNN、SVM、Softmax classifier、2层神经网络、Higher Level Representations: Image Features. 建议作业完成顺序：  k近邻分类：knn.ipynb &amp; k_nearest_neighbor.py svm线性分类：svm.ipynb &amp; linea">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/cs231n_assignment1.png">
<meta property="article:published_time" content="2022-12-20T14:49:13.000Z">
<meta property="article:modified_time" content="2022-12-30T14:02:48.011Z">
<meta property="article:author" content="龚泽颖">
<meta property="article:tag" content="介绍">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="cs231n">
<meta property="article:tag" content="作业">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/cs231n_assignment1.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://serika-onoe.github.io/2022/12/20/CS231n-Assignment-1/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"W5J7GO3HT8","apiKey":"53b7d5d068605c78d1a20d7f3671629a","indexName":"test_serika","hits":{"per_page":3},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CS231n Assignment 1 (Updating)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-12-30 22:02:48'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/CodeByZach/pace/themes/green/pace-theme-flash.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Kung's Blog" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-language"></i><span> English</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/cs231n_assignment1.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Kung's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-language"></i><span> English</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">CS231n Assignment 1 (Updating)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-20T14:49:13.000Z" title="发表于 2022-12-20 22:49:13">2022-12-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-12-30T14:02:48.011Z" title="更新于 2022-12-30 22:02:48">2022-12-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="CS231n Assignment 1 (Updating)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="cs231n-assignment-1">CS231n Assignment 1</h1>
<p>作业1分为五个部分：KNN、SVM、Softmax classifier、2层神经网络、Higher
Level Representations: Image Features.</p>
<p>建议作业完成顺序：</p>
<ol type="1">
<li>k近邻分类：knn.ipynb &amp; k_nearest_neighbor.py</li>
<li>svm线性分类：svm.ipynb &amp; linear_svm.py &amp;
linear_classifier.py</li>
<li>softmax线性分类：softmax.ipynb &amp; softmax.py</li>
<li>两层神经网络：two_layer_net.ipynb &amp; neural_net.py</li>
</ol>
<h1 id="k-nearest-neighbor-knn">k-Nearest Neighbor (kNN)</h1>
<p>在knn.ipynb中，调用了k_nearest_neighbor.py文件。</p>
<p>k近邻分类算法步骤如下介绍：</p>
<ol type="1">
<li>记住所有训练图像</li>
<li>计算测试图像与所有训练图像的距离（常用L2距离）</li>
<li>选择与测试图像距离最小的k张训练图像</li>
<li>计算这k张图像所对应的类别出现的次数，选择出现次数最多的类别记为预测类别</li>
</ol>
<h2 id="k_nearest_neighbor.py">k_nearest_neighbor.py</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">KNearestNeighbor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; a kNN classifier with L2 distance &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#定义一个k近邻分类器的类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<h3 id="训练">训练</h3>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Train the classifier. For k-nearest neighbors this is just</span></span><br><span class="line"><span class="string">    memorizing the training data.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_train, D) containing the training data</span></span><br><span class="line"><span class="string">      consisting of num_train samples each of dimension D.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing the training labels, where</span></span><br><span class="line"><span class="string">         y[i] is the label for X[i].</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#self.X_train 是训练数据，维度是 (N,D)，训练集有N个样本，每个样本特征是D维</span></span><br><span class="line">    <span class="comment">#self.y_train 是标签，维度是（N,）,即N个训练样本对应的标签</span></span><br><span class="line">    self.X_train = X</span><br><span class="line">    self.y_train = y</span><br></pre></td></tr></table></figure>
<h3
id="预测计算测试图像和所有训练图像的l2距离">预测：计算测试图像和所有训练图像的L2距离</h3>
<p>预测时首先需要计算测试样本与所有训练样本的距离,然后根据距离判断样本的类别。</p>
<p>计算距离需要我们实现三种方法，分别为需要双重循环，单循环，不需要循环。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X, k=<span class="number">1</span>, num_loops=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Predict labels for test data using this classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_test, D) containing test data consisting</span></span><br><span class="line"><span class="string">         of num_test samples each of dimension D.</span></span><br><span class="line"><span class="string">    - k: The number of nearest neighbors that vote for the predicted labels.</span></span><br><span class="line"><span class="string">    - num_loops: Determines which implementation to use to compute distances</span></span><br><span class="line"><span class="string">      between training points and testing points.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">      test data, where y[i] is the predicted label for the test point X[i].</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> num_loops == <span class="number">0</span>:</span><br><span class="line">        dists = self.compute_distances_no_loops(X)</span><br><span class="line">    <span class="keyword">elif</span> num_loops == <span class="number">1</span>:</span><br><span class="line">        dists = self.compute_distances_one_loop(X)</span><br><span class="line">    <span class="keyword">elif</span> num_loops == <span class="number">2</span>:</span><br><span class="line">        dists = self.compute_distances_two_loops(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid value %d for num_loops&quot;</span> % num_loops)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.predict_labels(dists, k=k)</span><br></pre></td></tr></table></figure>
<hr />
<h4 id="双重循环实现">双重循环实现</h4>
<p>第i个测试样本与第j个训练样本的距离<span
class="math inline">\(dist[i,j]\)</span>等于用第i个测试图像的特征向量减去第j个训练图像的特征向量的值</p>
<p><span
class="math inline">\(dist[i,j]=\sqrt{\sum_{d}(x_{test[i,d]}-x_{train[j,d]})^2}\)</span></p>
<p><img
src="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/dij.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_distances_two_loops</span>(<span class="params">self, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using a nested loop over both the training data and the</span></span><br><span class="line"><span class="string">    test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_test, D) containing test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">      is the Euclidean distance between the ith test point and the jth training</span></span><br><span class="line"><span class="string">      point.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_test):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_train):</span><br><span class="line">            <span class="comment">#####################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                             #</span></span><br><span class="line">            <span class="comment"># Compute the l2 distance between the ith test point and the jth    #</span></span><br><span class="line">            <span class="comment"># training point, and store the result in dists[i, j]. You should   #</span></span><br><span class="line">            <span class="comment"># not use a loop over dimension, nor use np.linalg.norm().          #</span></span><br><span class="line">            <span class="comment">#####################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            dists[i][j]=np.sqrt(np.<span class="built_in">sum</span>(np.square(X[i,:]-self.X_train[j,:])))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<hr />
<h4 id="单循环实现">单循环实现</h4>
<p>利用numpy的broadcast机制，可以直接计算第i张测试图像与所有训练样本的距离</p>
<p><img
src="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/dij_single.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_distances_one_loop</span>(<span class="params">self, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using a single loop over the test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_test):</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                               #</span></span><br><span class="line">        <span class="comment"># Compute the l2 distance between the ith test point and all training #</span></span><br><span class="line">        <span class="comment"># points, and store the result in dists[i, :].                        #</span></span><br><span class="line">        <span class="comment"># Do not use np.linalg.norm().                                        #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        dists[i,:]=np.sqrt(np.<span class="built_in">sum</span>(np.square(X[i,:]-self.X_train),axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<hr />
<h4 id="无循环实现">无循环实现</h4>
<p>两个矩阵不能直接相减，不用循环计算距离，考虑距离公式，同时需保证最后得到的dists.shape满足(num_test,num_train)=(500,5000)</p>
<p><span class="math inline">\((a-b)^2=a^2+b^2-2ab\)</span></p>
<p><img
src="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/dij_no.png" /></p>
<p>注意点：</p>
<ol type="1">
<li>np.square，返回原列表每个元素的平方值，shape不变。</li>
<li>np.sum(,axis=1)按列相加（横向），我之前在这个地方没想通。以np.sum(np.square(self.X_train),
axis = 1)为例：</li>
</ol>
<ul>
<li>np.shape(self.X_train)=(5000,3072)</li>
<li>np.shape(np.square(self.X_train))=(5000,3072)</li>
<li>np.shape(np.sum(np.square(self.X_train)))=(5000,)这表示的是一个含有5000个元素的一维数组，并不是(5000,1)具有5000行的二维list。因此可以被boardcasting,(1,5000)-&gt;(500,5000)</li>
</ul>
<ol start="3" type="1">
<li>transpose针对二维及以上list有效。以np.transpose([np.sum(np.square(X),
axis = 1)]))为例，结合第2点可知:</li>
</ol>
<ul>
<li>得到的np.sum(np.square(X), axis =
1)]))是一个一维数组，shape为(500,)</li>
<li>因此加上[]变成(1,500)</li>
<li>之后转置得到(500,1),因此可以被boardcasting,(500,1)-&gt;(500,5000)</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_distances_no_loops</span>(<span class="params">self, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using no explicit loops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">    <span class="comment"># Compute the l2 distance between all test points and all training      #</span></span><br><span class="line">    <span class="comment"># points without using any explicit loops, and store the result in      #</span></span><br><span class="line">    <span class="comment"># dists.                                                                #</span></span><br><span class="line">    <span class="comment">#                                                                       #</span></span><br><span class="line">    <span class="comment"># You should implement this function using only basic array operations; #</span></span><br><span class="line">    <span class="comment"># in particular you should not use functions from scipy,                #</span></span><br><span class="line">    <span class="comment"># nor use np.linalg.norm().                                             #</span></span><br><span class="line">    <span class="comment">#                                                                       #</span></span><br><span class="line">    <span class="comment"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span></span><br><span class="line">    <span class="comment">#       and two broadcast sums.                                         #</span></span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    dists = np.sqrt(-<span class="number">2</span>*np.dot(X, self.X_train.T) + np.<span class="built_in">sum</span>(np.square(self.X_train), axis = <span class="number">1</span>) + np.transpose([np.<span class="built_in">sum</span>(np.square(X), axis = <span class="number">1</span>)]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<h3
id="预测根据k个最邻近距离中的多数确定标签">预测：根据K个最邻近距离中的多数确定标签</h3>
<ul>
<li>np.argsort() 返回一个数组排好序后各元素对应的原来的位置序号</li>
</ul>
<p>Examples:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#One dimensional array:</span></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">np.argsort(x)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#Two-dimensional array:</span></span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">0</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">x</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">ind = np.argsort(x, axis=<span class="number">0</span>)  <span class="comment"># sorts along first axis (down)</span></span><br><span class="line">ind</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">np.take_along_axis(x, ind, axis=<span class="number">0</span>)  <span class="comment"># same as np.sort(x, axis=0)</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>
<ul>
<li>np.bincount() 计算非负整数数组中每个值的出现次数。</li>
</ul>
<p>Examples:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#[0,1,2,3,4]</span></span><br><span class="line">np.bincount(np.arange(<span class="number">5</span>))</span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">np.bincount(np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">7</span>]))</span><br><span class="line">array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<hr />
<p>需要实现两个功能</p>
<ol type="1">
<li><p>选择与测试图像最相似（距离最小）的k张训练图像
np.argsort(dists[i])函数是将dist中的i行元素从小到大排列，并得到对应的index。然后再取前k个索引（也就是得到距离最近的k张图像的索引）</p></li>
<li><p>计算这k张图像所对应的类别出现的次数，选择出现次数最多的类别</p></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_labels</span>(<span class="params">self, dists, k=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Given a matrix of distances between test points and training points,</span></span><br><span class="line"><span class="string">    predict a label for each test point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">      gives the distance betwen the ith test point and the jth training point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">      test data, where y[i] is the predicted label for the test point X[i].</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_test = dists.shape[<span class="number">0</span>]</span><br><span class="line">    y_pred = np.zeros(num_test)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_test):</span><br><span class="line">        <span class="comment"># A list of length k storing the labels of the k nearest neighbors to</span></span><br><span class="line">        <span class="comment"># the ith test point.</span></span><br><span class="line">        closest_y = []</span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">        <span class="comment"># Use the distance matrix to find the k nearest neighbors of the ith    #</span></span><br><span class="line">        <span class="comment"># testing point, and use self.y_train to find the labels of these       #</span></span><br><span class="line">        <span class="comment"># neighbors. Store these labels in closest_y.                           #</span></span><br><span class="line">        <span class="comment"># Hint: Look up the function numpy.argsort.                             #</span></span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        closest_y = self.y_train[np.argsort(dists[i])[:k]]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">        <span class="comment"># Now that you have found the labels of the k nearest neighbors, you    #</span></span><br><span class="line">        <span class="comment"># need to find the most common label in the list closest_y of labels.   #</span></span><br><span class="line">        <span class="comment"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span></span><br><span class="line">        <span class="comment"># label.                                                                #</span></span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        y_pred[i] = np.argmax(np.bincount(closest_y))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="knn.ipynb">knn.ipynb</h2>
<p>讨论knn中k的取值问题</p>
<ul>
<li>np.array_split() 将一个数组拆分为多个子数组，可以大小不等。</li>
</ul>
<p>Examples:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">8.0</span>)</span><br><span class="line">np.array_split(x, <span class="number">3</span>)</span><br><span class="line">[array([<span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>]), array([<span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>]), array([<span class="number">6.</span>,  <span class="number">7.</span>])]</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">9</span>)</span><br><span class="line">np.array_split(x, <span class="number">4</span>)</span><br><span class="line">[array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]), array([<span class="number">3</span>, <span class="number">4</span>]), array([<span class="number">5</span>, <span class="number">6</span>]), array([<span class="number">7</span>, <span class="number">8</span>])]</span><br></pre></td></tr></table></figure>
<ul>
<li>dictionary.setdefault(keyname, value)返回具有指定键的项目的值。</li>
</ul>
<p>Examples:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">car = &#123;</span><br><span class="line">  <span class="string">&quot;brand&quot;</span>: <span class="string">&quot;Ford&quot;</span>,</span><br><span class="line">  <span class="string">&quot;model&quot;</span>: <span class="string">&quot;Mustang&quot;</span>,</span><br><span class="line">  <span class="string">&quot;year&quot;</span>: <span class="number">1964</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">x = car.setdefault(<span class="string">&quot;color&quot;</span>, <span class="string">&quot;White&quot;</span>)</span><br></pre></td></tr></table></figure>
<ul>
<li>hstack() 按列顺序(横向)把数组给堆叠起来</li>
</ul>
<p>Examples:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b=[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="built_in">print</span>(np.hstack((a,b)))</span><br><span class="line"><span class="comment">#[1 2 3 4 5 6 ]</span></span><br><span class="line"></span><br><span class="line">a=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">b=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">c=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">d=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line"><span class="built_in">print</span>(np.hstack((a,b,c,d)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[1 1 1 1]</span></span><br><span class="line"><span class="string"> [2 2 2 2]</span></span><br><span class="line"><span class="string"> [3 3 3 3]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
<hr />
<p>knn需要计算这k张图像所对应的类别出现的次数，选择出现次数最多的类别，那么问题来了，k应该取几效果会比较好呢？</p>
<p>这需要做两个任务：</p>
<ol type="1">
<li>划分训练集</li>
<li>交叉验证</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_folds = <span class="number">5</span></span><br><span class="line">k_choices = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">X_train_folds = []</span><br><span class="line">y_train_folds = []</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Split up the training data into folds. After splitting, X_train_folds and    #</span></span><br><span class="line"><span class="comment"># y_train_folds should each be lists of length num_folds, where                #</span></span><br><span class="line"><span class="comment"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span></span><br><span class="line"><span class="comment"># Hint: Look up the numpy array_split function.                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">X_train_folds = np.array_split(X_train,num_folds)</span><br><span class="line">y_train_folds = np.array_split(y_train,num_folds)</span><br><span class="line"></span><br><span class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A dictionary holding the accuracies for different values of k that we find</span></span><br><span class="line"><span class="comment"># when running cross-validation. After running cross-validation,</span></span><br><span class="line"><span class="comment"># k_to_accuracies[k] should be a list of length num_folds giving the different</span></span><br><span class="line"><span class="comment"># accuracy values that we found when using that value of k.</span></span><br><span class="line">k_to_accuracies = &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Perform k-fold cross validation to find the best value of k. For each        #</span></span><br><span class="line"><span class="comment"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span></span><br><span class="line"><span class="comment"># where in each case you use all but one of the folds as training data and the #</span></span><br><span class="line"><span class="comment"># last fold as a validation set. Store the accuracies for all fold and all     #</span></span><br><span class="line"><span class="comment"># values of k in the k_to_accuracies dictionary.                               #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">    k_to_accuracies.setdefault(k, [])</span><br><span class="line">    <span class="comment">#print(k_to_accuracies)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_folds):</span><br><span class="line">    classifier = KNearestNeighbor()</span><br><span class="line">    X_val_train = np.vstack(X_train_folds[<span class="number">0</span>:i] + X_train_folds[i+<span class="number">1</span>:])</span><br><span class="line">    y_val_train = np.hstack(y_train_folds[<span class="number">0</span>:i] + y_train_folds[i+<span class="number">1</span>:])</span><br><span class="line">    <span class="comment">#print(X_val_train, y_val_train)</span></span><br><span class="line">    classifier.train(X_val_train, y_val_train)</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">        y_val_pred = classifier.predict(X_train_folds[i], k)</span><br><span class="line">        num_correct = np.<span class="built_in">sum</span>(y_val_pred == y_train_folds[i])</span><br><span class="line">        accuracy = <span class="built_in">float</span>(num_correct) / <span class="built_in">len</span>(y_val_pred)</span><br><span class="line">        k_to_accuracies[k] += [accuracy]</span><br><span class="line">        <span class="comment">#print(k,k_to_accuracies[k])</span></span><br><span class="line">    <span class="comment">#print(k_to_accuracies)</span></span><br><span class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the computed accuracies</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">sorted</span>(k_to_accuracies):</span><br><span class="line">    <span class="keyword">for</span> accuracy <span class="keyword">in</span> k_to_accuracies[k]:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;k = %d, accuracy = %f&#x27;</span> % (k, accuracy))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#OUTPUT</span></span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.263000</span></span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.257000</span> </span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.264000</span></span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.278000</span></span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.266000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.239000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.249000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.240000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.266000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.254000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.248000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.266000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.280000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.292000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.280000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.262000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.282000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.273000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.290000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.273000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.265000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.296000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.276000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.284000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.280000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.260000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.295000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.279000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.283000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.280000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.252000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.289000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.278000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.282000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.274000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.270000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.279000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.279000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.282000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.285000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.271000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.288000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.278000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.269000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.266000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.256000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.270000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.263000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.256000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.263000</span></span><br></pre></td></tr></table></figure>
<p>通过这行代码可以选出最好的k值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_k = k_choices[accuracies_mean.argmax()]</span><br></pre></td></tr></table></figure>
<hr />
<h3 id="inline-question-1"><strong>Inline Question 1</strong></h3>
<p>Notice the structured patterns in the distance matrix, where some
rows or columns are visibly brighter. (Note that with the default color
scheme black indicates low distances while white indicates high
distances.)</p>
<ul>
<li>What in the data is the cause behind the distinctly bright
rows?</li>
<li>What causes the columns?</li>
</ul>
<p><span class="math inline">\(\color{blue}{\textit Your
Answer:}\)</span> <em>fill this in.</em></p>
<ol type="1">
<li><p>The test image is far different from all the train
image.</p></li>
<li><p>The train image is unsimilar to all the test image.</p></li>
</ol>
<hr />
<h3 id="inline-question-2"><strong>Inline Question 2</strong></h3>
<p>We can also use other distance metrics such as L1 distance. For pixel
values <span class="math inline">\(p_{ij}^{k}\)</span> at location <span
class="math inline">\((i,j)\)</span> of some image <span
class="math inline">\(I_k\)</span>,</p>
<p>the mean <span class="math inline">\(\mu\)</span> across all pixels
over all images is <span
class="math display">\[\mu=\frac{1}{nhw}\sum_{k=1}^n\sum_{i=1}^{h}\sum_{j=1}^{w}p_{ij}^{(k)}\]</span>
And the pixel-wise mean <span class="math inline">\(\mu_{ij}\)</span>
across all images is <span
class="math display">\[\mu_{ij}=\frac{1}{n}\sum_{k=1}^np_{ij}^{(k)}.\]</span>
The general standard deviation <span
class="math inline">\(\sigma\)</span> and pixel-wise standard deviation
<span class="math inline">\(\sigma_{ij}\)</span> is defined
similarly.</p>
<p>Which of the following preprocessing steps will not change the
performance of a Nearest Neighbor classifier that uses L1 distance?
Select all that apply.</p>
<ol type="1">
<li><p>Subtracting the mean <span class="math inline">\(\mu\)</span>
(<span
class="math inline">\(\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu\)</span>.)</p></li>
<li><p>Subtracting the per pixel mean <span
class="math inline">\(\mu_{ij}\)</span> (<span
class="math inline">\(\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu_{ij}\)</span>.)</p></li>
<li><p>Subtracting the mean <span class="math inline">\(\mu\)</span> and
dividing by the standard deviation <span
class="math inline">\(\sigma\)</span>.</p></li>
<li><p>Subtracting the pixel-wise mean <span
class="math inline">\(\mu_{ij}\)</span> and dividing by the pixel-wise
standard deviation <span
class="math inline">\(\sigma_{ij}\)</span>.</p></li>
<li><p>Rotating the coordinate axes of the data.</p></li>
</ol>
<p><span class="math inline">\(\color{blue}{\textit Your
Answer:}\)</span></p>
<p>1, 3 will not change the L1 Distance.</p>
<p><span class="math inline">\(\color{blue}{\textit Your
Explanation:}\)</span></p>
<p><img
src="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/inline1.jpg" /></p>
<p><img
src="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/inline2.jpg" /></p>
<p><img
src="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/inline3.jpg" /></p>
<hr />
<h3 id="inline-question-3"><strong>Inline Question 3</strong></h3>
<p>Which of the following statements about <span
class="math inline">\(k\)</span>-Nearest Neighbor (<span
class="math inline">\(k\)</span>-NN) are true in a classification
setting, and for all <span class="math inline">\(k\)</span>? Select all
that apply. 1. The decision boundary of the k-NN classifier is linear.
2. The training error of a 1-NN will always be lower than or equal to
that of 5-NN. 3. The test error of a 1-NN will always be lower than that
of a 5-NN. 4. The time needed to classify a test example with the k-NN
classifier grows with the size of the training set. 5. None of the
above.</p>
<p><span class="math inline">\(\color{blue}{\textit Your
Answer:}\)</span></p>
<p>2, 4.</p>
<p><span class="math inline">\(\color{blue}{\textit Your
Explanation:}\)</span></p>
<ol type="1">
<li><p>False. It depends on the given categories of data, if you give a
category with a circle boundary to its neighborhood, it is
non-linear.</p></li>
<li><p>True. In fact the training error of a 1-NN is always 0, and
5-NN's lower bound is 0. It is because the nearest neighbor of test data
is always going to be itself in 1-NN.</p></li>
<li><p>False. The value of k is thus data-dependent, that is why we need
to perform cross validation to determine the best k for your intended
application and dataset.</p></li>
<li><p>True. At test, KNN needs to make a full pass through the entire
data set and sort points by distance. The time needed thus grows with
the size of the data.</p></li>
</ol>
<hr />
<h2 id="knn部分参考链接">KNN部分参考链接:</h2>
<ol type="1">
<li><p>cs231n官网: <a
target="_blank" rel="noopener" href="https://cs231n.github.io/">https://cs231n.github.io/</a></p></li>
<li><p>cs231n作业，assignment1-knn详解（注重算法与代码的结合）: <a
target="_blank" rel="noopener" href="https://blog.csdn.net/qq_24906797/article/details/89245722">https://blog.csdn.net/qq_24906797/article/details/89245722</a></p></li>
<li><p>cs231n assignment1 knn: <a
target="_blank" rel="noopener" href="https://blog.csdn.net/SpicyCoder/article/details/94992552">https://blog.csdn.net/SpicyCoder/article/details/94992552</a></p></li>
<li><p>【本课程配套的代码作业讲解见置顶评论】斯坦福CS231N计算机视觉作业讲解：
<a
target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1t4411U78z/?spm_id_from=333.337.search-card.all.click&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a">https://www.bilibili.com/video/BV1t4411U78z/?spm_id_from=333.337.search-card.all.click&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a</a></p></li>
<li><p>CS231N作业详解零基础版： <a
target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV19z411b7u9/?p=6&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a">https://www.bilibili.com/video/BV19z411b7u9/?p=6&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a</a></p></li>
</ol>
<h1 id="support-vector-machine-svm">Support Vector Machine (SVM)</h1>
<p>在svm.ipynb中，调用了linear_svm.py和linear_classifier.py两个文件。</p>
<p>为方便理解，先介绍SVM的引入基于的几个概念：</p>
<ol type="1">
<li><p>我们要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：</p>
<ul>
<li><p>一个是评分函数（score
function），它是原始图像数据到类别分值的映射。</p></li>
<li><p>另一个是损失函数（loss
function），它是用来量化预测分类标签的得分与真实标签之间一致性的。</p></li>
</ul>
<p>该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。从图像像素值到所属类别的评分函数（score
function）</p></li>
<li><p>该我们现在定义评分函数为<span class="math inline">\(f:R^D \to
R^K\)</span>，该函数是原始图像像素到分类分值的映射。在<strong>线性分类器</strong>中，一个线性映射：<span
class="math inline">\(f(x_i,W,b)=Wx_i+b\)</span>。在函数中，数据<span
class="math inline">\((x_i,y_i)\)</span>是给定的，不能修改。但是我们可以调整权重矩阵<span
class="math inline">\(W\)</span>这个参数，使得评分函数的结果与训练数据集中图像的真实类别一致，即评分函数在正确的分类的位置应当得到最高的评分（score）。</p></li>
<li><p>我们将使用损失函数（****Loss Function）（有时也叫代价函数****Cost
Function或目标函数****Objective）来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。多类支持向量机（SVM）损失函数是其中一种。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值Delta
$ $。</p></li>
</ol>
<h2 id="svm简介">SVM简介</h2>
<h3 id="公式说明">公式说明</h3>
<p>SVM算法由两个部分组成：数据损失（data
loss），即所有样例的的平均损失L_i，以及正则化损失（regularization
loss）。完整公式如下：</p>
<p><img
src="https://camo.githubusercontent.com/19467d143a4397b56c0aaf83d66de6ba9a4615c801e5ed3f2e1272b80748e789/687474703a2f2f7a686968752e636f6d2f6571756174696f6e3f7465783d4c253344253543646973706c61797374796c652b253543756e64657262726163652537422b25354366726163253742312537442537424e25374425354373756d5f692b4c5f692537445f253742646174612b2535432b2b6c6f7373253744253242253543756e64657262726163652537422535436c616d6264612b52253238572532392537445f253742726567756c6172697a6174696f6e2b2535432b6c6f7373253744" /></p>
<p>将其展开完整公式是：</p>
<p><span class="math inline">\(L=\frac{1}{N}\sum_i\sum_{j \not=
y_i}[\max(0,f(x_i;W)j-f(x_i;W){y_i}+\Delta)]+\lambda \sum_k \sum_l
W^2_{k,l}\)</span></p>
<p>其中参数意义如下： * X(N,D),N是训练集的数据量。 *
W(D,C),C代表图片分类的数量。 * y(N,) * i 是迭代第N个训练集数据 * j
是第C个图片分类 * <span class="math inline">\(\lambda\)</span>
正则化惩罚，添加到了损失函数里面，并用超参数<span
class="math inline">\(\lambda\)</span>来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。引入正则化惩罚还带来很多良好的性质，其中最好的性质就是对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响。</p>
<h3 id="注意点">注意点</h3>
<p>超参数在绝大多数情况下设为<span
class="math inline">\(\Delta\)</span>=1.0都是安全的。超参数<span
class="math inline">\(\Delta\)</span>和<span
class="math inline">\(\lambda\)</span>看起来是两个不同的超参数，但实际上他们一起控制同一个权衡：即损失函数中的数据损失和正则化损失之间的权衡。</p>
<h2 id="linear_svm.py">linear_svm.py</h2>
<p>损失函数公式： <span class="math inline">\(L=\frac{1}{N}\sum_i\sum_{j
\not= y_i}[\max(0,f(x_i;W)j-f(x_i;W){y_i}+\Delta)]+\lambda \sum_k \sum_l
W^2_{k,l}\)</span></p>
<p>我们想通过一个方法来得到损失函数L的最小值，这里考虑使用计算W的梯度来不停的对L进行优化，这里想的就是初始化一个W，然后计算W的梯度，接着不停的迭代W，直到收敛或者达到迭代次数。那问题就变成如何求L对于W的梯度了。</p>
<h3 id="循环求解">循环求解</h3>
<p>后面的正则项，就是<span class="math inline">\(\lambda \sum_k \sum_l
W^2_{k,l}\)</span>，求导即为 <span class="math display">\[
\frac{dL}{dw}(正则项)=2*\lambda*W
\]</span></p>
<p>主要是求前面数据损失函数的梯度。那么，我们先把L给拆分一下，这样可以去掉一个求和符号
<span class="math display">\[L_i=\sum_{j \not=
y_i}\max(0,x_iw_j-x_iw_{y_i}+\Delta)\]</span></p>
<p>（1）考虑<span class="math inline">\(j \not= y_i\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\frac{dL_i}{dw_j}=
1(x_iw_j-x_iw_{y_i}+\Delta&gt;0) *
    \begin{array}
    {|c|}
    x_{i1} \\
    x_{i2}\\
    \vdots&amp;\\
    x_{iD}
    \end{array}
\end{aligned}
\]</span> 所以得到 <span class="math display">\[
\frac{dL_i}{dw_j}=
1(x_iw_j-x_iw_{y_i}+\Delta&gt;0) *{x_i}
\]</span></p>
<p>（2）考虑<span class="math inline">\(j = y_i\)</span>，则满足</p>
<ul>
<li><span
class="math display">\[\frac{d(x_iw_j)}{dw_{y_i}}=0\]</span></li>
<li><span
class="math display">\[\frac{d(-x_iw_{y_i})}{dw_{y_i}}=-x_i\]</span></li>
</ul>
<p>因此代入以下公式 <span class="math display">\[
\begin{aligned}
\frac{dL_i}{dw_{y_i}}=
-\sum_{j \not= y_i}1(x_iw_j-x_iw_{y_i}+\Delta&gt;0) *
    \begin{array}
    {|c|}
    x_{i1} \\
    x_{i2}\\
    \vdots&amp;\\
    x_{iD}
    \end{array}
\end{aligned}
\]</span></p>
<p>最终可以得到</p>
<p><span class="math display">\[\frac{dL_i}{dw_{y_i}}=
-\sum_{j \not= y_i}1(x_iw_j-x_iw_{y_i}+\Delta&gt;0) * x_i
\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">svm_loss_naive</span>(<span class="params">W, X, y, reg</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Structured SVM loss function, naive implementation (with loops).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dW = np.zeros(W.shape)  <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the loss and the gradient</span></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        correct_class_score = scores[y[i]]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            margin = scores[j] - correct_class_score + <span class="number">1</span>  <span class="comment"># note delta = 1</span></span><br><span class="line">            <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</span><br><span class="line">                loss += margin</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Right now the loss is a sum over all training examples, but we want it</span></span><br><span class="line">    <span class="comment"># to be an average instead so we divide by num_train.</span></span><br><span class="line">    loss /= num_train</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add regularization to the loss.</span></span><br><span class="line">    loss += reg * np.<span class="built_in">sum</span>(W * W)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Compute the gradient of the loss function and store it dW.                #</span></span><br><span class="line">    <span class="comment"># Rather that first computing the loss and then computing the derivative,   #</span></span><br><span class="line">    <span class="comment"># it may be simpler to compute the derivative at the same time that the     #</span></span><br><span class="line">    <span class="comment"># loss is being computed. As a result you may need to modify some of the    #</span></span><br><span class="line">    <span class="comment"># code above to compute the gradient.                                       #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_train):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            margin = scores[j] - correct_class_score + <span class="number">1</span>  <span class="comment">#note delta = 1 </span></span><br><span class="line">            <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</span><br><span class="line">                dW[:,j]+=X[i,:]</span><br><span class="line">                dW[:,y[i]]+=-X[i,:]</span><br><span class="line">    </span><br><span class="line">    dW /= num_train</span><br><span class="line">    dW += <span class="number">2</span>*reg*dW</span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h3 id="向量化实现">向量化实现</h3>
<p>第一个部分，损失函数。公式和前面基本一致，数据损失函数部分</p>
<p>分类正确即对应 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scores[<span class="built_in">range</span>(N),y]=<span class="number">0</span> </span><br></pre></td></tr></table></figure></p>
<p>分类错误即对应</p>
<p><span class="math display">\[L_i=\sum_{j \not=
y_i}\max(0,x_iw_j-x_iw_{y_i}+\Delta)\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scores = np.dot(X,W) - scores[<span class="built_in">range</span>(N),[y]].T + <span class="number">1</span></span><br><span class="line">np.maximum(<span class="number">0</span>,scores)</span><br></pre></td></tr></table></figure>
<p>因为X(N,D),N是训练集的数据量。W(D,C),C代表图片分类的数量。所以一开始保证维数一致:<span
class="math display">\[scores = X * W\]</span></p>
<ul>
<li><p>np.dot(X,W) shape: (N,C)</p></li>
<li><p>scores shape: (N,C)</p>
<ul>
<li>scores[range(N),[y]].T shape: (N,C) -&gt; (1,N) -&gt; (N,1)</li>
<li>表示选取每个图片的正确分类，给它们评分函数的相应位置置0，说明损失为0，而其他位置则需要按照SVM的公式计算损失并且和0比较大小。</li>
</ul></li>
</ul>
<p>后面的正则项损失函数部分，</p>
<p><span class="math display">\[
\frac{dL}{dw}(正则项)=2*\lambda*W
\]</span></p>
<p>二维数组的np.sum, shape是( , ) 也就是一个数值。</p>
<p>第二个部分，梯度求解。用到了链式法则： <span class="math display">\[
\frac{dL}{dw}=\frac{dL}{dS}*\frac{dS}{dw}
\]</span></p>
<p><img
src="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/SVMdw.png" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">svm_loss_vectorized</span>(<span class="params">W, X, y, reg</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Structured SVM loss function, vectorized implementation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as svm_loss_naive.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros(W.shape)  <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the structured SVM loss, storing the    #</span></span><br><span class="line">    <span class="comment"># result in loss.                                                           #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    N=<span class="built_in">len</span>(y)</span><br><span class="line">    scores = np.dot(X,W)</span><br><span class="line">    scores -= scores[<span class="built_in">range</span>(N),[y]].T</span><br><span class="line">    scores += <span class="number">1</span></span><br><span class="line">    scores[<span class="built_in">range</span>(N),y]=<span class="number">0</span></span><br><span class="line">    margin = np.maximum(<span class="number">0</span>,scores)</span><br><span class="line">    loss = np.<span class="built_in">sum</span>(margin) / N + reg * np.<span class="built_in">sum</span>(np.square(W)) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the gradient for the structured SVM     #</span></span><br><span class="line">    <span class="comment"># loss, storing the result in dW.                                           #</span></span><br><span class="line">    <span class="comment">#                                                                           #</span></span><br><span class="line">    <span class="comment"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span></span><br><span class="line">    <span class="comment"># to reuse some of the intermediate values that you used to compute the     #</span></span><br><span class="line">    <span class="comment"># loss.                                                                     #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    ds = np.zeros_like(margin)</span><br><span class="line">    ds[margin&gt;<span class="number">0</span>]=<span class="number">1</span></span><br><span class="line">    ds[<span class="built_in">range</span>(N),y]-=np.<span class="built_in">sum</span>(ds,axis=<span class="number">1</span>)</span><br><span class="line">    ds /= N</span><br><span class="line">    dW = X.T.dot(ds)</span><br><span class="line">    dW += <span class="number">2</span> * reg * W</span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h2 id="linear_classifier.py">linear_classifier.py</h2>
<h3 id="训练-1">训练</h3>
<ul>
<li>np.random.choice() 从给定的一维数组生成随机样本。</li>
</ul>
<p>Examples:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#从大小为 3 的 np.arange(5) 生成均匀随机样本：</span></span><br><span class="line">np.random.choice(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">array([<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>]) <span class="comment"># random</span></span><br><span class="line"><span class="comment">#This is equivalent to np.random.randint(0,5,3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#从大小为 3 的 np.arange(5) 生成一个非均匀随机样本：</span></span><br><span class="line">np.random.choice(<span class="number">5</span>, <span class="number">3</span>, p=[<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0</span>])</span><br><span class="line">array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>]) <span class="comment"># random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#从大小为 3 的 np.arange(5) 生成一个统一的随机样本，无需替换，说明没有重复取值：</span></span><br><span class="line">np.random.choice(<span class="number">5</span>, <span class="number">3</span>, replace=<span class="literal">False</span>)</span><br><span class="line">array([<span class="number">3</span>,<span class="number">1</span>,<span class="number">0</span>]) <span class="comment"># random</span></span><br><span class="line"><span class="comment">#This is equivalent to np.random.permutation(np.arange(5))[:3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#从大小为 3 的 np.arange(5) 生成非均匀随机样本，无需替换：</span></span><br><span class="line">np.random.choice(<span class="number">5</span>, <span class="number">3</span>, replace=<span class="literal">False</span>, p=[<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0</span>])</span><br><span class="line">array([<span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>]) <span class="comment"># random</span></span><br></pre></td></tr></table></figure>
<p>实现train函数，作用是从每一个 iteration 中选出 batch_size
个训练样本投入到 SVM 中，然后再计算一次 Loss
函数进行梯度下降，避免计算太频繁导致时间消耗过大。有两部分需要补全，第一个是随机选择数据，第二个是梯度下降，实现都比较简单</p>
<p>梯度下降公式： <img
src="https://math.jianshu.com/math?formula=%5Ctheta%20%3D%5Ctheta%20-%5Ceta%20%5Ccdot%20%5Ctriangledown%20_%7B%5Ctheta%20%7DJ%5Cleft%20(%20%5Ctheta%20%5Cright%20)" /></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LinearClassifier</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.W = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        X,</span></span><br><span class="line"><span class="params">        y,</span></span><br><span class="line"><span class="params">        learning_rate=<span class="number">1e-3</span>,</span></span><br><span class="line"><span class="params">        reg=<span class="number">1e-5</span>,</span></span><br><span class="line"><span class="params">        num_iters=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">        batch_size=<span class="number">200</span>,</span></span><br><span class="line"><span class="params">        verbose=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Train this linear classifier using stochastic gradient descent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (N, D) containing training data; there are N</span></span><br><span class="line"><span class="string">          training samples each of dimension D.</span></span><br><span class="line"><span class="string">        - y: A numpy array of shape (N,) containing training labels; y[i] = c</span></span><br><span class="line"><span class="string">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span></span><br><span class="line"><span class="string">        - learning_rate: (float) learning rate for optimization.</span></span><br><span class="line"><span class="string">        - reg: (float) regularization strength.</span></span><br><span class="line"><span class="string">        - num_iters: (integer) number of steps to take when optimizing</span></span><br><span class="line"><span class="string">        - batch_size: (integer) number of training examples to use at each step.</span></span><br><span class="line"><span class="string">        - verbose: (boolean) If true, print progress during optimization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Outputs:</span></span><br><span class="line"><span class="string">        A list containing the value of the loss function at each training iteration.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_train, dim = X.shape</span><br><span class="line">        num_classes = (</span><br><span class="line">            np.<span class="built_in">max</span>(y) + <span class="number">1</span></span><br><span class="line">        )  <span class="comment"># assume y takes values 0...K-1 where K is number of classes</span></span><br><span class="line">        <span class="keyword">if</span> self.W <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># lazily initialize W</span></span><br><span class="line">            self.W = <span class="number">0.001</span> * np.random.randn(dim, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run stochastic gradient descent to optimize W</span></span><br><span class="line">        loss_history = []</span><br><span class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">            X_batch = <span class="literal">None</span></span><br><span class="line">            y_batch = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Sample batch_size elements from the training data and their           #</span></span><br><span class="line">            <span class="comment"># corresponding labels to use in this round of gradient descent.        #</span></span><br><span class="line">            <span class="comment"># Store the data in X_batch and their corresponding labels in           #</span></span><br><span class="line">            <span class="comment"># y_batch; after sampling X_batch should have shape (batch_size, dim)   #</span></span><br><span class="line">            <span class="comment"># and y_batch should have shape (batch_size,)                           #</span></span><br><span class="line">            <span class="comment">#                                                                       #</span></span><br><span class="line">            <span class="comment"># Hint: Use np.random.choice to generate indices. Sampling with         #</span></span><br><span class="line">            <span class="comment"># replacement is faster than sampling without replacement.              #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            bindex = np.random.choice(num_train,batch_size)</span><br><span class="line">            X_batch = X[bindex]</span><br><span class="line">            y_batch = y[bindex]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># evaluate loss and gradient</span></span><br><span class="line">            loss, grad = self.loss(X_batch, y_batch, reg)</span><br><span class="line">            loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># perform parameter update</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Update the weights using the gradient and the learning rate.          #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            self.W = self.W - learning_rate * grad</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;iteration %d / %d: loss %f&quot;</span> % (it, num_iters, loss))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss_history</span><br></pre></td></tr></table></figure>
<h3 id="预测">预测</h3>
<p>调参环节，从不同的 learning rate 与 regularization strengths
中选出使验证集正确率最高的组合。对每一种组合都训一遍
SVM，然后计算一次正确率。不过在 learning rate
较大的两个情况训练时，发生了计算溢出的情况。题面中说这是正常现象，正确率接近39%​就算成功。</p>
<ul>
<li>np.mean(y_train == y_train_pred)解释：
<ul>
<li>mean是求平均值的意思</li>
<li>y_train ==
y_train_pred意思就是判断训练的值和预测的值是否相同，相等返回1</li>
<li>将相等的全部加起来/总训练数，就是训练集的准确率了，mean这里就是统计相等的做除法算出准确率的作用</li>
<li>所以 np.mean(y_train == y_train_pred)就是算训练集准确率的意思</li>
</ul></li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Use the validation set to tune hyperparameters (regularization strength and</span></span><br><span class="line"><span class="comment"># learning rate). You should experiment with different ranges for the learning</span></span><br><span class="line"><span class="comment"># rates and regularization strengths; if you are careful you should be able to</span></span><br><span class="line"><span class="comment"># get a classification accuracy of about 0.39 (&gt; 0.385) on the validation set.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: you may see runtime/overflow warnings during hyper-parameter search. </span></span><br><span class="line"><span class="comment"># This may be caused by extreme values, and is not a bug.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># results is dictionary mapping tuples of the form</span></span><br><span class="line"><span class="comment"># (learning_rate, regularization_strength) to tuples of the form</span></span><br><span class="line"><span class="comment"># (training_accuracy, validation_accuracy). The accuracy is simply the fraction</span></span><br><span class="line"><span class="comment"># of data points that are correctly classified.</span></span><br><span class="line">results = &#123;&#125;</span><br><span class="line">best_val = -<span class="number">1</span>   <span class="comment"># The highest validation accuracy that we have seen so far.</span></span><br><span class="line">best_svm = <span class="literal">None</span> <span class="comment"># The LinearSVM object that achieved the highest validation rate.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Write code that chooses the best hyperparameters by tuning on the validation #</span></span><br><span class="line"><span class="comment"># set. For each combination of hyperparameters, train a linear SVM on the      #</span></span><br><span class="line"><span class="comment"># training set, compute its accuracy on the training and validation sets, and  #</span></span><br><span class="line"><span class="comment"># store these numbers in the results dictionary. In addition, store the best   #</span></span><br><span class="line"><span class="comment"># validation accuracy in best_val and the LinearSVM object that achieves this  #</span></span><br><span class="line"><span class="comment"># accuracy in best_svm.                                                        #</span></span><br><span class="line"><span class="comment">#                                                                              #</span></span><br><span class="line"><span class="comment"># Hint: You should use a small value for num_iters as you develop your         #</span></span><br><span class="line"><span class="comment"># validation code so that the SVMs don&#x27;t take much time to train; once you are #</span></span><br><span class="line"><span class="comment"># confident that your validation code works, you should rerun the validation   #</span></span><br><span class="line"><span class="comment"># code with a larger value for num_iters.                                      #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Provided as a reference. You may or may not want to change these hyperparameters</span></span><br><span class="line">learning_rates = [<span class="number">1e-7</span>, <span class="number">5e-5</span>]</span><br><span class="line">regularization_strengths = [<span class="number">2.5e4</span>, <span class="number">5e4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"><span class="keyword">for</span> rs <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">  <span class="keyword">for</span> lr <span class="keyword">in</span> learning_rates:</span><br><span class="line">    svm = LinearSVM()</span><br><span class="line">    loss_hist = svm.train(X_train,y_train,lr,rs,num_iters=<span class="number">1500</span>)</span><br><span class="line">    y_train_pred = svm.predict(X_train)</span><br><span class="line">    train_accuracy = np.mean(y_train == y_train_pred)</span><br><span class="line">    y_val_pred = svm.predict(X_val)</span><br><span class="line">    val_accuracy = np.mean(y_val == y_val_pred)</span><br><span class="line">    <span class="keyword">if</span> val_accuracy &gt; best_val:</span><br><span class="line">      best_val = val_accuracy</span><br><span class="line">      best_svm = svm</span><br><span class="line">    results[(lr,rs)] = train_accuracy,val_accuracy</span><br><span class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Print out results.</span></span><br><span class="line"><span class="keyword">for</span> lr, reg <span class="keyword">in</span> <span class="built_in">sorted</span>(results):</span><br><span class="line">    train_accuracy, val_accuracy = results[(lr, reg)]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;lr %e reg %e train accuracy: %f val accuracy: %f&#x27;</span> % (</span><br><span class="line">                lr, reg, train_accuracy, val_accuracy))</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;best validation accuracy achieved during cross-validation: %f&#x27;</span> % best_val)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#output result</span></span><br><span class="line">    lr <span class="number">1.000000e-07</span> reg <span class="number">2.500000e+04</span> train accuracy: <span class="number">0.371204</span> val accuracy: <span class="number">0.387000</span></span><br><span class="line">    lr <span class="number">1.000000e-07</span> reg <span class="number">5.000000e+04</span> train accuracy: <span class="number">0.351857</span> val accuracy: <span class="number">0.356000</span></span><br><span class="line">    lr <span class="number">5.000000e-05</span> reg <span class="number">2.500000e+04</span> train accuracy: <span class="number">0.054959</span> val accuracy: <span class="number">0.068000</span></span><br><span class="line">    lr <span class="number">5.000000e-05</span> reg <span class="number">5.000000e+04</span> train accuracy: <span class="number">0.100265</span> val accuracy: <span class="number">0.087000</span></span><br><span class="line">    best validation accuracy achieved during cross-validation: <span class="number">0.387000</span></span><br></pre></td></tr></table></figure>
<h3 id="inline-question-1-1"><strong>Inline Question 1</strong></h3>
<p>It is possible that once in a while a dimension in the gradcheck will
not match exactly. What could such a discrepancy be caused by? Is it a
reason for concern? What is a simple example in one dimension where a
gradient check could fail? How would change the margin affect of the
frequency of this happening? <em>Hint: the SVM loss function is not
strictly speaking differentiable</em></p>
<p><span class="math inline">\(\color{blue}{\textit Your
Answer:}\)</span></p>
<p>It is possible that the numerical gradient does not match the actual
gradient, because the max function is non-linear, continuous at 0 but
not derivable, so the numerical gradient is inaccurate at this
situation.</p>
<h3 id="inline-question-2-1"><strong>Inline question 2</strong></h3>
<p>Describe what your visualized SVM weights look like, and offer a
brief explanation for why they look the way they do.</p>
<p><span class="math inline">\(\color{blue}{\textit Your
Answer:}\)</span></p>
<p>Each class of weighted visual image shows roughly the shape of the
objects in that class as well as the background colour. When an image
has a shape or background colour similar to that class, there is a high
probability that it will be classified as such.</p>
<h2 id="svm部分参考链接">SVM部分参考链接:</h2>
<ol type="1">
<li><p>cs231n官网: <a
target="_blank" rel="noopener" href="https://cs231n.github.io/">https://cs231n.github.io/</a></p></li>
<li><p>深度学习课程 CS231n Assignment1 SVM部分: <a
target="_blank" rel="noopener" href="http://marvolo.top/archives/17202">http://marvolo.top/archives/17202</a></p></li>
<li><p>CS231-Multi-calss SVM的求导: <a
target="_blank" rel="noopener" href="https://www.cnblogs.com/chenyusheng0803/p/10018306.html">https://www.cnblogs.com/chenyusheng0803/p/10018306.html</a></p></li>
<li><p>机器学习算法：梯度下降法——原理篇 <a
target="_blank" rel="noopener" href="https://www.jianshu.com/p/424b7b70df7b">https://www.jianshu.com/p/424b7b70df7b</a></p></li>
<li><p>CS231N作业详解零基础版： <a
target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV19z411b7u9/?p=9&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a">https://www.bilibili.com/video/BV19z411b7u9/?p=9&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a</a></p></li>
</ol>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%BB%8B%E7%BB%8D/">介绍</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/cs231n/">cs231n</a><a class="post-meta__tags" href="/tags/%E4%BD%9C%E4%B8%9A/">作业</a></div><div class="post_share"><div class="social-share" data-image="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/cs231n_assignment1.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/12/31/FEDformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><img class="prev-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">FEDformer论文阅读</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8A%EF%BC%89/"><img class="next-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Transformer解析，适合没有NLP基础的小白入门 （上）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2020/02/11/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" title="我的第一篇博客"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/background/wuming.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2020-02-11</div><div class="title">我的第一篇博客</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">龚泽颖</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/serika-onoe" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:zgong313@connect.hkust-gz.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">雪融化了，是春天！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#cs231n-assignment-1"><span class="toc-number">1.</span> <span class="toc-text">CS231n Assignment 1</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#k-nearest-neighbor-knn"><span class="toc-number">2.</span> <span class="toc-text">k-Nearest Neighbor (kNN)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#k_nearest_neighbor.py"><span class="toc-number">2.1.</span> <span class="toc-text">k_nearest_neighbor.py</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">2.1.1.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E8%AE%A1%E7%AE%97%E6%B5%8B%E8%AF%95%E5%9B%BE%E5%83%8F%E5%92%8C%E6%89%80%E6%9C%89%E8%AE%AD%E7%BB%83%E5%9B%BE%E5%83%8F%E7%9A%84l2%E8%B7%9D%E7%A6%BB"><span class="toc-number">2.1.2.</span> <span class="toc-text">预测：计算测试图像和所有训练图像的L2距离</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%8C%E9%87%8D%E5%BE%AA%E7%8E%AF%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.1.2.1.</span> <span class="toc-text">双重循环实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E5%BE%AA%E7%8E%AF%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.1.2.2.</span> <span class="toc-text">单循环实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A0%E5%BE%AA%E7%8E%AF%E5%AE%9E%E7%8E%B0"><span class="toc-number">2.1.2.3.</span> <span class="toc-text">无循环实现</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B%E6%A0%B9%E6%8D%AEk%E4%B8%AA%E6%9C%80%E9%82%BB%E8%BF%91%E8%B7%9D%E7%A6%BB%E4%B8%AD%E7%9A%84%E5%A4%9A%E6%95%B0%E7%A1%AE%E5%AE%9A%E6%A0%87%E7%AD%BE"><span class="toc-number">2.1.3.</span> <span class="toc-text">预测：根据K个最邻近距离中的多数确定标签</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#knn.ipynb"><span class="toc-number">2.2.</span> <span class="toc-text">knn.ipynb</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#inline-question-1"><span class="toc-number">2.2.1.</span> <span class="toc-text">Inline Question 1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#inline-question-2"><span class="toc-number">2.2.2.</span> <span class="toc-text">Inline Question 2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#inline-question-3"><span class="toc-number">2.2.3.</span> <span class="toc-text">Inline Question 3</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#knn%E9%83%A8%E5%88%86%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">2.3.</span> <span class="toc-text">KNN部分参考链接:</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#support-vector-machine-svm"><span class="toc-number">3.</span> <span class="toc-text">Support Vector Machine (SVM)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#svm%E7%AE%80%E4%BB%8B"><span class="toc-number">3.1.</span> <span class="toc-text">SVM简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F%E8%AF%B4%E6%98%8E"><span class="toc-number">3.1.1.</span> <span class="toc-text">公式说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E7%82%B9"><span class="toc-number">3.1.2.</span> <span class="toc-text">注意点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#linear_svm.py"><span class="toc-number">3.2.</span> <span class="toc-text">linear_svm.py</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E6%B1%82%E8%A7%A3"><span class="toc-number">3.2.1.</span> <span class="toc-text">循环求解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%91%E9%87%8F%E5%8C%96%E5%AE%9E%E7%8E%B0"><span class="toc-number">3.2.2.</span> <span class="toc-text">向量化实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#linear_classifier.py"><span class="toc-number">3.3.</span> <span class="toc-text">linear_classifier.py</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83-1"><span class="toc-number">3.3.1.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%B5%8B"><span class="toc-number">3.3.2.</span> <span class="toc-text">预测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#inline-question-1-1"><span class="toc-number">3.3.3.</span> <span class="toc-text">Inline Question 1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#inline-question-2-1"><span class="toc-number">3.3.4.</span> <span class="toc-text">Inline question 2</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#svm%E9%83%A8%E5%88%86%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">3.4.</span> <span class="toc-text">SVM部分参考链接:</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （下）"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer解析，适合没有NLP基础的小白入门 （下）"/></a><div class="content"><a class="title" href="/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （下）">Transformer解析，适合没有NLP基础的小白入门 （下）</a><time datetime="2023-01-11T09:00:10.000Z" title="发表于 2023-01-11 17:00:10">2023-01-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/31/FEDformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="FEDformer论文阅读"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="FEDformer论文阅读"/></a><div class="content"><a class="title" href="/2022/12/31/FEDformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="FEDformer论文阅读">FEDformer论文阅读</a><time datetime="2022-12-31T03:47:23.000Z" title="发表于 2022-12-31 11:47:23">2022-12-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/20/CS231n-Assignment-1/" title="CS231n Assignment 1 (Updating)"><img src="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/cs231n_assignment1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CS231n Assignment 1 (Updating)"/></a><div class="content"><a class="title" href="/2022/12/20/CS231n-Assignment-1/" title="CS231n Assignment 1 (Updating)">CS231n Assignment 1 (Updating)</a><time datetime="2022-12-20T14:49:13.000Z" title="发表于 2022-12-20 22:49:13">2022-12-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8A%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （上）"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer解析，适合没有NLP基础的小白入门 （上）"/></a><div class="content"><a class="title" href="/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8A%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （上）">Transformer解析，适合没有NLP基础的小白入门 （上）</a><time datetime="2022-12-14T01:42:13.000Z" title="发表于 2022-12-14 09:42:13">2022-12-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/" title="强化学习初探"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="强化学习初探"/></a><div class="content"><a class="title" href="/2022/12/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/" title="强化学习初探">强化学习初探</a><time datetime="2022-12-12T15:17:01.000Z" title="发表于 2022-12-12 23:17:01">2022-12-12</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 龚泽颖</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script defer src="https://cdn.jsdelivr.net/gh/CodeByZach/pace/pace.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>