<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4） | 空之影的技术博客</title><meta name="author" content="空之影"><meta name="copyright" content="空之影"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="导航 这篇文章是关于Transformer的原理详细解读。  本博客的 Transformer 系列文章共计四篇，导航如下：   Transformer 零基础解析教程，从Encoder-Decoder架构说起（1&#x2F;4） Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2&#x2F;4）本篇 Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2&#x2F;4）">
<meta property="og:url" content="https://serika-onoe.github.io/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/index.html">
<meta property="og:site_name" content="空之影的技术博客">
<meta property="og:description" content="导航 这篇文章是关于Transformer的原理详细解读。  本博客的 Transformer 系列文章共计四篇，导航如下：   Transformer 零基础解析教程，从Encoder-Decoder架构说起（1&#x2F;4） Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2&#x2F;4）本篇 Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg">
<meta property="article:published_time" content="2023-01-11T09:00:10.000Z">
<meta property="article:modified_time" content="2023-02-14T15:50:33.992Z">
<meta property="article:author" content="空之影">
<meta property="article:tag" content="科研">
<meta property="article:tag" content="transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://serika-onoe.github.io/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"W5J7GO3HT8","apiKey":"53b7d5d068605c78d1a20d7f3671629a","indexName":"test_serika","hits":{"per_page":3},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-14 23:50:33'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/CodeByZach/pace/themes/green/pace-theme-flash.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="空之影的技术博客" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-language"></i><span> English</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">空之影的技术博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-language"></i><span> English</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-11T09:00:10.000Z" title="发表于 2023-01-11 17:00:10">2023-01-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-14T15:50:33.992Z" title="更新于 2023-02-14 23:50:33">2023-02-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0-%E6%95%99%E7%A8%8B/">笔记, 教程</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="导航">导航</h1>
<p><strong>这篇文章是关于Transformer的原理详细解读。</strong></p>
<blockquote>
<p>本博客的 Transformer 系列文章共计四篇，导航如下：</p>
</blockquote>
<ul>
<li><p><a
href="https://serika-onoe.github.io/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/">Transformer
零基础解析教程，从Encoder-Decoder架构说起（1/4）</a></p></li>
<li><p><a
href="https://serika-onoe.github.io/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/">Transformer
零基础解析教程，剥洋葱般层层剖析内在原理（2/4）<strong>本篇</strong></a></p></li>
<li><p><a
href="https://serika-onoe.github.io/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/">Transformer
零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）</a></p></li>
<li><p><a
href="https://serika-onoe.github.io/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/">Transformer
零基础解析教程，完整版代码最终挑战（4/4）</a></p></li>
</ul>
<h1 id="transformer之前的翻译模型">Transformer之前的翻译模型</h1>
<p>在Transformer之前，递归神经网络(RNN)一直是处理序列数据的首选方法，大家做机器翻译用的最多的就是基于RNN的Encoder-Decoder模型。</p>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-02.jpg" />
<center>
<code>图1: RNN的工作方式</code>
</center>
<p>输入:</p>
<ul>
<li><p>输入向量 <span class="math inline">\(\vec{x_t}\)</span>
(编码词)</p></li>
<li><p>隐藏状态向量 <span
class="math inline">\(\vec{h_{t-1}}\)</span>（包含当前块之前的序列状态)</p></li>
</ul>
<p>输出：</p>
<ul>
<li>输出向量 <span class="math inline">\(\vec{o_t}\)</span></li>
</ul>
<p>权重：</p>
<ul>
<li><p><span class="math inline">\({W}\)</span>—— <span
class="math inline">\(\vec{x_t}\)</span> 和 <span
class="math inline">\(\vec{h_t}\)</span> 之间的权重</p></li>
<li><p><span class="math inline">\({V}\)</span>—— <span
class="math inline">\(\vec{ h_{t-1} }\)</span> 和 <span
class="math inline">\(\vec{h_t}\)</span> 之间的权重</p></li>
<li><p><span class="math inline">\({U}\)</span>—— <span
class="math inline">\(\vec{h_t}\)</span> 和 <span
class="math inline">\(\vec{o_t}\)</span> 之间的权重</p></li>
</ul>
<p>RNN的工作方式类似于前馈神经网络，它会将输入序列一个接一个地读取。因此在基于RNN的Encoder-Decoder模型中，编码器的目标是从顺序输入中提取数据，并将其编码为向量（即输入的表示形式）。而解码器代替输出固定长度向量的分类器，与单独使用输入中的每个符号的编码器一样，解码器在多个时间步长内生成每个输出符号。</p>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-03.jpg" />
<center>
<code>图2: Encoder-Decoder进行英-法翻译的例子</code>
</center>
<p><br/><br/>
例如，在机器翻译中，输入是英文句子，输出是翻译出的法语句子。Encoder将按顺序展开每个单词，并形成输入英文句子的固定长度向量表示（也就是上篇博客中的<span
class="math inline">\(C\)</span>）。然后Decoder将固定长度的向量表示作为输入，依次产生每个法语单词，形成翻译后的法语句子。</p>
<h2 id="原有模型的缺陷">原有模型的缺陷</h2>
<p>原有的模型，即基于RNN的Encoder-Decoder存在一些问题：</p>
<ol type="1">
<li><p>训练速度慢：输入数据需要一个接一个地顺序处理，这种串行的循环过程不适用于擅长并行计算的GPU。</p></li>
<li><p>难以处理长序列：</p>
<ul>
<li>如果输入序列太长，会出现梯度消失和爆炸问题。一般在训练过程中会在loss中看到NaN（Not
a Number）。这些也称为 RNN 中的长期依赖问题。</li>
<li>上下文向量长度固定，使用固定长度的向量表示输入序列来解码一个全新的句子是很困难的。如果输入序列很大，则上下文向量无法存储所有信息。此外，也很难区分具有相似单词但具有不同含义的句子。</li>
</ul></li>
</ol>
<p>第1个问题很好理解，不过第2个问题中，关于梯度消失和爆炸的问题，可以先看看以下的补充说明。</p>
<h3 id="什么是梯度消失和梯度爆炸">什么是梯度消失和梯度爆炸</h3>
<p>高中数学有教过，我们可以利用微分的方法来求函数的最大值与最小值。在机器学习中，梯度是一个向量，它表示网络误差函数关于所有权重的偏导数。梯度优化算法就是<strong>通过不断计算梯度，并使用梯度优化算法来调整权重，使得代价函数Cost
Function (预测结果究竟与实际答案差了多少)
越来越小</strong>，也意味着网络能够更好地适应训练数据。当网络的Cost
Function达到最小值时，网络就能对新的数据进行较好的预测。</p>
<p>当我们的代价函数是线性函数时，我们就能够用梯度下降法(Gradient
Descent)来快速的求出代价函数（在图中记为<span
class="math inline">\(J(w)\)</span>）的最小值，如图：</p>
<img src="https://i.imgur.com/EFs14Nt.png" />
<center>
<code>图3: 梯度下降法的示意图</code>
</center>
<p><br/><br/>
而梯度消失和梯度爆炸是深度神经网络训练中的两种典型问题。</p>
<p><strong>梯度消失（vanishing gradient）</strong>：
指在深层网络训练中，由于梯度的较小值逐层传递，导致较深层的权值参数的更新量非常小，趋近于0。这样会导致较深层网络的参数无法得到有效更新，从而使整个网络无法学习。</p>
<p><strong>梯度爆炸（exploding gradient）</strong>：
指在深层网络训练中，由于梯度的较大值逐层传递，导致较深层的权值参数的更新量非常大，甚至无限大。这样会导致较深层网络的参数更新量过大，从而使整个网络无法学习。</p>
<p>如果有疑问，请参考下面RNN的消失和爆炸的原理说明：</p>
<h3 id="rnn梯度消失和爆炸的原理">RNN梯度消失和爆炸的原理</h3>
<p><strong>RNN的统一定义为</strong></p>
<p><span class="math display">\[
\begin{equation}h_t = f\left(x_t, h_{t-1};\theta\right)\end{equation}
\]</span></p>
<ul>
<li><span
class="math inline">\(h_t\)</span>是每一步的输出，也就是隐藏状态，由当前输入<span
class="math inline">\(x_t\)</span>和前一时刻的输出<span
class="math inline">\(h_{t-1}\)</span>共同决定</li>
<li><span class="math inline">\(\theta\)</span>则是可训练的参数</li>
</ul>
<p>(在做基本分析时，我们可以假设<span
class="math inline">\(h_t,x_t,\theta\)</span>都是一维的，这可以让我们获得最直观的理解，其结果对高维情形仍有参考价值。)</p>
<p><strong>RNN梯度的表达式为</strong></p>
<p><span class="math display">\[
\begin{equation}\frac{d h_t}{d\theta} = \frac{\partial h_t}{\partial
h_{t-1}}\frac{d h_{t-1}}{d\theta} + \frac{\partial h_t}{\partial
\theta}\end{equation}
\]</span></p>
<p>这个公式的意思是，我们可以递推地计算出每一个时间步的隐藏状态对于参数的偏导数，也就是梯度。这样就可以用梯度下降算法来更新网络中的参数，使得网络能够更好地适应训练数据。</p>
<p>可以看到，其实RNN的梯度也是一个RNN，当前时刻梯度<span
class="math inline">\(\frac{d h_t}{d\theta}\)</span>是前一时刻梯度<span
class="math inline">\(\frac{d
h_{t-1}}{d\theta}\)</span>与当前运算梯度<span
class="math inline">\(\frac{\partial h_t}{\partial
\theta}\)</span>的函数。同时，从上式我们就可以看出，其实梯度消失或者梯度爆炸现象几乎是必然存在的：</p>
<ul>
<li>当<span class="math inline">\(\left|\frac{\partial h_t}{\partial
h_{t-1}}\right| &lt;
1\)</span>时，意味着历史的梯度信息逐步衰减，因此步数多了梯度必然消失（好比<span
class="math inline">\(\lim\limits_{n\to\infty} 0.9^n \to
0\)</span>）；</li>
<li>当<span class="math inline">\(\left|\frac{\partial h_t}{\partial
h_{t-1}}\right| &gt;
1\)</span>时，意味着历史的梯度信息逐步增强，因此步数多了梯度必然爆炸（好比<span
class="math inline">\(\lim\limits_{n\to\infty} 1.1^n \to
\infty\)</span>）</li>
<li>也有可能有些时刻大于1，有些时刻小于1，最终稳定在1附近，但这样概率很小，需要很精巧地设计模型和参数才行。</li>
</ul>
<img
src="https://editor.analyticsvidhya.com/uploads/51317greatlearning.png" />
<center>
<code>图4: 梯度爆炸和梯度消失的示意图</code>
</center>
<p><br/><br/></p>
<blockquote>
<p>提问：梯度消失就是梯度变成零吗？</p>
</blockquote>
<p>并不是，我们刚刚说梯度消失是<span
class="math inline">\(\left|\frac{\partial h_t}{\partial
h_{t-1}}\right|\)</span>一直小于1，历史梯度不断衰减，但不意味着总的梯度就为0了，具体来说，一直迭代下去，我们有</p>
<p><span class="math display">\[
\begin{equation}\begin{aligned}\frac{d h_t}{d\theta} =&amp;
\frac{\partial h_t}{\partial h_{t-1}}\frac{d h_{t-1}}{d\theta} +
\frac{\partial h_t}{\partial \theta}\\
=&amp; \frac{\partial h_t}{\partial \theta}+\frac{\partial h_t}{\partial
h_{t-1}}\frac{\partial h_{t-1}}{\partial \theta}+\frac{\partial
h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial
h_{t-2}}\frac{\partial h_{t-2}}{\partial \theta}+\dots\\
\end{aligned}\end{equation}
\]</span></p>
<p>显然，其实只要<span class="math inline">\(\frac{\partial
h_t}{\partial
\theta}\)</span>不为0，那么总梯度为0的概率其实是很小的；但是一直迭代下去的话，那么<span
class="math inline">\(\frac{\partial h_1}{\partial
\theta}\)</span>这一项前面的稀疏就是<span
class="math inline">\(t-1\)</span>项的连乘<span
class="math inline">\(\frac{\partial h_t}{\partial
h_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}}\cdots\frac{\partial
h_2}{\partial
h_1}\)</span>，如果它们的绝对值都小于1，那么结果就会趋于0，这样一来，<span
class="math inline">\(\frac{d
h_t}{d\theta}\)</span>几乎就没有包含最初的梯度<span
class="math inline">\(\frac{\partial h_1}{\partial
\theta}\)</span>的信息了。</p>
<p>这才是RNN中梯度消失的含义：<strong>距离当前时间步越长，那么其反馈的梯度信号越不显著，最后可能完全没有起作用。</strong>这就意味着RNN对长距离语义的捕捉能力失效了。说白了，优化过程都跟长距离的反馈没关系，那我们怎么保证学习出来的模型能有效捕捉长距离呢？</p>
<p>所以对于一般的RNN模型来说，步数多了，梯度消失或爆炸几乎都是不可避免的，我们只能通过让RNN执行有限的步数来缓解这个问题。直到上世纪末提出的LSTM极大地改进了这个问题。</p>
<h3 id="针对梯度消失爆炸的改进lstm">针对梯度消失/爆炸的改进：LSTM</h3>
<p><strong>[论文1]:</strong> <a
target="_blank" rel="noopener" href="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">Hochreiter,
Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neural
computation 9.8 (1997): 1735-1780.</a> 引入了长短期记忆 (LSTM)
网络，其明确设计用于避免长期依赖问题。每个LSTM单元允许过去的信息跳过当前单元的所有处理并移动到下一个单元；这允许内存保留更长时间，并使数据能够不变地与其一起流动。LSTM
由一个决定要存储哪些新信息的输入门和一个决定要删除哪些信息的遗忘门组成。</p>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-05.jpg" />
<center>
<code>图5: LSTM的工作方式</code>
</center>
<p><br/><br/> 当然，LSTM 具有改进的记忆力，能够处理比 RNN
更长的序列。然而，由于LSTM更加复杂，使得LSTM与RNN相比运行更慢。</p>
<h3
id="针对上下文向量长度固定的改进attention">针对上下文向量长度固定的改进：Attention</h3>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-06.jpg" />
<center>
<code>图6: 长序列输入到原有模型的例子</code>
</center>
<p><br/><br/>
假设有一段长文本（输入），将其记忆下来（转换为固定长度的向量），然后在不回顾这段文本的情况下，按顺序翻译出整段文本（输出）。这很难，也不是我们的目标做法。相反，当我们翻译一句话时，我们会一部分一部分地看，逐段关注句子的某一部分，从而保证翻译的准确性，这就引入了下文所提到的Attention机制。</p>
<p><strong>[论文2]</strong>: <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Dzmitry Bahdanau, Kyunghyun Cho,
Yoshua Bengio: “Neural Machine Translation by Jointly Learning to Align
and Translate”, 2014; [http://arxiv.org/abs/1409.0473
arXiv:1409.0473].</a>
提出了一种在编码器-解码器模型中搜索与预测目标词相关的源句子部分的方法，也就是
Attention
机制，我们可以使用Attention机制翻译相对较长的句子而不影响其性能。例如，翻译成“noir”（在法语中意为“黑色”），注意力机制将关注单词可能的“black”，而忽略句子中的其他单词。</p>
<img src="https://i.imgur.com/5y6SCvU.png" />
<center>
<code>图7: Attention机制让输出单词关注相关的输入部分</code>
</center>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-07.jpg" />
<center>
<code>图8: Bahadanau的论文模型在测试集上长序列的 BLEU 分数明显更优</code>
</center>
<p><br/><br/>
由此可见，Attention机制提高了编码器-解码器网络的性能，但速度瓶颈仍然是RNN必须逐字处理的工作机制。</p>
<p>很自然的，我们会接着考虑：<strong>可以用更好的模型替换掉RNN这种顺序结构的模型吗？</strong></p>
<p>答案是：<strong>Yes, attention is all you need!</strong></p>
<p>哈哈，有点一语双关的感觉（说不定Google
Brain团队当时起名的时候就是这么想的）。在2017年，我们得到了一个令人满意的答案，一款名为Transformer的“神器”横空出世，至今风头不减当年。它也是我们这篇文章要探讨的主题。</p>
<h3 id="transformer诞生对标rnn">Transformer诞生，对标RNN</h3>
<p><strong>[论文3]</strong>: <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Ashish Vaswani, Noam Shazeer,
Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, Illia Polosukhin: “Attention Is All You Need”, 2017;
[http://arxiv.org/abs/1706.03762 arXiv:1706.03762].</a>
第一次正式介绍了一款在翻译领域超越了RNN的新模型Transformer，<strong>Transformer是一种Encoder-Decoder架构，使用Attention机制来处理输入和生成输出。</strong></p>
<blockquote>
<p>We propose a new simple network architecture, the
<strong>Transformer</strong>, based solely on <strong>attention
mechanisms</strong>, dispensing with recurrence and convolutions
entirely.</p>
<p><em>Reference: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I. (2017). </em><br>
<em>Attention is all you need.</em></p>
</blockquote>
<p>Transformer在定义上就表明，它抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。其实更准确地讲，<strong>Transformer由且仅由Self
Attenion和Feed Forward Neural
Network组成</strong>。采用Attention机制是因为考虑到RNN（或者LSTM，GRU等）的计算限制是顺序处理的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：</p>
<ol type="1">
<li><p>时间<span class="math inline">\(t\)</span>的计算依赖<span
class="math inline">\(t-1\)</span>时刻的计算结果，这样限制了模型的并行能力。</p></li>
<li><p>顺序计算的过程中RNN会因为长期依赖导致信息丢失问题。</p></li>
</ol>
<p>Transformer的提出解决了上面两个问题：</p>
<ol type="1">
<li><p>不采用类似RNN的顺序结构，而是具有并行性，符合现有的GPU框架。</p></li>
<li><p>使用了Attention机制，将序列中的任意两个位置之间的距离缩小为一个常量，采用了和LSTM不同的思路，从而允许处理不同长度的输入序列，并且摆脱了长期依赖的影响。</p></li>
</ol>
<p>这也就是为什么它在机器翻译任务中打败了以前基于RNN的Encoder-Decoder模型，并且在其他应用领域也非常受欢迎的原因。</p>
<h1 id="transformer原理">Transformer原理</h1>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg" width = "50%" />
<center>
<code>图9: Transformer模型的整体架构</code>
</center>
<p>上图是Transformer的整体架构图，结构上看起来和Encoder-Decoder模型很相似，左边是Encoder部分，右边是Decoder部分。为了方便理解，下面把Transformer分成四个部分进行详细说明:</p>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts.jpg" width = "50%" />
<center>
<code>图10: Transformer模型可分为4个组成部分</code>
</center>
<p>简单介绍一下各部分的任务：</p>
<ul>
<li><p><strong>Input</strong>：输入是单词的Embedding再加上位置编码，然后进入编码器或解码器。</p></li>
<li><p><strong>Encoder</strong>：这个结构可以循环很多次（N次），也就是说有很多层（N层）。每一层又可以分成Attention层和全连接层，再额外加了一些处理，比如Skip
Connection，做跳跃连接，然后还加了Normalization层。其实它本身的模型还是很简单的。</p></li>
<li><p><strong>Decoder</strong>：同样可以循环N次，第一次输入是前缀信息，之后的就是上一次产出的Embedding，加入位置编码，然后进入一个可以重复很多次的模块。该模块可以分成三块来看，第一块也是Attention层，第二块是Cross
Attention，不是Self
Attention，第三块是全连接层。也用了跳跃连接和Normalization。</p></li>
<li><p><strong>Output</strong>：最后的输出要通过Linear层（全连接层），再通过Softmax做预测。</p></li>
</ul>
<p>我们可以对Transformer的工作过程1~6进行可视化，如下所示:</p>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_process.png" width = "80%" />
<center>
<code>图11: 图解Transformer的工作过程</code>
</center>
<p><br>
以英-中翻译为例：假设我们输入<code>"Why do we work?"</code>，输出可以是<code>"为什么我们要工作？"</code>。那么Transformer的工作步骤是：</p>
<ol type="1">
<li>输入自然语言序列到编码器: Why do we work?</li>
<li>编码器先输出隐藏层, 再输入到解码器;</li>
<li>输入<span
class="math inline">\(&lt;start&gt;\)</span>(起始)符号到解码器;</li>
<li>得到第一个字"为";</li>
<li>将得到的第一个字"为"落下来再输入到解码器;</li>
<li>得到第二个字"什";</li>
<li>将得到的第二字再落下来,
重复5、6步的相关动作依次生成“么”、“我”、“们”、“要”、“工”、“作”、“
？”，直到解码器输出<span
class="math inline">\(&lt;end&gt;\)</span>(终止符),
则代表序列生成完成。</li>
</ol>
<h2 id="input">Input</h2>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(1).png" width = "100%" />
<center>
<code>图12: Transformer模型的输入部分</code>
</center>
<p>我们同样以上篇采用过的英-中翻译例子，我们输入"Tom chase
Jerry"，期待输出的翻译结果为"汤姆追逐杰瑞"。</p>
<p>Transformer的Decoder的输入处理方法与Encoder的输入处理方法步骤是相似的，但不完全一样。Encoder接受源语言数据，并通过Self-Attention和Feed-Forward网络进行编码；Decoder接受目标语言数据，并在Self-Attention、Encoder-Decoder
Attention和Feed-Forward网络的基础上进行解码。在有监督训练时，Decoder的输入包括Target
Embedding和一个Masked Multi-Head
Attention，用于生成目标语言的下一个词。输出Embedding在预测时用于生成下一个目标词。那么下面，以Encoder为例，描述输入过程和细节分析：</p>
<ol type="1">
<li><p>首先，向Transformer输入文本 <code>"Tom chase Jerry"</code>
。</p></li>
<li><p>随后，Transformer会将原始的英文句子"Tom chase
Jerry"进行分词(tokenization)，比如得到单词序列['&lt;START&gt;', 'Tom',
'chase', 'Jerry',
'&lt;END&gt;']。请注意，分词后的token包括'&lt;START&gt;'和'&lt;END&gt;'标记。</p></li>
<li><p>接下来，将每个单词映射到对应的词向量上。<strong>实际上Transformer使用的是512维的向量</strong>，那么假设我们使用4维的词向量表示单词，那么对于单词'&lt;START&gt;',
'Tom', 'chase', 'Jerry', '&lt;END&gt;'，它们的词向量可能是：</p></li>
</ol>
<p>        <span class="math inline">\(v_{&lt;START&gt;} :
[-0.1,0.2,-0.3,0.4]\)</span></p>
<p>        <span class="math inline">\(v_{Tom} : [0.5, 0.2, -0.1,
0.3]\)</span></p>
<p>        <span class="math inline">\(v_{chase} : [-0.2, 0.4, 0.1,
0.6]\)</span></p>
<p>        <span class="math inline">\(v_{Jerry} : [-0.2, 0.3, 0.1,
-0.5]\)</span></p>
<p>        <span class="math inline">\(v_{&lt;END&gt;} :
[0.3,-0.2,0.1,-0.4]\)</span></p>
<ol start="4" type="1">
<li>通过使用sin和cos函数来生成位置向量，这种方式可以向模型描述各个单词之间的顺序关系，并且能够在维度空间上均匀分布位置向量。可以假设'<START>'的位置编号为1，"Tom"的位置编号为2，"chase"的位置编号为3，"Jerry"的位置编号为4，"<END>"的位置编号为5。那么它们位置向量可能是：</li>
</ol>
<p>        <span class="math inline">\(p_1 =
[sin(\frac{1}{10000^{2*\frac{1}{4}}}),
cos(\frac{1}{10000^{2*\frac{1}{4}}}),sin(\frac{2}{10000^{2*\frac{1}{4}}}),
cos(\frac{2}{10000^{2*\frac{1}{4}}}) ]\)</span></p>
<p>        <span class="math inline">\(p_2 =
[sin(\frac{2}{10000^{2*\frac{1}{4}}}),
cos(\frac{2}{10000^{2*\frac{1}{4}}}),sin(\frac{3}{10000^{2*\frac{1}{4}}}),
cos(\frac{3}{10000^{2*\frac{1}{4}}}) ]\)</span></p>
<p>        <span class="math inline">\(p_3 =
[sin(\frac{3}{10000^{2*\frac{1}{4}}}),
cos(\frac{3}{10000^{2*\frac{1}{4}}}),sin(\frac{4}{10000^{2*\frac{1}{4}}}),
cos(\frac{4}{10000^{2*\frac{1}{4}}}) ]\)</span></p>
<p>        <span class="math inline">\(p_4 =
[sin(\frac{4}{10000^{2*\frac{1}{4}}}),
cos(\frac{4}{10000^{2*\frac{1}{4}}}),sin(\frac{5}{10000^{2*\frac{1}{4}}}),
cos(\frac{5}{10000^{2*\frac{1}{4}}}) ]\)</span></p>
<p>        <span class="math inline">\(p_5 =
[sin(\frac{5}{10000^{2*\frac{1}{4}}}),
cos(\frac{5}{10000^{2*\frac{1}{4}}}),sin(\frac{6}{10000^{2*\frac{1}{4}}}),
cos(\frac{6}{10000^{2*\frac{1}{4}}}) ]\)</span></p>
<ol start="5" type="1">
<li>最后，输入到Transformer中的序列就是由词向量和位置向量相加得到的，例如，“Tom
chase Jerry”的输入序列可能是:</li>
</ol>
<p>        [<span class="math inline">\(v_ {&lt;START&gt;}\)</span> +
<span class="math inline">\(p_1\)</span> , <span
class="math inline">\(v_{Tom}\)</span> + <span
class="math inline">\(p_2\)</span>, <span
class="math inline">\(v_{chase}\)</span> + <span
class="math inline">\(p_3\)</span>, <span
class="math inline">\(v_{Jerry}\)</span> + <span
class="math inline">\(p_4\)</span>, <span class="math inline">\(v_
{&lt;END&gt;}\)</span> + <span class="math inline">\(p_5\)</span>]</p>
<p>下面分别介绍过程中相关的知识点：</p>
<h3 id="分词tokenization">分词(Tokenization)</h3>
<blockquote>
<p>什么是分词</p>
</blockquote>
<p>Transformer
模型的输入通常是序列数据，如文本、语音等。这些数据在输入之前需要进行预处理，其中一个重要的步骤就是分词。<strong>分词是获取词向量之前的一个必要步骤。</strong></p>
<p>分词是NLP的一个重要概念，<strong>表示将文本(text)切分成符号(token)的过程。</strong>token可以是以下三种类型：</p>
<ol type="1">
<li><p>单词 (word) —— 例如，短语“dogs like
cats”由三个词标记组成：“dogs”、“like”和“cats”。</p></li>
<li><p>字符 (character) —— 例如，短语“your
fish”由九个字符标记组成。<strong>（请注意，空格算作标记之一）</strong></p></li>
<li><p>子词 (subword) ——
其中单个词可以是单个标记或多个标记。子词由词根、前缀或后缀组成。例如，使用子词作为标记的语言模型可能会将单词“dogs”视为两个标记（词根“dog”和复数后缀“s”）。相同的语言模型可能会将单个词“更高”视为两个子词（词根“high”和后缀“er”）。</p></li>
</ol>
<blockquote>
<p>分词的作用？</p>
</blockquote>
<p>通过分词，模型可以将文本分成若干个单独的词汇单元，可以减少翻译系统需要处理的信息量，提高翻译效率和准确性，同时更好地维护语言的语法结构。</p>
<h3 id="嵌入embedding">嵌入(Embedding)</h3>
<p>NLP中，<strong>使用分词后的词向量作为模型输入</strong>是常见的做法，而词向量是一种特定类型的嵌入。</p>
<blockquote>
<p>什么是嵌入？</p>
</blockquote>
<p>嵌入是NLP中使用的一种技术，以机器学习模型能够理解的数字格式表示单词、短语甚至整个句子。</p>
<blockquote>
<p>嵌入的作用？</p>
</blockquote>
<p>嵌入的目标是<strong>通过机器学习模型能够理解的方式捕捉词的含义和上下文，并将词语的语义信息转化为数字，使得它们可以被计算机理解。</strong>从另一种角度来看，嵌入也可以被视为一种降维形式，将高维的单词表示转化为低维的词向量。</p>
<p>举个例子，此前对于单词或句子，很容易想到类似 one-hot
的编码向量表示方法，一个单词或句子在向量中仅被表示为一个非零元素，其他元素都是零，从而将单词或句子映射为高维和稀疏的向量。</p>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/onehot.png" width = "80%" />
<center>
<code>图13: 图解one hot的编码方式</code>
</center>
<p><br/><br/>
这种表示方法确实可以避免线性不可分的问题，但由于大多数元素为零，因此计算效率会低下。此外，使用
one-hot
编码表示的词向量不能很好地表示单词之间的语义关系，因此机器学习模型难以学习到单词的含义。因此，嵌入技术是更有效的选择。</p>
<p>与其相反，嵌入是一种以较低维度的密集格式表示单词或句子的方法，更适合机器学习模型。<strong>通过将单词或句子映射到一个较低的维度空间，嵌入可以捕捉到单词或句子的含义和相互关系，同时舍弃不太重要的信息。</strong></p>
<p><strong>常见的词向量编码方式是word2vec。</strong>相比于one-hot编码只允许我们将单词作为单个不同的条目来解释，word2vec允许我们寻找每个单词和其他单词的关系，从而创建更好的特征表示。</p>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/word2.png" width = "80%" />
<center>
<code>图14: 图解word2vec的编码方式</code>
</center>
<p><br/><br/>
word2vec模型使用神经网络来学习一个词的向量表示。它将每个词映射到一个高维向量，其中语义相似的词在向量空间中是紧密相连的。例如，在word2vec模型中，"banana"这个词的向量表示可能是[-0.2,
0.5, 0.1, 0.3,
...]，代表这个词的含义和上下文，(一个词向量往往是300~1000维，向量中的每个元素代表这个词的意义或上下文的一个维度，
<strong>嵌入向量的维数通常比 one-hot 编码向量的维数低得多</strong>
)。单个数字的含义本身不可解释，只有在与其他词或句子的向量相关时才有意义</p>
<blockquote>
<p>输入的embedding是否需要经过训练</p>
</blockquote>
<p>将单词<span
class="math inline">\(x\)</span>的embedding输入encoder，有两种常见的选择：</p>
<ol type="1">
<li><p>使用Pre-trained的<strong>embeddings并固化</strong>，这种情况下embedding取自一个预先训练好的模型，在训练过程中不更新。实际就是一个Lookup
Table（查找表）。<strong>这是bert（一种特殊的Transformer模型，专门用于自然语言处理任务）选择的做法。</strong></p></li>
<li><p>对其进行随机初始化（当然也可以选择Pre-trained的结果），但<strong>设为Trainable</strong>。这样在training过程中不断地对embeddings进行改进。即End2End（端到端）训练方法，意味着模型从头到尾都被训练，所有的参数，包括嵌入都在训练过程中被更新。<strong>这也是Transformer选择的做法。</strong></p></li>
<li><p>有些情况下，在Encoder的输入层之前还会使用一个词汇表（vocabulary），并且对于OOV（out-of-vocabulary）的单词使用一个特殊的embedding，例如UNK（unknown）或PAD（padding）。</p></li>
</ol>
<h3 id="位置编码positional-encoding">位置编码(Positional Encoding)</h3>
<blockquote>
<p>为什么需要知道每个单词的位置，并且添加位置编码呢？</p>
</blockquote>
<p>首先，咱们知道，一句话中同一个词如果的出现位置不同，意思可能发生翻天覆地的变化，就比如：我欠他100W
和
他欠我100W。这两句话的意思一个地狱一个天堂。可见获取词语出现在句子中的位置信息是一件很重要的事情。</p>
<p>而Transformer没有用RNN也没有卷积，它使用的注意力机制(主要是由于self
attention)，不能获取词语位置信息，就算打乱一句话中词语的位置，每个词还是能与其他词之间计算attention值。所以为了让模型能利用序列的顺序，必须输入序列中词的位置，所以Transformer采用的方法是给每一个词向量，包括包括'&lt;START&gt;'和'&lt;END&gt;'都需要添加位置编码。</p>
<blockquote>
<p>怎么得到positional encoding呢？</p>
</blockquote>
<p>Transformer使用的是正余弦位置编码。位置编码通过使用不同频率的正弦、余弦函数生成，然后和对应位置的词向量相加，位置向量维度必须和词向量的维度一致。过程如上图，PE（positional
encoding）计算公式如下：</p>
<p><span class="math display">\[PE_{(pos,2i)} =
sin(\frac{pos}{10000^{\frac{2 i}{d_{model}}}})\]</span> <span
class="math display">\[PE_{(pos,2i+1)} = cos(\frac{pos}{10000^{\frac{2
i}{d_{model}}}})\]</span></p>
<p>解释一下上面的公式：</p>
<ul>
<li><p><span
class="math inline">\(pos\)</span>表示单词在句子中的绝对位置，<span
class="math inline">\(pos=0, 1, 2, \dots\)</span>，例如：Jerry在"Tom
chase Jerry"中的pos=2；</p></li>
<li><p><span
class="math inline">\(d_{model}\)</span>表示词向量的维度，一般<span
class="math inline">\(d_{model}\)</span>=512；2i和2i+1表示奇偶性，i表示词向量中的第几维，例如这里<span
class="math inline">\(d_{model}\)</span>=512，故<span
class="math inline">\(i=0, 1, 2, \dots, 255\)</span>。</p></li>
</ul>
<p>至于上面两个公式是怎么得来的，其实不重要，很有可能是作者根据经验自己造的，而且公式也不唯一，后续Google在Bert中的采用类似词向量的方法通过训练PE，说明<strong>这种求位置向量的方法还是存在一定问题滴</strong>。</p>
<blockquote>
<p>为什么是将positional encoding与词向量相加，而不是拼接呢？</p>
</blockquote>
<p>事实上，拼接或者相加都可以，只是词向量本身的维度（512维）就已经蛮大了。再拼接一个512维的位置向量（变成1024维）这样训练起来会相对慢一些，影响学习效率。两者既然效果差不多，那当然是选择学习难度较小的相加了。</p>
<p>这段代码实现了Transformer模型中的位置编码，主要用于确定输入序列中每个单词的位置信息问题。</p>
<ul>
<li><strong>d_model</strong>: 定义词向量的维度。</li>
<li><strong>dropout</strong>:
一种正则化方式，随机让部分网络参数为0，以防过拟合。</li>
<li><strong>max_len</strong>: 输入句子的最大长度。</li>
</ul>
<p>在初始化中，首先使用nn.Dropout类创建了一个dropout层。然后根据论文中的公式，预先计算出位置编码，并将其存储在pe变量中。</p>
<p>在forward函数中，将输入x加上pe变量中对应位置的位置编码，最后进行dropout操作（一种正则化方式，在训练时会随机将部分网络参数设置为0，从而防止过拟合）。</p>
<p>需要注意的是，这里的<strong>位置编码是在计算时预先计算好了并存储下来，而不是在运行时动态计算，这样可以减少计算量</strong>。</p>
<h2 id="encoder">Encoder</h2>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(2).png" width = "40%" />
<center>
<code>图15: Transformer模型的编码器部分</code>
</center>
<p><br/><br/>
在论文中，有6层编码器，即<code>“Nx”的N=6</code>。Transformer的编码器的每一层(Encoder
layer)是由4个sub-layer（子层）组成的：</p>
<p>第一个子层是多头注意力机制 (multi-head self-attention
mechanism)，它通过计算输入序列中各个位置的关系，生成关于该位置的输入的新的表示。
第二个子层是残差连接 (residual
connection)，它将第一个子层的输出与原始输入相加，并进行归一化以维护其统计特性。
第三个子层是全连接前馈层（feed-forward
layer），它通过一个多层神经网络对第二个子层的输出进行非线性变换，从而生成新的表示。
第四个子层是残差连接 (residual
connection)，它将第三个子层的输出与原始输入相加，并进行归一化以维护其统计特性。</p>
<p>这段代码实现了一个transformer编码器中的一个编码层EncoderLayer。这个编码层由两部分组成，分别是自注意力机制self-attn和前馈网络feed_forward。在初始化时，通过传入size、self_attn、feed_forward和dropout参数来初始化编码层。</p>
<p>在前向传播过程中，首先使用自注意力机制对输入x进行处理，然后将处理后的结果经过前馈网络进一步处理，最后返回处理结果。</p>
<p>具体来说，使用 <span class="math inline">\(sublayer[0](x, lambda x:
self.self_attn(x, x, x, mask))\)</span>
调用SublayerConnection类对输入进行处理，进行自注意力机制。 之后使用
<span class="math inline">\(sublayer[1](x, self.feed_forward)\)</span>
将结果经过前馈网络进一步处理。</p>
<h3 id="multi-head-attention">Multi-Head Attention</h3>
<p><strong>Multi-Head Attention是Self
Attention机制的进一步细化，因此先从Self Attention讲起：</strong></p>
<h4 id="从self-attention讲起">从Self Attention讲起</h4>
<p>假设下面的句子是我们要翻译的输入句子：</p>
<p><code>The animal didn't cross the street because it was too tired</code></p>
<p>这句话中的“it”指的是什么？指的是街道还是动物？这对人来说是一个简单的问题，但对算法模型来说却不那么简单。</p>
<img
src="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png" />
<center>
<code>图16: 对单词“it”编码时Attention大部分集中在“The animal”上</code>
</center>
<p>在该例子中，当模型处理“it”这个词时，self attention
允许它把“it”和“animal”联系起来。而广泛地说，<strong>当模型处理每个单词（输入序列中的每个位置）时，自注意力允许它查看输入序列中的其他位置以寻找有助于更好地编码该单词的线索。</strong></p>
<p>而上篇我们详细讲解过，Attention的本质，这里我们简单描述下：<strong>Attention实际上做的就是数据库中的检索操作，本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。</strong></p>
<p>这个类似于搜索引擎或者推荐系统的基本原理，以谷歌Google为例:</p>
<ul>
<li>用户给定需查询的问题(Query)</li>
<li>Google后台有各种文章标题(Key)和文章本身(Value)</li>
</ul>
<p>通过定义并计算问题Query与文章标题Key的相关性，用户就可以找到最匹配的文章Value。当然Attention的目标不是通过Query得到Value，而是通过Query得到Value的加权和。</p>
<p>那么回到计算Self Attention的过程上来，这次我们以新的输入“Thinking
Machines”为例进行过程描述：</p>
<p><strong>单词级别-第一步：</strong></p>
<p>从每个编码器的输入向量（词向量+位置编码）创建三个向量：一个Query查询向量、一个Key键向量和一个Value值向量。</p>
<img
src="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png" /><br />

<center>
<code>图17: qkv向量是通过将embedding分别乘以训练的三类权重矩阵而创建的</code>
</center>
<p><br/><br/> 比如产生"Thinking"的三个向量的过程如下：</p>
<ol type="1">
<li>将<span class="math inline">\(X_1\)</span>乘以<span
class="math inline">\(W^Q\)</span>权重矩阵产生<span
class="math inline">\(q_1\)</span>，即与该词关联的Query向量。</li>
<li>将<span class="math inline">\(X_1\)</span>乘以<span
class="math inline">\(W^K\)</span>权重矩阵产生<span
class="math inline">\(k_1\)</span>，即与该词关联的Key向量。</li>
<li>将<span class="math inline">\(X_1\)</span>乘以<span
class="math inline">\(W^V\)</span>权重矩阵产生<span
class="math inline">\(v_1\)</span>，即与该词关联的Value向量。</li>
</ol>
<p>请注意，qkv向量的维度小于embedding向量。它们的维数是
64，而embedding和编码器输入/输出向量的维数是
512。它们不必更小，这是一种使multi-head
attention（大部分）计算保持不变的架构选择。</p>
<p><strong>单词级别-第二步</strong></p>
<p>计算每个单词的分数，分数是通过Query向量与当前正在评分单词的Key向量的点积计算得出的。<strong>当我们在特定位置对单词进行编码时，分数决定了将多少注意力放在输入句子的其他部分。</strong></p>
<p>假设我们正在计算本例中第一个词“Thinking”的自注意力。我们需要根据这个词对输入句子的每个词进行评分:</p>
<img
src="https://jalammar.github.io/images/t/transformer_self_attention_score.png" /><br />

<center>
<code>图18: 单词的分数是对应位置Query向量和Key向量的点积</code>
</center>
<p><strong>单词级别-第三步</strong></p>
<p>将分数除以8，这个数字是论文中使用的关键向量维度64的平方根。</p>
<blockquote>
<p>上式为什么要除以<span
class="math inline">\(\sqrt{d_k}\)</span>呢？</p>
</blockquote>
<p>好问题！如果不进行除以<span
class="math inline">\(\sqrt{d_k}\)</span>的操作，那么<span
class="math inline">\(QK^T\)</span>的值将取决于<span
class="math inline">\(d_k\)</span>的大小。当<span
class="math inline">\(d_k\)</span>较大时，<span
class="math inline">\(QK^T\)</span>中的元素将变得相对较大，可能会导致softmax函数的输出非常小，导致梯度消失的问题。当<span
class="math inline">\(d_k\)</span>较小时，<span
class="math inline">\(QK^T\)</span>中的元素将变得相对较小，可能会导致softmax函数的输出非常大，导致梯度爆炸的问题。为了避免这种情况，Transformer中对<span
class="math inline">\(QK^T\)</span>的每个元素都除以<span
class="math inline">\(\sqrt{d_k}\)</span>，以使得在不同<span
class="math inline">\(d_k\)</span>下，<span
class="math inline">\(QK^T\)</span>的元素大小分布更加平稳，从而更稳定地进行softmax函数的计算和反向传播。因此，除以<span
class="math inline">\(\sqrt{d_k}\)</span>是一种既简单又有效的方法来缩放注意力分数，这也是为什么Transformer中会采用这种方式。
<em>--By ChatGPT</em></p>
<blockquote>
<p>为什么是<span class="math inline">\(\sqrt{d_k}\)</span>而不是<span
class="math inline">\(d_k\)</span>呢？</p>
</blockquote>
<p>在Transformer中，除以<span
class="math inline">\(\sqrt{d_k}\)</span>是为了缩小内积的值的范围，避免softmax函数的梯度消失和梯度爆炸问题。而为什么不使用<span
class="math inline">\(d_k\)</span>作为缩放系数呢？这是因为<span
class="math inline">\(d_k\)</span>的值通常会非常大（比如说，如果使用了512维的向量，<span
class="math inline">\(d_k\)</span>就为512），这样就会导致内积的值也变得非常大，从而使得softmax函数的梯度非常小，导致梯度消失的问题。此外，除以<span
class="math inline">\(\sqrt{d_k}\)</span>可以使得每个维度对注意力分数的影响相等，这是因为所有维度上的内积被相同的因子<span
class="math inline">\(\sqrt{d_k}\)</span>缩放。这种均衡性有助于确保注意力机制可以平衡地关注每个维度上的信息。因此，使用<span
class="math inline">\(\sqrt{d_k}\)</span>作为缩放系数是一种更好的选择。<em>--By
ChatGPT</em></p>
<p><strong>单词级别-第四步</strong></p>
<p>然后通过 softmax 操作传递结果。softmax
对分数进行归一化处理，使它们都为正且加起来为 1。</p>
<p>softmax
分数决定了在这个位置上，输入句子的每个单词会被投入的注意力占比。</p>
<img
src="https://jalammar.github.io/images/t/self-attention_softmax.png" /><br />

<center>
<code>图19: 缩小维度并进行归一化处理。</code>
</center>
<p><strong>单词级别-第五步</strong></p>
<p>将每个Value向量乘以 softmax
分数（准备将它们相加）。这里的直觉是保持我们想要关注的单词的值不变，并淹没不相关的单词（例如，通过将它们乘以像
0.001
这样的小数字）。然后是对加权值向量求和，这会在该位置产生自注意层的输出。</p>
<img
src="https://jalammar.github.io/images/t/self-attention-output.png" /><br />

<center>
<code>图20: 输出第一个单词的self attention计算结果</code>
</center>
<p><br/><br/>
自注意力计算到此结束。生成的向量是我们可以发送到前馈神经网络的向量。然而，在实际实现中，此计算以矩阵形式完成。</p>
<blockquote>
<p>为什么实际要用矩阵而不是神经网络呢？</p>
</blockquote>
<p>因为矩阵运算能用GPU加速，会更快，同时参数量更少，更节省空间。</p>
<p>既然我们已经看到了单词级别的计算过程，那么让我们来看看Self
Attention实际使用的矩阵计算：</p>
<p><strong>矩阵计算-第一步</strong></p>
<p>计算Query,
Key和Value共计三个矩阵。为此，我们将嵌入打包到矩阵X中，然后将其乘以我们训练过的权重矩阵
<span class="math inline">\((W^Q、W^K、W^V)\)</span>。</p>
<img
src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png" />
<center>
<code>图21: 计算QKV矩阵</code>
</center>
<p><strong>矩阵计算-第二步</strong></p>
<p>由于我们处理的是矩阵，我们可以将单词形式的第二步到第五步压缩为一个公式来计算自注意力层的输出。</p>
<img
src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" /><br />

<center>
<code>图22: 矩阵形式的self attention计算公式</code>
</center>
<h4 id="multi-head-attention原理">Multi-head Attention原理</h4>
<p>该论文通过添加一种称为Multi-head
Attention的机制，进一步细化了自注意力层。主要体现在两个方面：</p>
<ol type="1">
<li><p>它扩展了模型关注不同位置的能力。比如要翻译像“The animal didn't
cross the street because it was too
tired”这样的句子，知道“it”和哪几个词有关会很有用。</p></li>
<li><p>它为注意力层提供了多个representation subspaces
(表示子空间)。正如我们接下来将看到的，对于Multi-head
Attention，我们有多组QKV权重矩阵，其中的每一个都是随机初始化的。训练之后，每个集合用于将输入（初始输入或来自较低编码器/解码器的向量）投影到不同的表示子空间中。</p></li>
</ol>
<img
src="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png" /><br />

<center>
<code>图23: Multi-head Attention每个头各产生不同的 QKV 矩阵</code>
</center>
<p><br/><br/></p>
<img
src="https://jalammar.github.io/images/t/transformer_attention_heads_z.png" /><br />

<center>
<code>图24: 通过相同的计算过程，8个attention head最终会得到8个不同的Z矩阵</code>
</center>
<p><br/><br/></p>
<p>这给我们带来了一些挑战。前馈层不需要8个矩阵——它需要一个矩阵（每个单词一个向量）。所以我们需要一种方法将这8个压缩成一个矩阵。因此我们连接这些矩阵，然后将它们乘以一个额外的权重矩阵
<span class="math inline">\(W^O\)</span>。</p>
<img
src="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png" /><br />

<center>
<code>图25: 将多个Z矩阵通过矩阵乘法合并成总的Z矩阵</code>
</center>
<br/><br/> <img
src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" />
<center>
<code>图26: 将每个头的输出拼接在一起并通过一个Linear层，映射成类似单头的输出</code>
</center>
<p><br/><br/> 现在我们已经谈到了attention
head，让我们重新审视我们之前的例子，看看当我们在示例句子中对单词“it”进行编码时，不同的attention
head集中在什么地方：</p>
<img
src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png" /><br />

<center>
<code>图27: 不同attention head对it的注意力权重不同</code>
</center>
<p><br/><br/> 当我们对“it”这个词进行编码时，一个attention head最关注“the
animal”，而另一个attention head则关注“tired”。这说明不同attention
head很可能从不同角度来理解it和其他单词的关系，比如it的指代的对象是“the
animal”，而这个对象所处的状态是“tired”。因为Attention是注意力的意思而不是表示相等的意思，那么从不同角度看待同一个事物，得到不同的答案自然也是没问题的。</p>
<h3 id="add-normalize">Add &amp; Normalize</h3>
<p>可以注意到编码器/解码器的每个子层（比如self attention,
ffnn）之后都带有一个 <strong>Add &amp; Normalize</strong>。</p>
<img
src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png" /><br />

<center>
<code>图28: 每个编码器中都有2个Add &amp; Normalize子层</code>
</center>
<p><br/><br/></p>
<p>我之前没有听说过残差连接，因此看着这张图好久也没看出residual这个词体现在哪里，问了“Chat老师”才明白：</p>
<ul>
<li><p>Add表示残差连接，残差连接来源于<strong>[论文4]</strong>: <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">Kaiming He, Xiangyu Zhang,
Shaoqing Ren, Jian Sun: “Deep Residual Learning for Image Recognition”,
2015; [http://arxiv.org/abs/1512.03385 arXiv:1512.03385].</a></p></li>
<li><p>Norm表示LayerNorm，LayerNorm来源于<strong>[论文5]</strong>: <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">Jimmy Lei Ba, Jamie Ryan Kiros,
Geoffrey E. Hinton: “Layer Normalization”, 2016;
[http://arxiv.org/abs/1607.06450 arXiv:1607.06450].</a>。</p></li>
</ul>
<p>Encoder端和Decoder端每个子模块实际的输出为：<span
class="math inline">\(LayerNorm(x+Sublayer(x))\)</span>，其中<span
class="math inline">\({Sublayer}(x)\)</span>为子模块的输出。这样做有助于模型更好地捕捉长期依赖关系。</p>
<p>关于这一部分的更多技术细节我以问答的形式展示在下面。</p>
<p><strong>问题一</strong></p>
<blockquote>
<p>什么是残差连接</p>
</blockquote>
<p>残差连接是一种网络设计方法，它的作用是在每一层的输入和输出之间添加一个跨层连接（shortcut
connection），并通过标准化来处理连接处的数据。在Transformer中，每个Encoder层都由一个残差连接和两个正规化步骤组成。</p>
<p>残差连接的公式是 y=LayerNorm(x+Sublayer(x))</p>
<ul>
<li>Sublayer(x)可以是任意的层，比如Multi-Head
Attention或Feed-Forward层。</li>
<li>LayerNorm是对每个样本的所有隐藏单元进行归一化，以防止过拟合和提高模型的鲁棒性。</li>
<li>其实也就是Add &amp; Normolize</li>
</ul>
<p>残差连接的好处是能够有效地减轻梯度消失问题，简化网络训练过程，并帮助网络更好地捕捉长期依赖关系。</p>
<p><strong>问题二</strong></p>
<blockquote>
<p>为什么引入残差连接</p>
</blockquote>
<img
src="https://img-blog.csdnimg.cn/94d2b8dbc1ea432c9ca85f62e29cb454.png#pic_center" />
<center>
<code>图29:  Residual learning：a building block</code>
</center>
<ul>
<li>X是这一层残差块的输入值</li>
<li>F(X)是经过第一层线性变化并激活后的输出，也称为残差</li>
<li>在第二层输出值激活前加入X，这条路径称作跨层连接（shortcut
connection）。</li>
</ul>
<p>《Deep Residual Learning for Image Recognition》论文中的Figure
2展示了残差学习的建筑块。这个建筑块由两个卷积层组成，每个卷积层后面跟着一个ReLU激活函数。在这个建筑块的最后，有一条跨越了两个卷积层的连接，它绕过了这两个卷积层，直接将输入信息传递到输出端。这个跨层连接被称为“残差连接”。</p>
<p>引入残差连接的目的就是为了防止<strong>在深度神经网络训练中发生退化问题</strong>，使得更深的网络能够更好地学习特征表达。</p>
<p><strong>问题三</strong></p>
<blockquote>
<p>什么是退化，为什么深度神经网络会发生退化？</p>
</blockquote>
<p>退化问题通常指的是：当神经网络层数增加时，网络的性能开始变差，表现为训练误差的增加和泛化误差的增加。一种可能的原因是，在网络较深的时候，由于神经网络的非线性层的存在，梯度传播变得困难，导致网络的训练变得困难。</p>
<p>而残差连接通过引入跨层连接，可以使梯度在网络中更容易地流动，从而减轻梯度消失的问题，进一步提高了网络的性能。</p>
<p>举个例子：假如某个神经网络的最优网络层数是18层，但是我们在设计的时候并不知道到底多少层是最优解，本着层数越深越好的理念，我们设计了32层，那么32层神经网络中有14层其实是多余的，我们要想达到18层神经网络的最优效果，必须保证这多出来的14层网络必须进行恒等映射，恒等映射的意思就是说，输入什么，输出就是什么，可以理解成F(x)=x这样的函数，因为只有进行了这样的恒等映射咱们才能保证这多出来的14层神经网络不会影响我们最优的效果。</p>
<p>实际上，在训练过程中，网络的参数并不是直接训练得到的。相反，网络是通过反向传播算法来更新网络的参数，因此参数的更新是从后向前进行的，要想保证训练参数能够很精确的完成F(x)=x的恒等映射其实是很困难的。这个时候大神们就提出了ResNet（残差神经网络）来解决神经网络退化的问题。</p>
<p><strong>问题四</strong></p>
<blockquote>
<p>为什么添加了残差块能防止神经网络退化问题呢？</p>
</blockquote>
<p>咱们再来看看添加了残差块后，咱们之前说的要完成恒等映射的函数变成什么样子了。是不是就变成h(X)=F(X)+X，我们要让h(X)=X，那么是不是相当于只需要让F(X)=0就可以了，这里就巧妙了！神经网络通过训练变成0是比变成X容易很多的，因为大家都知道，咱们一般初始化神经网络的参数的时候就是设置的[0,1]之间的随机数嘛。所以经过网络变换后很容易接近于0。举个例子：</p>
<p><img
src="https://img-blog.csdnimg.cn/20200326001443472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /></p>
<p>假设该网络只经过线性变换，没有bias也没有激活函数。我们发现因为随机初始化权重一般偏向于0，那么经过该网络的输出值为[0.6
0.6]，很明显会更接近与[0 0]，而不是[2
1]，相比与学习h(x)=x，模型要更快到学习F(x)=0。</p>
<p>并且ReLU能够将负数激活为0，过滤了负数的线性变化，也能够更快的使得F(x)=0。用学习残差F(x)=0更新该冗余层的参数来代替学习h(x)=x更新冗余层的参数。通过学习残差F(x)=0来让该层网络恒等映射上一层的输入，使得有了这些冗余层的网络效果与没有多余层的浅层网络相同。这样很大程度上解决了网络的退化问题。</p>
<p><strong>问题五</strong></p>
<blockquote>
<p>为什么要进行Normalize呢？</p>
</blockquote>
<p>在神经网络进行训练之前，都需要对于输入数据进行Normalize归一化，目的有二：</p>
<ol type="1">
<li><p>能够加快训练的速度。归一化能够加快训练速度的原因是可以使得数据在网络中传递时，每一层的输出值分布更加均匀，避免了在激活函数饱和区域的情况下，网络梯度消失或爆炸，从而保证了训练的速度和效果。</p></li>
<li><p>提高训练的稳定性。归一化能够提高训练的稳定性的原因是使得数据在网络中传递时，每一层的输出值都保持在一个相对小的范围内，避免了出现数据的偏移和方差过大的情况，从而减小了模型对于数据的敏感度，使得模型更加稳定。</p></li>
</ol>
<p>在Transformer中，归一化层通常在每个多头自注意力和前馈网络之间进行，这些归一化层被称为"Layer
Normalization"。它们在一个batch内的每个样本中对每个特征维度进行独立归一化，并且使用样本的均值和方差来标准化每个特征。</p>
<p><strong>问题六</strong></p>
<blockquote>
<p>为什么使用Layer Normalization（LN）而不使用Batch
Normalization（BN）呢？</p>
</blockquote>
<p>先看图，LN是在同一个样本中不同神经元之间进行归一化，而BN是在同一个batch中不同样本之间的同一位置的神经元之间进行归一化。</p>
<img
src="https://img-blog.csdnimg.cn/20200326202939489.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /><br />

<center>
<code>图30: Layer Normalization和Batch Normalization可视化</code>
</center>
<p>可以看出，相比于图像领域，NLP任务中的输入是文本数据，一般使用词向量表示，每个词向量是一个高维稠密向量。在NLP任务中，同一个batch中的不同样本可能具有相似的词向量，但每个样本的长度（即词向量的数量）不同。因此，在NLP任务中，每个词向量的不同维度可能具有不同的意义，将每个维度视为不同的特征，并对不同的特征进行归一化处理更加合理。因此，Layer
Normalization更适合NLP任务中的使用。而Batch
Normalization更适用于图像领域的任务，其中输入是具有相同空间维度的图像。</p>
<p>因此，总结原因如下：</p>
<ol type="1">
<li><p>LN更适用于序列数据：在NLP任务中，输入通常是一系列标记的序列，例如单词或字符。这种序列数据通常具有可变的长度和不同的分布。与固定大小的图像批次相比，序列的不同位置可能具有截然不同的统计属性，因此使用BN将导致不同位置的特征之间出现耦合。相比之下，LN可以更好地适应序列数据的变化。</p></li>
<li><p>LN避免了对小批次大小的依赖：在BN中，对于每个小批次，特征的均值和方差是在该小批次上计算的。对于小批次，可能会出现均值和方差计算上的不准确性，从而导致性能下降。相比之下，LN不涉及小批次的计算，而是仅使用样本的特征，这使得它对小批次大小的依赖更小。</p></li>
<li><p>LN更适用于深度网络：随着神经网络的加深，BN计算的均值和方差将越来越不可靠，这会导致性能下降。而LN则不会受到这个问题的影响，因为它在每个特征上进行归一化，而不是在整个批次上计算。</p></li>
<li><p>LN可以更好地适应动态计算图：在深度学习中，一些计算图是动态的，其中图的结构在运行时可以更改。BN的计算依赖于图的结构，因此在这些情况下可能会遇到困难。相比之下，LN只依赖于每个特征的值，因此在动态计算图的环境下更容易使用。</p></li>
</ol>
因此，我们可以可视化Add &amp; Normalize操作，如下图所示： <img
src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png" /><br />

<center>
<code>图28: 可视化Add &amp; Normalize操作</code>
</center>
<h3 id="feed-forward">Feed Forward</h3>
<p>每一层经过self attention之后，还会有一个Feed Forward
Network(FFN)，这个FFN的作用就是空间变换。FFN包含了2层linear
transformation层，中间的激活函数是ReLu。</p>
<p><span class="math display">\[FFN(x) = \max(0, xW_1 + b_1 )W_2 +
b_2\]</span></p>
<blockquote>
<p>attention层的output最后会和 <span class="math inline">\(W_O\)</span>
相乘，为什么这里又要增加一个2层的FFN网络？</p>
</blockquote>
<p>这是因为FFN的加入引入了非线性(ReLu激活函数)，变换了attention
output的空间,
从而增加了模型的表现能力。把FFN去掉模型也是可以用的，但是效果差了很多。</p>
<h2 id="decoder">Decoder</h2>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(3).png" width = "50%" />
<center>
<code>图30: Transformer模型的编码器部分</code>
</center>
<p>论文中Decoder也是N=6层堆叠的结构。被分为3个sub-layer，Encoder与Decoder有<strong>三大主要的不同</strong>：</p>
<ol type="1">
<li><p>Decoder sub-layer-1使用的是“<strong>Masked</strong>” Multi-Headed
Attention机制，<strong>防止为了模型看到要预测的数据，防止泄露</strong>。</p></li>
<li><p>sub-layer-2是一个Encoder-Decoder Multi-head Attention。</p></li>
<li><p>LinearLayer和SoftmaxLayer作用于sub-layer-3的输出后面，来预测对应的word的概率。</p></li>
</ol>
<p>如果你弄懂了Encoder部分，Decoder部分也就没有那么可怕了：</p>
<ul>
<li>输入都是 embedding + positional Encoding。</li>
<li>Decoder也是N=6层堆叠的结构。被分为3个sub-layer，具体细节方面：
<ol type="1">
<li>masked multi-head
attention：由于在机器翻译中，Decode的过程是一个顺序的过程，也就是当解码第k个位置时，我们只能看到第k
- 1
及其之前的解码结果，因此<strong>加了mask，防止模型看到要预测的数据。这点和Encoder不同</strong></li>
<li>Encoder-Decoder Multi-Head
Attention：和Encoder的类似，每一层Decoder都会接受Encoder最后一层输出作为key和value，而当前解码器输出作为query。然后计算输入序列和目标序列中每个位置之间的相似度，最后将所有头的结果拼接在一起得到最终的输出。</li>
<li>FeedForward：和Encoder一样</li>
</ol></li>
<li>最后都连接了LinearLayer和SoftmaxLayer</li>
</ul>
<img
src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKtDCvJ7TGjfZPuP%2Fencoder-decoder-2.jpg?generation=1583677008527428&amp;alt=media" />
<center>
<code>图31: Transformer每一层Decoder都会分别接受Encoder最后一层输出</code>
</center>
<p>由此可见，<strong>只有masked multi-head
attention需要详细讲解，其余的在encoder处都已经掌握了。</strong></p>
<h3 id="masked-multi-head-attention">Masked Multi-Head-Attention</h3>
<p>Masked
Multi-Head-Attention则是在传统的多头注意力层的基础上，在计算过程中添加了一个遮挡（mask）机制。这个遮挡机制可以避免解码器在生成目标序列时看到未来的信息。</p>
<p>具体来说，在计算解码器在当前位置的输出值时，如果该位置对应的输入位置在目标序列中出现的位置在当前位置之后，那么这个输入位置就会被遮挡，不会被用来计算输出值。这样做能够避免解码器在生成目标序列时看到未来的信息，提高模型的效果。</p>
<p><strong>问题一</strong></p>
<blockquote>
<p>什么是mask</p>
</blockquote>
<p>mask表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer模型里面涉及两种mask，分别是
padding mask和sequence mask。 其中，padding mask在所有的scaled
dot-product attention 里面都需要用到，而sequence
mask只有在Decoder的Self-Attention里面用到。</p>
<p><strong>问题二</strong></p>
<blockquote>
<p>什么是padding mask？</p>
</blockquote>
<p>因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的Attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。</p>
<p>具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过softmax，这些位置的概率就会接近0！
而我们的padding mask
实际上是一个张量，每个值都是一个Boolean，值为false的地方就是我们要进行处理的地方。</p>
<p><strong>问题三</strong></p>
<blockquote>
<p>什么是sequence mask 文章前面也提到，sequence
mask是为了使得Decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。
那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p>
</blockquote>
<p>sequence mask的目的是防止Decoder “seeing the
future”，就像防止考生偷看考试答案一样。这里mask是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0。下面是个10维度的下三角矩阵：
$ [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],</p>
<p>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0],</p>
<p>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0],</p>
<p>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],</p>
<p>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],</p>
<p>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],</p>
<p>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],</p>
<p>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],</p>
<p>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],</p>
<p>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]] $</p>
<p>对于Decoder的Self-Attention，里面使用到的scaled dot-product
attention，同时需要padding mask和sequence
mask作为attn_mask，具体实现就是两个mask相加作为attn_mask。</p>
<p>其他情况，attn_mask一律等于padding mask。</p>
<p>举个例子：</p>
<p>假设最大允许的序列长度为10，先令padding mask为</p>
<p>[0 0 0 0 0 0 0 0 0 0]</p>
<p>然后假设当前句子一共有5个单词（加一个起始标识），在输入第三个单词的时候，前面有一个开始标识和两个单词，则此刻的sequence
mask为</p>
<p>[1 1 1 0 0 0]</p>
<p>然后padding mask和sequence mask相加，得</p>
<p>[1 1 1 0 0 0 0 0 0 0]</p>
<p><strong>问题四</strong></p>
<blockquote>
<p>为什么在模型训练阶段，Decoder的初始输入需要整体右移（Shifted
Right）一位？</p>
</blockquote>
<p>因为<span class="math inline">\(T-1\)</span>时刻需要预测<span
class="math inline">\(T\)</span>时刻的输出，所以Decoder的输入需要整体后移一位</p>
<p>举例说明：<code>汤姆追逐杰瑞</code> →
<code>Tom chase Jerry</code></p>
<p>位置关系： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0-“Tom”</span><br><span class="line">1-“chase”</span><br><span class="line">2-“Jerry”</span><br></pre></td></tr></table></figure> 操作：整体右移一位（Shifted Right）
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0-&lt;/s&gt;【起始符】目的是为了预测下一个Token</span><br><span class="line">1-“Tom”</span><br><span class="line">2-“chase”</span><br><span class="line">3-“Jerry”</span><br></pre></td></tr></table></figure></p>
<img
src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKtPv2OdAZhNaaw3%2Ftransformer-process-2.gif?generation=1583677021471399&amp;alt=media" />
<center>
<code>图32: 另一个例子说明Transformer模型的解码工作过程(省略了&lt;/s&gt;)</code>
</center>
<h2 id="output">Output</h2>
<img
src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(4).png" />
<center>
<code>图33: Decoder之后的输出部分</code>
</center>
<p>Decoder最终输出的结果是一个浮点型数据的向量，我们要如何把这个向量转为一个单词呢？这个就是Linear和softmax要做的事情了。</p>
<p>Linear层是一个全连接的神经网络，输出神经元个数一般等于我们的词汇表大小。Decoder输出的结果会输入到Linear层，然后再用softmax进行转换，得到的是词汇表大小的向量，向量的每个值对应的是当前Decoder是对应的这个词的概率，我们只要取概率最大的词，就是当前词语Decoder的结果了。</p>
<p>也就是说，Decoder的输出值首先经过一次线性变换，然后Softmax得到输出的概率分布，然后通过词典，输出概率最大的对应的单词作为我们的预测输出。</p>
<h1 id="transformer训练tricks">Transformer训练Tricks</h1>
<p>这里有两个训练小技巧，第一个是label平滑，第二个就是学习率要有个worm
up过程，然后再下降。</p>
<p>1、Label Smoothing（regularization）</p>
<p>由传统的 <span class="math display">\[
\begin{equation}
P_i=
\begin{cases}
1&amp; \text{ $ i = y $ } \\
0&amp; \text{ $ i \neq y $ }
\end{cases}
\end{equation}
\]</span></p>
<p>变为</p>
<p><span class="math display">\[
\begin{equation}
P_i=
\begin{cases}
1−ϵ&amp; \text{ $ i = y $ } \\
\frac{ϵ}{K−1}&amp; \text{ $ i \neq y $ }
\end{cases}
\end{equation}
\]</span></p>
<p>注：<span class="math inline">\(K\)</span>表示多分类的类别总数，<span
class="math inline">\(\epsilon\)</span>是一个较小的超参数。</p>
<p>2、<strong>[论文6]</strong>: <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.10464">Mikel Artetxe, Holger Schwenk:
“Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual
Transfer and Beyond”, 2018; [http://arxiv.org/abs/1812.10464
arXiv:1812.10464]. DOI: [https://dx.doi.org/10.1162/tacl_a_00288
10.1162/tacl_a_00288].</a></p>
<p>Noam learning rate
schedule是一种在训练深度学习模型时调整学习率的方法。这种方法是由Google
AI团队的Noam
Shazeer在2018年提出的。它的基本思想是，随着模型的训练进程，学习率应该逐渐降低。具体来说，学习率是根据训练步数的对数来调整的。这个方法可以帮助模型在训练初期快速收敛，并在训练后期更稳定地优化。</p>
<p>学习率不按照Noam Learning Rate
Schedule，可能就得不到一个好的Transformer。</p>
<p><span
class="math display">\[lr=d_{model}^{−0.5}⋅min(step_{num}^{−0.5}, step_{num}\cdot
warmup\_steps^{−1.5})\]</span></p>
<p>公式表示学习率随着训练步数的增加而逐渐降低，在训练的前<span
class="math inline">\(warmup_steps\)</span>步中学习率是线性增长的,
之后学习率是指数下降的。如图所示：</p>
<img
src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKtJ7Yme6Ll5rgo9%2Flr-worm-up.jpg?generation=1583677009478369&amp;alt=media" />
<center>
<code>图34: Noam learning rate schedule学习率随着训练步数的增加先上升后下降</code>
</center>
<h1 id="transformer特点">Transformer特点</h1>
<h2 id="优点">优点</h2>
<ol type="1">
<li><p>每层计算<strong>复杂度比RNN要低</strong>。</p></li>
<li><p>可以进行<strong>并行计算</strong>。</p></li>
<li><p>从计算一个序列长度为n的信息要经过的路径长度来看,
CNN需要增加卷积层数来扩大视野，RNN需要从1到n逐个进行计算，而Self-attention只需要一步矩阵计算就可以。Self-Attention可以比RNN<strong>更好地解决长时依赖问题</strong>。当然如果计算量太大，比如序列长度N大于序列维度D这种情况，也可以用窗口限制Self-Attention的计算数量。</p></li>
<li><p>从作者在附录中给出的例子可以看出，Self-Attention<strong>模型更可解释，Attention结果的分布表明了该模型学习到了一些语法和语义信息</strong>。</p></li>
</ol>
<h2 id="缺点">缺点</h2>
<p>在原文中没有提到缺点，是后来在Universal
Transformers中指出的，主要是两点：</p>
<ol type="1">
<li><p>有些RNN轻易可以解决的问题Transformer没做到，比如复制String，或者推理时碰到的sequence长度比训练时更长（因为碰到了没见过的position
embedding）</p></li>
<li><p>理论上：transformers不是computationally
universal(图灵完备)，而RNN图灵。完备，这种非RNN式的模型是非图灵完备的的，<strong>无法单独完成NLP中推理、决策等计算问题</strong>（包括使用transformer的bert模型等等）。</p></li>
</ol>
<h1 id="transformer面试题及答案">Transformer面试题及答案</h1>
<h1 id="参考链接">参考链接</h1>
<h2 id="整体代码实现">整体代码实现</h2>
<ul>
<li><p>哈佛大学NLP组的notebook，很详细文字和代码描述，用pytorch实现</p>
<p><a
target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p></li>
<li><p>Google的TensorFlow官方的，用tf keras实现</p>
<p><a
target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/text/transformer">https://www.tensorflow.org/tutorials/text/transformer</a></p></li>
</ul>
<h2 id="参考网站">参考网站</h2>
<ol type="1">
<li><p>Self-Attention和Transformer</p>
<p><a
target="_blank" rel="noopener" href="https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#%E8%AF%8D%E5%90%91%E9%87%8FEmbedding%E8%BE%93%E5%85%A5">https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#%E8%AF%8D%E5%90%91%E9%87%8FEmbedding%E8%BE%93%E5%85%A5</a></p></li>
<li><p>史上最小白之Transformer详解:</p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/Tink1995/article/details/105080033">https://blog.csdn.net/Tink1995/article/details/105080033</a></p></li>
<li><p>史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！:</p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148656446">https://zhuanlan.zhihu.com/p/148656446</a></p></li>
<li><p>The Illustrated Transformer:</p>
<p><a
target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p></li>
<li><p>A Brief Overview of Recurrent Neural Networks (RNN):</p>
<p><a
target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/">https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/</a></p></li>
<li><p>Practical PyTorch: Translation with a Sequence to Sequence
Network and Attention:</p>
<p><a
target="_blank" rel="noopener" href="https://notebook.community/spro/practical-pytorch/seq2seq-translation/seq2seq-translation-batched">https://notebook.community/spro/practical-pytorch/seq2seq-translation/seq2seq-translation-batched</a></p></li>
<li><p>也来谈谈RNN的梯度消失/爆炸问题:</p>
<p><a
target="_blank" rel="noopener" href="https://kexue.fm/archives/7888">https://kexue.fm/archives/7888</a></p></li>
<li><p>Transformer 架构逐层功能介绍和详细解释:</p>
<p><a
target="_blank" rel="noopener" href="https://avoid.overfit.cn/post/a895d880dab245609c177db7598446d4">https://avoid.overfit.cn/post/a895d880dab245609c177db7598446d4</a></p></li>
<li><p>Pytorch中 nn.Transformer的使用详解与Transformer的黑盒讲解:</p>
<p><a
target="_blank" rel="noopener" href="https://blog.51cto.com/u_11466419/5530949">https://blog.51cto.com/u_11466419/5530949</a></p></li>
<li><p>【深度学习】Attention is All You Need : Transformer模型:</p>
<p><a
target="_blank" rel="noopener" href="https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/">https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/</a></p></li>
<li><p>Bert前篇：手把手带你详解Transformer原理:</p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/364659780">https://zhuanlan.zhihu.com/p/364659780</a></p></li>
<li><p>ChatGPT3：</p>
<p><a
target="_blank" rel="noopener" href="https://chat.openai.com/chat">https://chat.openai.com/chat</a></p></li>
<li><p>一文搞懂one-hot和embedding：</p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/Alex_81D/article/details/114287498">https://blog.csdn.net/Alex_81D/article/details/114287498</a></p></li>
<li><p>残差网络(Residual Network)：</p>
<p><a
target="_blank" rel="noopener" href="https://www.cnblogs.com/gczr/p/10127723.html">https://www.cnblogs.com/gczr/p/10127723.html</a></p></li>
<li><p>【经典精读】万字长文解读Transformer模型和Attention机制</p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/104393915?utm_source=ZHShareTargetIDMore">https://zhuanlan.zhihu.com/p/104393915?utm_source=ZHShareTargetIDMore</a></p></li>
</ol>
<h2 id="参考文献">参考文献</h2>
<ul>
<li><p><strong>[论文1]</strong>: <a
target="_blank" rel="noopener" href="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">Hochreiter,
Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neural
computation 9.8 (1997): 1735-1780.</a></p></li>
<li><p><strong>[论文2]</strong>: <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">Dzmitry Bahdanau, Kyunghyun Cho,
Yoshua Bengio: “Neural Machine Translation by Jointly Learning to Align
and Translate”, 2014; [http://arxiv.org/abs/1409.0473
arXiv:1409.0473].</a></p></li>
<li><p><strong>[论文3]</strong>: <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Ashish Vaswani, Noam Shazeer,
Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, Illia Polosukhin: “Attention Is All You Need”, 2017;
[http://arxiv.org/abs/1706.03762 arXiv:1706.03762].</a></p></li>
<li><p><strong>[论文4]</strong>: <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">Kaiming He, Xiangyu Zhang,
Shaoqing Ren, Jian Sun: “Deep Residual Learning for Image Recognition”,
2015; [http://arxiv.org/abs/1512.03385 arXiv:1512.03385].</a></p></li>
<li><p><strong>[论文5]</strong>: <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">Jimmy Lei Ba, Jamie Ryan Kiros,
Geoffrey E. Hinton: “Layer Normalization”, 2016;
[http://arxiv.org/abs/1607.06450 arXiv:1607.06450].</a></p></li>
<li><p><strong>[论文6]</strong>: <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.10464">Mikel Artetxe, Holger Schwenk:
“Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual
Transfer and Beyond”, 2018; [http://arxiv.org/abs/1812.10464
arXiv:1812.10464]. DOI: [https://dx.doi.org/10.1162/tacl_a_00288
10.1162/tacl_a_00288].</a></p></li>
</ul>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94/">科研</a><a class="post-meta__tags" href="/tags/transformer/">transformer</a></div><div class="post_share"><div class="social-share" data-image="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/"><img class="prev-cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/simpletransformers-logo.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/20/CS231n-Assignment-1/"><img class="next-cover" src="https://s1.ax1x.com/2023/02/04/pSyTy3q.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">CS231n Assignment 1 逐行解析</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/" title="Transformer 零基础解析教程，从Encoder-Decoder架构说起（1&#x2F;4）"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-14</div><div class="title">Transformer 零基础解析教程，从Encoder-Decoder架构说起（1&#x2F;4）</div></div></a></div><div><a href="/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/" title="Transformer 零基础解析教程，完整版代码最终挑战（4&#x2F;4）"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/harvard.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-05</div><div class="title">Transformer 零基础解析教程，完整版代码最终挑战（4&#x2F;4）</div></div></a></div><div><a href="/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/" title="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3&#x2F;4）"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/simpletransformers-logo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-31</div><div class="title">Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3&#x2F;4）</div></div></a></div><div><a href="/2022/12/09/%E6%88%91%E7%9A%84%E7%A7%91%E7%A0%94%E7%BB%8F%E5%8E%86/" title="我的项目经历"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Experience/uav3(1).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-09</div><div class="title">我的项目经历</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">空之影</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/serika-onoe" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:zgong313@connect.hkust-gz.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Transformer 解读系列共计四篇文章，均已大功告成，请放心享用！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AF%BC%E8%88%AA"><span class="toc-number">1.</span> <span class="toc-text">导航</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer%E4%B9%8B%E5%89%8D%E7%9A%84%E7%BF%BB%E8%AF%91%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">Transformer之前的翻译模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E6%9C%89%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BC%BA%E9%99%B7"><span class="toc-number">2.1.</span> <span class="toc-text">原有模型的缺陷</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="toc-number">2.1.1.</span> <span class="toc-text">什么是梯度消失和梯度爆炸</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rnn%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E7%88%86%E7%82%B8%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">2.1.2.</span> <span class="toc-text">RNN梯度消失和爆炸的原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%92%88%E5%AF%B9%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%88%86%E7%82%B8%E7%9A%84%E6%94%B9%E8%BF%9Blstm"><span class="toc-number">2.1.3.</span> <span class="toc-text">针对梯度消失&#x2F;爆炸的改进：LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%92%88%E5%AF%B9%E4%B8%8A%E4%B8%8B%E6%96%87%E5%90%91%E9%87%8F%E9%95%BF%E5%BA%A6%E5%9B%BA%E5%AE%9A%E7%9A%84%E6%94%B9%E8%BF%9Battention"><span class="toc-number">2.1.4.</span> <span class="toc-text">针对上下文向量长度固定的改进：Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transformer%E8%AF%9E%E7%94%9F%E5%AF%B9%E6%A0%87rnn"><span class="toc-number">2.1.5.</span> <span class="toc-text">Transformer诞生，对标RNN</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer%E5%8E%9F%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">Transformer原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#input"><span class="toc-number">3.1.</span> <span class="toc-text">Input</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E8%AF%8Dtokenization"><span class="toc-number">3.1.1.</span> <span class="toc-text">分词(Tokenization)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5embedding"><span class="toc-number">3.1.2.</span> <span class="toc-text">嵌入(Embedding)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81positional-encoding"><span class="toc-number">3.1.3.</span> <span class="toc-text">位置编码(Positional Encoding)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#encoder"><span class="toc-number">3.2.</span> <span class="toc-text">Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-head-attention"><span class="toc-number">3.2.1.</span> <span class="toc-text">Multi-Head Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8Eself-attention%E8%AE%B2%E8%B5%B7"><span class="toc-number">3.2.1.1.</span> <span class="toc-text">从Self Attention讲起</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multi-head-attention%E5%8E%9F%E7%90%86"><span class="toc-number">3.2.1.2.</span> <span class="toc-text">Multi-head Attention原理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#add-normalize"><span class="toc-number">3.2.2.</span> <span class="toc-text">Add &amp; Normalize</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feed-forward"><span class="toc-number">3.2.3.</span> <span class="toc-text">Feed Forward</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#decoder"><span class="toc-number">3.3.</span> <span class="toc-text">Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#masked-multi-head-attention"><span class="toc-number">3.3.1.</span> <span class="toc-text">Masked Multi-Head-Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#output"><span class="toc-number">3.4.</span> <span class="toc-text">Output</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer%E8%AE%AD%E7%BB%83tricks"><span class="toc-number">4.</span> <span class="toc-text">Transformer训练Tricks</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer%E7%89%B9%E7%82%B9"><span class="toc-number">5.</span> <span class="toc-text">Transformer特点</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E7%82%B9"><span class="toc-number">5.1.</span> <span class="toc-text">优点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%BA%E7%82%B9"><span class="toc-number">5.2.</span> <span class="toc-text">缺点</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer%E9%9D%A2%E8%AF%95%E9%A2%98%E5%8F%8A%E7%AD%94%E6%A1%88"><span class="toc-number">6.</span> <span class="toc-text">Transformer面试题及答案</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">7.</span> <span class="toc-text">参考链接</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">7.1.</span> <span class="toc-text">整体代码实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E7%BD%91%E7%AB%99"><span class="toc-number">7.2.</span> <span class="toc-text">参考网站</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">7.3.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/" title="Transformer 零基础解析教程，完整版代码最终挑战（4/4）"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/harvard.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 零基础解析教程，完整版代码最终挑战（4/4）"/></a><div class="content"><a class="title" href="/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/" title="Transformer 零基础解析教程，完整版代码最终挑战（4/4）">Transformer 零基础解析教程，完整版代码最终挑战（4/4）</a><time datetime="2023-02-05T03:53:10.000Z" title="发表于 2023-02-05 11:53:10">2023-02-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/" title="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/simpletransformers-logo.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）"/></a><div class="content"><a class="title" href="/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/" title="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）">Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）</a><time datetime="2023-01-31T09:00:10.000Z" title="发表于 2023-01-31 17:00:10">2023-01-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/" title="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）"/></a><div class="content"><a class="title" href="/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/" title="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）">Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）</a><time datetime="2023-01-11T09:00:10.000Z" title="发表于 2023-01-11 17:00:10">2023-01-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/20/CS231n-Assignment-1/" title="CS231n Assignment 1 逐行解析"><img src="https://s1.ax1x.com/2023/02/04/pSyTy3q.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CS231n Assignment 1 逐行解析"/></a><div class="content"><a class="title" href="/2022/12/20/CS231n-Assignment-1/" title="CS231n Assignment 1 逐行解析">CS231n Assignment 1 逐行解析</a><time datetime="2022-12-20T14:49:13.000Z" title="发表于 2022-12-20 22:49:13">2022-12-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/" title="Transformer 零基础解析教程，从Encoder-Decoder架构说起（1/4）"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 零基础解析教程，从Encoder-Decoder架构说起（1/4）"/></a><div class="content"><a class="title" href="/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/" title="Transformer 零基础解析教程，从Encoder-Decoder架构说起（1/4）">Transformer 零基础解析教程，从Encoder-Decoder架构说起（1/4）</a><time datetime="2022-12-14T01:42:13.000Z" title="发表于 2022-12-14 09:42:13">2022-12-14</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 空之影</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script defer src="https://cdn.jsdelivr.net/gh/CodeByZach/pace/pace.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>