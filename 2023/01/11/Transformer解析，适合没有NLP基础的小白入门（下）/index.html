<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Transformer解析，适合没有NLP基础的小白入门 （下） | Kung's Blog</title><meta name="author" content="龚泽颖"><meta name="copyright" content="龚泽颖"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="前言 Transformer解析博客分为上下两篇，您现在阅读的是下篇——关于Transformer的详解，在阅读本篇博客之前了解过Encoder-Decoder和Attention机制可能会更有帮助，因此可以参考我的上篇博客，里面我也做了详细讲解和简单举例，上篇博客地址：Transformer解析，适合没有NLP基础的小白入门 （上） 网上可以学习到Transformer的资料很多，大佬们大">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer解析，适合没有NLP基础的小白入门 （下）">
<meta property="og:url" content="https://serika-onoe.github.io/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/index.html">
<meta property="og:site_name" content="Kung&#39;s Blog">
<meta property="og:description" content="前言 Transformer解析博客分为上下两篇，您现在阅读的是下篇——关于Transformer的详解，在阅读本篇博客之前了解过Encoder-Decoder和Attention机制可能会更有帮助，因此可以参考我的上篇博客，里面我也做了详细讲解和简单举例，上篇博客地址：Transformer解析，适合没有NLP基础的小白入门 （上） 网上可以学习到Transformer的资料很多，大佬们大">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2023-01-11T09:00:10.000Z">
<meta property="article:modified_time" content="2023-01-15T16:31:47.177Z">
<meta property="article:author" content="龚泽颖">
<meta property="article:tag" content="科研">
<meta property="article:tag" content="项目">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://serika-onoe.github.io/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"W5J7GO3HT8","apiKey":"53b7d5d068605c78d1a20d7f3671629a","indexName":"test_serika","hits":{"per_page":3},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer解析，适合没有NLP基础的小白入门 （下）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-01-16 00:31:47'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/CodeByZach/pace/themes/green/pace-theme-flash.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Kung's Blog" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-language"></i><span> English</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Kung's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-language"></i><span> English</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transformer解析，适合没有NLP基础的小白入门 （下）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-11T09:00:10.000Z" title="发表于 2023-01-11 17:00:10">2023-01-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-01-15T16:31:47.177Z" title="更新于 2023-01-16 00:31:47">2023-01-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transformer解析，适合没有NLP基础的小白入门 （下）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="前言">前言</h1>
<p>Transformer解析博客分为上下两篇，您现在阅读的是下篇——关于Transformer的详解，在阅读本篇博客之前了解过Encoder-Decoder和Attention机制可能会更有帮助，因此可以参考我的上篇博客，里面我也做了详细讲解和简单举例，上篇博客地址：<a
href="https://serika-onoe.github.io/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%20%EF%BC%88%E4%B8%8A%EF%BC%89/">Transformer解析，适合没有NLP基础的小白入门
（上）</a></p>
<p>网上可以学习到Transformer的资料很多，大佬们大多写得都很专业，不过对于像我这样的初学者来说，很容易就感觉自己掉进了专业术语的坑里爬不上来了。所以借此机会，我总结出了这篇博客，在一边梳理脉络的同时，也希望能够帮到屏幕面前的读者。</p>
<h1 id="问题引入">问题引入</h1>
<h2 id="transformer之前的模型">Transformer之前的模型</h2>
<p>在Transformer之前，递归神经网络(RNN)一直是处理序列数据的首选方法，大家做机器翻译用的最多的就是基于RNN的Encoder-Decoder模型。</p>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-02.jpg" />
<center>
<code>图1: RNN的工作方式</code>
</center>
<p>输入:</p>
<ul>
<li><p>输入向量 <span class="math inline">\(\vec{x_t}\)</span>
(编码词)</p></li>
<li><p>隐藏状态向量 <span
class="math inline">\(\vec{h_{t-1}}\)</span>（包含当前块之前的序列状态)</p></li>
</ul>
<p>输出：</p>
<ul>
<li>输出向量 <span class="math inline">\(\vec{o_t}\)</span></li>
</ul>
<p>权重：</p>
<ul>
<li><p><span class="math inline">\({W}\)</span>—— <span
class="math inline">\(\vec{x_t}\)</span> 和 <span
class="math inline">\(\vec{h_t}\)</span> 之间的权重</p></li>
<li><p><span class="math inline">\({V}\)</span>—— <span
class="math inline">\(\vec{ h_{t-1} }\)</span> 和 <span
class="math inline">\(\vec{h_t}\)</span> 之间的权重</p></li>
<li><p><span class="math inline">\({U}\)</span>—— <span
class="math inline">\(\vec{h_t}\)</span> 和 <span
class="math inline">\(\vec{o_t}\)</span> 之间的权重</p></li>
</ul>
<p>RNN的工作方式类似于前馈神经网络，它会将输入序列一个接一个地读取。因此在基于RNN的Encoder-Decoder模型中，编码器的目标是从顺序输入中提取数据，并将其编码为向量（即输入的表示形式）。而解码器代替输出固定长度向量的分类器，与单独使用输入中的每个符号的编码器一样，解码器在多个时间步长内生成每个输出符号。</p>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-03.jpg" />
<center>
<code>图2: Encoder-Decoder进行英-法翻译的例子</code>
</center>
<p><br/><br/>
例如，在机器翻译中，输入是英文句子，输出是翻译出的法语句子。Encoder将按顺序展开每个单词，并形成输入英文句子的固定长度向量表示（也就是上篇博客中的<span
class="math inline">\(C\)</span>）。然后Decoder将固定长度的向量表示作为输入，依次产生每个法语单词，形成翻译后的法语句子。</p>
<h2
id="基于rnn的encoder-decoder的缺陷">基于RNN的Encoder-Decoder的缺陷</h2>
<p>原有的模型存在一些问题：</p>
<ol type="1">
<li><p>训练速度慢：输入数据需要一个接一个地顺序处理，可参考图1的结构，这种串行的循环过程不适用于擅长并行计算的GPU。</p></li>
<li><p>难以处理长序列：</p>
<ul>
<li>如果输入序列太长，会出现梯度消失和爆炸问题。一般在训练过程中会在loss中看到NaN（Not
a Number）。这些也称为 RNN 中的长期依赖问题。</li>
<li>上下文向量长度固定，使用固定长度的向量表示输入序列来解码一个全新的句子是很困难的。如果输入序列很大，则上下文向量无法存储所有信息。此外，也很难区分具有相似单词但具有不同含义的句子。</li>
</ul></li>
</ol>
<h3 id="rnn梯度消失和爆炸的原理">RNN梯度消失和爆炸的原理</h3>
<img
src="https://editor.analyticsvidhya.com/uploads/51317greatlearning.png" />
<center>
<code>图3: 梯度爆炸和梯度消失的示意图</code>
</center>
<p>RNN的统一定义为</p>
<p><span class="math display">\[
\begin{equation}h_t = f\left(x_t, h_{t-1};\theta\right)\end{equation}
\]</span></p>
<ul>
<li><span
class="math inline">\(h_t\)</span>是每一步的输出，也就是隐藏状态，由当前输入<span
class="math inline">\(x_t\)</span>和前一时刻的输出<span
class="math inline">\(h_{t-1}\)</span>共同决定</li>
<li><span class="math inline">\(\theta\)</span>则是可训练的参数</li>
</ul>
<p>在做基本分析时，我们可以假设<span
class="math inline">\(h_t,x_t,\theta\)</span>都是一维的，这可以让我们获得最直观的理解，并且其结果对高维情形仍有参考价值。</p>
<p>高中数学有教过，我们可以利用微分的方法来求函数的最大值与最小值。在机器学习中，梯度是一个向量，它表示网络误差函数关于所有权重的偏导数。梯度优化算法就是通过不断计算梯度，并使用梯度优化算法来调整权重，使得代价函数Cost
Function (预测结果究竟与实际答案差了多少)
越来越小，也意味着网络能够更好地适应训练数据。当网络的Cost
Function达到最小值时，网络就能对新的数据进行较好的预测。</p>
<p>当我们的代价函数是线性函数时，我们就能够用梯度下降法(Gradient
Descent)来快速的求出代价函数（在图中记为<span
class="math inline">\(J(w)\)</span>）的最小值，如图：</p>
<img src="https://i.imgur.com/EFs14Nt.png" />
<center>
<code>图4: 梯度下降法的示意图</code>
</center>
<p>在RNN中，可以求得梯度的表达式为</p>
<p><span class="math display">\[
\begin{equation}\frac{d h_t}{d\theta} = \frac{\partial h_t}{\partial
h_{t-1}}\frac{d h_{t-1}}{d\theta} + \frac{\partial h_t}{\partial
\theta}\end{equation}
\]</span></p>
<p>这个公式的意思是，我们可以递推地计算出每一个时间步的隐藏状态对于参数的偏导数，也就是梯度。这样就可以用梯度下降算法来更新网络中的参数，使得网络能够更好地适应训练数据。</p>
<p>可以看到，其实RNN的梯度也是一个RNN，当前时刻梯度<span
class="math inline">\(\frac{d h_t}{d\theta}\)</span>是前一时刻梯度<span
class="math inline">\(\frac{d
h_{t-1}}{d\theta}\)</span>与当前运算梯度<span
class="math inline">\(\frac{\partial h_t}{\partial
\theta}\)</span>的函数。同时，从上式我们就可以看出，其实梯度消失或者梯度爆炸现象几乎是必然存在的：</p>
<ul>
<li>当<span class="math inline">\(\left|\frac{\partial h_t}{\partial
h_{t-1}}\right| &lt;
1\)</span>时，意味着历史的梯度信息逐步衰减，因此步数多了梯度必然消失（好比<span
class="math inline">\(\lim\limits_{n\to\infty} 0.9^n \to
0\)</span>）；</li>
<li>当<span class="math inline">\(\left|\frac{\partial h_t}{\partial
h_{t-1}}\right| &gt;
1\)</span>时，意味着历史的梯度信息逐步增强，因此步数多了梯度必然爆炸（好比<span
class="math inline">\(\lim\limits_{n\to\infty} 1.1^n \to
\infty\)</span>）</li>
<li>也有可能有些时刻大于1，有些时刻小于1，最终稳定在1附近，但这样概率很小，需要很精巧地设计模型和参数才行。</li>
</ul>
<p>所以步数多了，梯度消失或爆炸几乎都是不可避免的，我们只能对于有限的步数去缓解这个问题。</p>
<p>说到这里，我们还没说清楚一个问题：什么是RNN的梯度消失/爆炸？梯度爆炸好理解，就是梯度数值发散，甚至慢慢就NaN了；那梯度消失就是梯度变成零吗？并不是，我们刚刚说梯度消失是<span
class="math inline">\(\left|\frac{\partial h_t}{\partial
h_{t-1}}\right|\)</span>一直小于1，历史梯度不断衰减，但不意味着总的梯度就为0了，具体来说，一直迭代下去，我们有</p>
<p><span class="math display">\[
\begin{equation}\begin{aligned}\frac{d h_t}{d\theta} =&amp;
\frac{\partial h_t}{\partial h_{t-1}}\frac{d h_{t-1}}{d\theta} +
\frac{\partial h_t}{\partial \theta}\\
=&amp; \frac{\partial h_t}{\partial \theta}+\frac{\partial h_t}{\partial
h_{t-1}}\frac{\partial h_{t-1}}{\partial \theta}+\frac{\partial
h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial
h_{t-2}}\frac{\partial h_{t-2}}{\partial \theta}+\dots\\
\end{aligned}\end{equation}
\]</span></p>
<p>显然，其实只要<span class="math inline">\(\frac{\partial
h_t}{\partial
\theta}\)</span>不为0，那么总梯度为0的概率其实是很小的；但是一直迭代下去的话，那么<span
class="math inline">\(\frac{\partial h_1}{\partial
\theta}\)</span>这一项前面的稀疏就是<span
class="math inline">\(t-1\)</span>项的连乘<span
class="math inline">\(\frac{\partial h_t}{\partial
h_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}}\cdots\frac{\partial
h_2}{\partial
h_1}\)</span>，如果它们的绝对值都小于1，那么结果就会趋于0，这样一来，<span
class="math inline">\(\frac{d
h_t}{d\theta}\)</span>几乎就没有包含最初的梯度<span
class="math inline">\(\frac{\partial h_1}{\partial
\theta}\)</span>的信息了，这才是RNN中梯度消失的含义：距离当前时间步越长，那么其反馈的梯度信号越不显著，最后可能完全没有起作用，这就意味着RNN对长距离语义的捕捉能力失效了。<strong>说白了，你优化过程都跟长距离的反馈没关系，怎么能保证学习出来的模型能有效捕捉长距离呢？</strong></p>
<h3 id="针对梯度消失爆炸的改进lstm">针对梯度消失/爆炸的改进：LSTM</h3>
<p><a
target="_blank" rel="noopener" href="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">[论文1]Hochreiter
&amp; Schmidhuber. 1997. Long Short-Term Memory</a> 引入了长短期记忆
(LSTM)
网络，其明确设计用于避免长期依赖问题。每个LSTM单元允许过去的信息跳过当前单元的所有处理并移动到下一个单元；这允许内存保留更长时间，并使数据能够不变地与其一起流动。LSTM
由一个决定要存储哪些新信息的输入门和一个决定要删除哪些信息的遗忘门组成。</p>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-05.jpg" />
<center>
<code>图5: LSTM的工作方式</code>
</center>
<p><br/><br/> 当然，LSTM 具有改进的记忆力，能够处理比 RNN
更长的序列。然而，由于LSTM更加复杂，使得LSTM与RNN相比运行更慢。</p>
<h3 id="固定长度的上下文向量">固定长度的上下文向量</h3>
<figure>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-06.jpg"
alt="使用固定长度的向量来表示输入序列。" />
<figcaption
aria-hidden="true">使用固定长度的向量来表示输入序列。</figcaption>
</figure>
<center>
<code>图6: 长序列输入到原有模型的例子</code>
</center>
<p><br/><br/>
想象一下，选择上面的一段（输入）并记住它（定长向量）。然后，在不引用的情况下翻译整个段落（输出）。这很困难。相反，我们更希望在将一个句子从一种语言翻译成另一种语言的时候，逐部分查看句子，每输出一个单词的时候都注意句子中的特定单词或短语。</p>
<h3
id="针对上下文向量长度固定的改进attention">针对上下文向量长度固定的改进：Attention</h3>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">[论文2]Dzmitry Bahdanau.
2014. Neural Machine Translation by Jointly Learning to Align and
Translate</a>
提出了一种在编码器-解码器模型中搜索与预测目标词相关的源句子部分的方法，也就是
Attention
机制，我们可以使用Attention机制翻译相对较长的句子而不影响其性能。例如，翻译成“noir”（在法语中意为“黑色”），注意力机制将关注单词“black”和可能的“cat”，而忽略句子中的其他单词。</p>
<img src="https://i.imgur.com/5y6SCvU.png" />
<center>
<code>图7: Attention机制让输出单词关注相关的输入部分</code>
</center>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-07.jpg" />
<center>
<code>图8: Bahadanau的论文模型在测试集上长序列的 BLEU 分数明显更优</code>
</center>
<p><br/><br/>
由此可见，Attention机制提高了编码器-解码器网络的性能，但速度瓶颈仍然是RNN必须逐字处理的工作机制。</p>
<p>很自然的，我们会接着考虑：<strong>可以用更好的模型替换掉RNN这种顺序结构的模型吗？</strong></p>
<p>答案是：<strong>Yes, attention is all you need!</strong></p>
<p>哈哈，有点一语双关的感觉（说不定Google
Brain团队当时起名的时候就是这么想的）。在2017年，我们得到了一个令人满意的答案，一款名为Transformer的“神器”横空出世，至今风头不减当年。它也是我们这篇文章要探讨的主题。</p>
<h1 id="transformer详解">Transformer详解</h1>
<h2 id="简介">简介</h2>
<p>正如刚刚提到的，<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">[论文3]Ashish Vaswani., 2017 .
Attention Is All You Need</a>
第一次正式介绍了一款在翻译领域超越了RNN的新模型Transformer，<strong>Transformer是一种Encoder-Decoder架构，使用Attention机制来处理输入和生成输出。</strong></p>
<h2 id="transformer为何优于rnn">Transformer为何优于RNN</h2>
<p>Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成，更准确地讲，Transformer由且仅由self-Attenion和Feed
Forward Neural
Network组成。采用Attention机制是因为考虑到RNN（或者LSTM，GRU等）的计算限制是顺序处理的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：</p>
<ol type="1">
<li><p>时间<span class="math inline">\(t\)</span>的计算依赖<span
class="math inline">\(t-1\)</span>时刻的计算结果，这样限制了模型的并行能力。</p></li>
<li><p>顺序计算的过程中RNN会因为长期依赖导致信息丢失问题。</p></li>
</ol>
<p>Transformer的提出解决了上面两个问题：</p>
<ol type="1">
<li><p>不采用类似RNN的顺序结构，而是具有并行性，符合现有的GPU框架。</p></li>
<li><p>使用了Attention机制，将序列中的任意两个位置之间的距离缩小为一个常量，采用了和LSTM不同的思路，从而允许处理不同长度的输入序列，并且摆脱了长期依赖的影响。</p></li>
</ol>
<p>这也就是为什么它在机器翻译任务种打败了以前基于RNN的Encoder-Decoder模型，并且在其他应用中也非常受欢迎的原因。</p>
<h2 id="原理">原理</h2>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg" width = "50%" />
<center>
<code>图9: Transformer模型的整体架构</code>
</center>
<p><br/><br/>
上图是Transformer的整体架构图，结构上看起来和Encoder-Decoder模型很相似，左边是Encoder部分，右边是Decoder部分。为了方便理解，下面把Transformer分成四个部分进行详细说明:</p>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts.jpg" width = "50%" />
<center>
<code>图10: Transformer模型可分为4个组成部分</code>
</center>
<p>简单介绍一下各部分的任务：</p>
<ul>
<li><p><strong>Input</strong>：输入是单词的Embedding再加上位置编码，然后进入编码器或解码器。</p></li>
<li><p><strong>Encoder</strong>：这个结构可以循环很多次（N次），也就是说有很多层（N层）。每一层又可以分成Attention层和全连接层，再额外加了一些处理，比如Skip
Connection，做跳跃连接，然后还加了Normalization层。其实它本身的模型还是很简单的。</p></li>
<li><p><strong>Decoder</strong>：同样可以循环N次，第一次输入是前缀信息，之后的就是上一次产出的Embedding，加入位置编码，然后进入一个可以重复很多次的模块。该模块可以分成三块来看，第一块也是Attention层，第二块是Cross
Attention，不是Self
Attention，第三块是全连接层。也用了跳跃连接和Normalization。</p></li>
<li><p><strong>Output</strong>：最后的输出要通过Linear层（全连接层），再通过Softmax做预测。</p></li>
</ul>
<p>以英-中翻译为例：假设我们输入<code>"Why do we work?"</code>，输出可以是<code>"为什么我们要工作？"</code>。那么Transformer的工作步骤是：</p>
<ol type="1">
<li>输入自然语言序列到编码器: Why do we work?</li>
<li>编码器输出的隐藏层, 再输入到解码器;</li>
<li>输入<span
class="math inline">\(&lt;start&gt;\)</span>(起始)符号到解码器;</li>
<li>得到第一个字"为";</li>
<li>将得到的第一个字"为"落下来再输入到解码器;</li>
<li>得到第二个字"什";</li>
<li>将得到的第二字再落下来,
重复5、6步的相关动作依次生成“么”、“我”、“们”、“要”、“工”、“作”、“
？”，直到解码器输出<span
class="math inline">\(&lt;end&gt;\)</span>(终止符),
则代表序列生成完成。</li>
</ol>
<p>我们可以对Transformer的工作过程1~6进行可视化，如下所示:</p>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_process.png" width = "80%" />
<center>
<code>图11: 图解Transformer的工作过程</code>
</center>
<h3 id="input">Input</h3>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(1).png" width = "100%" />
<center>
<code>图12: Transformer模型的输入部分</code>
</center>
<p>我们同样以上篇采用过的例子，我们希望进行英-中翻译，输入"Tom chase
Jerry"，输出的翻译结果为"汤姆追逐杰瑞"。</p>
<p>Transformer的Decoder的输入与Encoder的输入处理方法步骤是一样的，一个接受source数据，一个接受target数据，对应例子里面就是：Encoder接受英文"Tom
chase
Jerry"，Decoder接受中文"汤姆追逐杰瑞"。当然对Decoder来说，只是在有target数据时（也就是在进行有监督训练时）才会接受Outputs
Embedding，进行预测时则不会接收。那么，以Encoder为例，描述输入过程和细节分析：</p>
<ol type="1">
<li><p>首先，向Transformer输入文本 <code>"Tom chase Jerry"</code>
。</p></li>
<li><p>随后，Transformer会将原始的英文句子"Tom chase
Jerry"进行分词(tokenization)，比如得到单词序列['[START]', 'Tom',
'chase', 'Jerry',
'[END]']。请注意，分词后的token包括“[START]”和“[END]”标记。</p></li>
<li><p>接下来，将每个单词映射到对应的词向量上。假设我们使用4维的词向量表示单词，那么对于单词'[START]',
'Tom', 'chase', 'Jerry', '[END]'，它们的词向量可能是：</p></li>
</ol>
<ul>
<li><p><span class="math inline">\(v_{[START]} :
[-0.1,0.2,-0.3,0.4]\)</span></p></li>
<li><p><span class="math inline">\(v_{Tom} : [0.5, 0.2, -0.1,
0.3]\)</span></p></li>
<li><p><span class="math inline">\(v_{chase} : [-0.2, 0.4, 0.1,
0.6]\)</span></p></li>
<li><p><span class="math inline">\(v_{Jerry} : [-0.2, 0.3, 0.1,
-0.5]\)</span></p></li>
<li><p><span class="math inline">\(v_{[END]} :
[0.3,-0.2,0.1,-0.4]\)</span></p></li>
</ul>
<ol start="4" type="1">
<li>通过使用sin和cos函数来生成位置向量，这种方式可以向模型描述各个单词之间的顺序关系，并且能够在维度空间上均匀分布位置向量。可以假设'[START]'的位置编号为1，"Tom"的位置编号为2，"chase"的位置编号为3，"Jerry"的位置编号为4，"[END]"的位置编号为5。那么它们位置向量可能是：</li>
</ol>
<ul>
<li><p><span class="math inline">\(p_1 =
[sin(\frac{1}{10000^{2*\frac{1}{4}}}),
cos(\frac{1}{10000^{2*\frac{1}{4}}}),sin(\frac{2}{10000^{2*\frac{1}{4}}}),
cos(\frac{2}{10000^{2*\frac{1}{4}}}) ]\)</span></p></li>
<li><p><span class="math inline">\(p_2 =
[sin(\frac{2}{10000^{2*\frac{1}{4}}}),
cos(\frac{2}{10000^{2*\frac{1}{4}}}),sin(\frac{3}{10000^{2*\frac{1}{4}}}),
cos(\frac{3}{10000^{2*\frac{1}{4}}}) ]\)</span></p></li>
<li><p><span class="math inline">\(p_3 =
[sin(\frac{3}{10000^{2*\frac{1}{4}}}),
cos(\frac{3}{10000^{2*\frac{1}{4}}}),sin(\frac{4}{10000^{2*\frac{1}{4}}}),
cos(\frac{4}{10000^{2*\frac{1}{4}}}) ]\)</span></p></li>
<li><p><span class="math inline">\(p_4 =
[sin(\frac{4}{10000^{2*\frac{1}{4}}}),
cos(\frac{4}{10000^{2*\frac{1}{4}}}),sin(\frac{5}{10000^{2*\frac{1}{4}}}),
cos(\frac{5}{10000^{2*\frac{1}{4}}}) ]\)</span></p></li>
<li><p><span class="math inline">\(p_5 =
[sin(\frac{5}{10000^{2*\frac{1}{4}}}),
cos(\frac{5}{10000^{2*\frac{1}{4}}}),sin(\frac{6}{10000^{2*\frac{1}{4}}}),
cos(\frac{6}{10000^{2*\frac{1}{4}}}) ]\)</span></p></li>
</ul>
<ol start="5" type="1">
<li>最后，输入到Transformer中的序列就是由词向量和位置向量相加得到的，例如，“Tom
chase Jerry”的输入序列可能是: [<span class="math inline">\(v_
{[START]}\)</span> + <span class="math inline">\(p_1\)</span> , <span
class="math inline">\(v_{Tom}\)</span> + <span
class="math inline">\(p_2\)</span>, <span
class="math inline">\(v_{chase}\)</span> + <span
class="math inline">\(p_3\)</span>, <span
class="math inline">\(v_{Jerry}\)</span> + <span
class="math inline">\(p_4\)</span>, <span class="math inline">\(v_
{[END]}\)</span> + <span class="math inline">\(p_5\)</span>]</li>
</ol>
<h3 id="分词tokenization">分词(Tokenization)</h3>
<p>分词是NLP的一个重要概念，<strong>表示将文本(text)切分成符号(token)的过程。</strong>token可以是以下三种类型：</p>
<ol type="1">
<li><p>单词 (word) —— 例如，短语“dogs like
cats”由三个词标记组成：“dogs”、“like”和“cats”。</p></li>
<li><p>字符 (character) —— 例如，短语“your
fish”由九个字符标记组成。<strong>（请注意，空格算作标记之一）</strong></p></li>
<li><p>子词 (subword) ——
其中单个词可以是单个标记或多个标记。子词由词根、前缀或后缀组成。例如，使用子词作为标记的语言模型可能会将单词“dogs”视为两个标记（词根“dog”和复数后缀“s”）。相同的语言模型可能会将单个词“更高”视为两个子词（词根“high”和后缀“er”）。</p></li>
</ol>
<p>通过分词，模型可以将文本分成若干个单独的词汇单元，可以减少翻译系统需要处理的信息量，提高翻译效率和准确性，同时更好地维护语言的语法结构。</p>
<p>Transformer
模型的输入通常是序列数据，如文本、语音等。这些数据在输入之前需要进行预处理，其中一个重要的步骤就是分词。<strong>分词是获取词向量之前的一个必要步骤。</strong></p>
<h3 id="嵌入embedding">嵌入(Embedding)</h3>
<p>NLP中，<strong>使用分词后的词向量作为模型输入</strong>是常见的做法，而词向量是一种特定类型的嵌入。</p>
<p>那么什么是嵌入呢？它是NLP中使用的一种技术，以机器学习模型能够理解的数字格式表示单词、短语甚至整个句子。而其中的词向量用于在多维空间中表示单词，它的目标在于通过机器学习模型能够理解的方式捕捉一个词的含义和上下文，并将词语的语义信息转化为数字，使得它们可以被计算机理解。</p>
<p>可以认为，嵌入是一种降维的形式。NLP中，单词或句子可以被表示为one
hot编码向量，它是高维和稀疏的，虽然避免了线性不可分的问题，但也意味着向量中的大多数元素都是零。这可能会导致计算效率低下，而且机器学习模型也难以通过学习来理解单词或句子的含义和相互关系。</p>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/onehot.png" width = "80%" />
<center>
<code>图13: 图解one hot的编码方式</code>
</center>
<p><br/><br/>
嵌入是一种以较低维度的密集格式表示单词或句子的方法，更适合机器学习模型。通过将单词或句子映射到一个较低的维度空间，嵌入可以捕捉到单词或句子的含义和相互关系，同时舍弃不太重要的信息。常见的词向量编码方式是word2vec。相比于one-hot编码只允许我们将单词作为单个不同的条目来解释，word2vec允许我们寻找每个单词和其他单词的关系，从而创建更好的特征表示。</p>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/word2.png" width = "80%" />
<center>
<code>图14: 图解word2vec的编码方式</code>
</center>
<p><br/><br/>
word2vec模型使用神经网络来学习一个词的向量表示。它将每个词映射到一个高维向量，其中语义相似的词在向量空间中是紧密相连的。例如，在word2vec模型中，"soccer"这个词的向量表示可能是[-0.2,
0.5, 0.1, 0.3,
...]，代表这个词的含义和上下文，(一个词向量往往是300-1000维，向量中的每个元素代表这个词的意义或上下文的一个维度，单个数字的含义本身不可解释，只有在与其他词或句子的向量相关时才有意义)。</p>
<p>Encoder输入单词<span
class="math inline">\(x\)</span>的embedding，有两种常见的选择：</p>
<ol type="1">
<li><p>使用Pre-trained的<strong>embeddings并固化</strong>，这种情况下embedding取自一个预先训练好的模型，在训练过程中不更新。实际就是一个Lookup
Table（查找表）。</p></li>
<li><p>对其进行随机初始化（当然也可以选择Pre-trained的结果），但<strong>设为Trainable</strong>。这样在training过程中不断地对embeddings进行改进。即End2End（端到端）训练方法，意味着模型从头到尾都被训练，所有的参数，包括嵌入都在训练过程中被更新。<strong>这也是Transformer选择的做法。</strong></p></li>
</ol>
<h3 id="位置编码positional-encoding">位置编码(Positional Encoding)</h3>
<h4 id="问题一">问题一</h4>
<blockquote>
<p>为什么需要知道每个单词的位置，并且添加位置编码呢？</p>
</blockquote>
<p>首先，咱们知道，一句话中同一个词如果的出现位置不同，意思可能发生翻天覆地的变化，就比如：我欠他100W
和
他欠我100W。这两句话的意思一个地狱一个天堂。可见获取词语出现在句子中的位置信息是一件很重要的事情。</p>
<p>而Transformer没有用RNN也没有卷积，它使用的注意力机制(主要是由于self
attention)，不能获取词语位置信息，就算打乱一句话中词语的位置，每个词还是能与其他词之间计算attention值。所以为了让模型能利用序列的顺序，必须输入序列中词的位置，所以Transformer采用的方法是给每一个词向量，包括包括“[START]”和“[END]”都需要添加位置编码。</p>
<h4 id="问题二">问题二</h4>
<blockquote>
<p>positional encoding怎么获取呢？</p>
</blockquote>
<p>Transformer使用的是正余弦位置编码。位置编码通过使用不同频率的正弦、余弦函数生成，然后和对应的位置的词向量相加，位置向量维度必须和词向量的维度一致。过程如上图，PE（positional
encoding）计算公式如下：</p>
<p><span class="math display">\[PE_{(pos,2i)} =
sin(\frac{pos}{10000^{\frac{2 i}{d_{model}}}})\]</span> <span
class="math display">\[PE_{(pos,2i+1)} = cos(\frac{pos}{10000^{\frac{2
i}{d_{model}}}})\]</span></p>
<p>解释一下上面的公式： - <span
class="math inline">\(pos\)</span>表示单词在句子中的绝对位置，pos=0，1，2…，例如：Jerry在"Tom
chase Jerry"中的pos=2； - <span
class="math inline">\(d_model\)</span>表示词向量的维度，一般<span
class="math inline">\(d_model\)</span>=512；2i和2i+1表示奇偶性，i表示词向量中的第几维，例如这里<span
class="math inline">\(d_model\)</span>=512，故i=0，1，2…255。</p>
<p>至于上面这个公式是怎么得来的，其实不重要，因为很有可能是作者根据经验自己造的，而且公式也不唯一，后续Google在Bert中的采用类似词向量的方法通过训练PE，说明这种求位置向量的方法还是存在一定问题。</p>
<h4 id="问题三">问题三</h4>
<blockquote>
<p>为什么是将positional encoding与词向量相加，而不是拼接呢？</p>
</blockquote>
<p>事实上，拼接或者相加都可以，只是本身词向量的维度，512维就已经蛮大了，再拼接一个512维的位置向量，变成1024维，这样训练起来会相对慢一些，影响效率。两者既然效果差不多，那当然是选择学习习难度较小的相加了。</p>
<h3 id="encoder">Encoder</h3>
<h3 id="decoder">Decoder</h3>
<h3 id="output">Output</h3>
<h1 id="参考链接">参考链接</h1>
<ol type="1">
<li><p>Self-Attention和Transformer</p>
<p><a
target="_blank" rel="noopener" href="https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#%E8%AF%8D%E5%90%91%E9%87%8FEmbedding%E8%BE%93%E5%85%A5">https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#%E8%AF%8D%E5%90%91%E9%87%8FEmbedding%E8%BE%93%E5%85%A5</a></p></li>
<li><p>史上最小白之Transformer详解:</p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/Tink1995/article/details/105080033">https://blog.csdn.net/Tink1995/article/details/105080033</a></p></li>
<li><p>史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！:</p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148656446">https://zhuanlan.zhihu.com/p/148656446</a></p></li>
<li><p>The Illustrated Transformer:</p>
<p><a
target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p></li>
<li><p>A Brief Overview of Recurrent Neural Networks (RNN):</p>
<p><a
target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/">https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/</a></p></li>
<li><p>Practical PyTorch: Translation with a Sequence to Sequence
Network and Attention:</p>
<p><a
target="_blank" rel="noopener" href="https://notebook.community/spro/practical-pytorch/seq2seq-translation/seq2seq-translation-batched">https://notebook.community/spro/practical-pytorch/seq2seq-translation/seq2seq-translation-batched</a></p></li>
<li><p>也来谈谈RNN的梯度消失/爆炸问题:</p>
<p><a
target="_blank" rel="noopener" href="https://kexue.fm/archives/7888">https://kexue.fm/archives/7888</a></p></li>
<li><p>Transformer 架构逐层功能介绍和详细解释:</p>
<p><a
target="_blank" rel="noopener" href="https://avoid.overfit.cn/post/a895d880dab245609c177db7598446d4">https://avoid.overfit.cn/post/a895d880dab245609c177db7598446d4</a></p></li>
<li><p>Pytorch中 nn.Transformer的使用详解与Transformer的黑盒讲解:</p>
<p><a
target="_blank" rel="noopener" href="https://blog.51cto.com/u_11466419/5530949">https://blog.51cto.com/u_11466419/5530949</a></p></li>
<li><p>【深度学习】Attention is All You Need : Transformer模型:</p>
<p><a
target="_blank" rel="noopener" href="https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/">https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/</a></p></li>
<li><p>Bert前篇：手把手带你详解Transformer原理:</p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/364659780">https://zhuanlan.zhihu.com/p/364659780</a></p></li>
<li><p>ChatGPT3：</p>
<p><a
target="_blank" rel="noopener" href="https://chat.openai.com/chat">https://chat.openai.com/chat</a></p></li>
<li><p>一文搞懂one-hot和embedding：</p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/Alex_81D/article/details/114287498">https://blog.csdn.net/Alex_81D/article/details/114287498</a></p></li>
</ol>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94/">科研</a><a class="post-meta__tags" href="/tags/%E9%A1%B9%E7%9B%AE/">项目</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/12/31/FEDformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><img class="next-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">FEDformer论文阅读</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/12/09/%E6%88%91%E7%9A%84%E7%A7%91%E7%A0%94%E7%BB%8F%E5%8E%86/" title="我的项目经历"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Experience/uav3(1).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-09</div><div class="title">我的项目经历</div></div></a></div><div><a href="/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8A%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （上）"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-14</div><div class="title">Transformer解析，适合没有NLP基础的小白入门 （上）</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">龚泽颖</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/serika-onoe" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:zgong313@connect.hkust-gz.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">雪融化了，是春天！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E5%BC%95%E5%85%A5"><span class="toc-number">2.</span> <span class="toc-text">问题引入</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#transformer%E4%B9%8B%E5%89%8D%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.</span> <span class="toc-text">Transformer之前的模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8Ernn%E7%9A%84encoder-decoder%E7%9A%84%E7%BC%BA%E9%99%B7"><span class="toc-number">2.2.</span> <span class="toc-text">基于RNN的Encoder-Decoder的缺陷</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#rnn%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E7%88%86%E7%82%B8%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">2.2.1.</span> <span class="toc-text">RNN梯度消失和爆炸的原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%92%88%E5%AF%B9%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%88%86%E7%82%B8%E7%9A%84%E6%94%B9%E8%BF%9Blstm"><span class="toc-number">2.2.2.</span> <span class="toc-text">针对梯度消失&#x2F;爆炸的改进：LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BA%E5%AE%9A%E9%95%BF%E5%BA%A6%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87%E5%90%91%E9%87%8F"><span class="toc-number">2.2.3.</span> <span class="toc-text">固定长度的上下文向量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%92%88%E5%AF%B9%E4%B8%8A%E4%B8%8B%E6%96%87%E5%90%91%E9%87%8F%E9%95%BF%E5%BA%A6%E5%9B%BA%E5%AE%9A%E7%9A%84%E6%94%B9%E8%BF%9Battention"><span class="toc-number">2.2.4.</span> <span class="toc-text">针对上下文向量长度固定的改进：Attention</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer%E8%AF%A6%E8%A7%A3"><span class="toc-number">3.</span> <span class="toc-text">Transformer详解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">3.1.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#transformer%E4%B8%BA%E4%BD%95%E4%BC%98%E4%BA%8Ernn"><span class="toc-number">3.2.</span> <span class="toc-text">Transformer为何优于RNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E7%90%86"><span class="toc-number">3.3.</span> <span class="toc-text">原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#input"><span class="toc-number">3.3.1.</span> <span class="toc-text">Input</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E8%AF%8Dtokenization"><span class="toc-number">3.3.2.</span> <span class="toc-text">分词(Tokenization)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5embedding"><span class="toc-number">3.3.3.</span> <span class="toc-text">嵌入(Embedding)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81positional-encoding"><span class="toc-number">3.3.4.</span> <span class="toc-text">位置编码(Positional Encoding)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E4%B8%80"><span class="toc-number">3.3.4.1.</span> <span class="toc-text">问题一</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E4%BA%8C"><span class="toc-number">3.3.4.2.</span> <span class="toc-text">问题二</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E4%B8%89"><span class="toc-number">3.3.4.3.</span> <span class="toc-text">问题三</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder"><span class="toc-number">3.3.5.</span> <span class="toc-text">Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decoder"><span class="toc-number">3.3.6.</span> <span class="toc-text">Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#output"><span class="toc-number">3.3.7.</span> <span class="toc-text">Output</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">4.</span> <span class="toc-text">参考链接</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （下）"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer解析，适合没有NLP基础的小白入门 （下）"/></a><div class="content"><a class="title" href="/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （下）">Transformer解析，适合没有NLP基础的小白入门 （下）</a><time datetime="2023-01-11T09:00:10.000Z" title="发表于 2023-01-11 17:00:10">2023-01-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/31/FEDformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="FEDformer论文阅读"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="FEDformer论文阅读"/></a><div class="content"><a class="title" href="/2022/12/31/FEDformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="FEDformer论文阅读">FEDformer论文阅读</a><time datetime="2022-12-31T03:47:23.000Z" title="发表于 2022-12-31 11:47:23">2022-12-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/20/CS231n-Assignment-1/" title="CS231n Assignment 1 (Updating)"><img src="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/cs231n_assignment1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CS231n Assignment 1 (Updating)"/></a><div class="content"><a class="title" href="/2022/12/20/CS231n-Assignment-1/" title="CS231n Assignment 1 (Updating)">CS231n Assignment 1 (Updating)</a><time datetime="2022-12-20T14:49:13.000Z" title="发表于 2022-12-20 22:49:13">2022-12-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8A%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （上）"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer解析，适合没有NLP基础的小白入门 （上）"/></a><div class="content"><a class="title" href="/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8A%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （上）">Transformer解析，适合没有NLP基础的小白入门 （上）</a><time datetime="2022-12-14T01:42:13.000Z" title="发表于 2022-12-14 09:42:13">2022-12-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/" title="强化学习初探"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="强化学习初探"/></a><div class="content"><a class="title" href="/2022/12/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/" title="强化学习初探">强化学习初探</a><time datetime="2022-12-12T15:17:01.000Z" title="发表于 2022-12-12 23:17:01">2022-12-12</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 龚泽颖</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script defer src="https://cdn.jsdelivr.net/gh/CodeByZach/pace/pace.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>