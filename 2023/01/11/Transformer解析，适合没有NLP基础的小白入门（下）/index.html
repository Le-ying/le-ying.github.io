<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Transformer解析，适合没有NLP基础的小白入门 （下） | Kung's Blog</title><meta name="author" content="龚泽颖"><meta name="copyright" content="龚泽颖"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="前言 Transformer解析博客分为上下两篇，您现在阅读的是下篇——关于Transformer的详解，在阅读本篇博客之前了解过Encoder-Decoder和Attention机制可能会更有帮助，因此可以参考我的上篇博客，里面我也做了详细讲解和简单举例，上篇博客地址：Transformer解析，适合没有NLP基础的小白入门 （上） 网上可以学习到Transformer的资料很多，大佬们大">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer解析，适合没有NLP基础的小白入门 （下）">
<meta property="og:url" content="https://serika-onoe.github.io/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/index.html">
<meta property="og:site_name" content="Kung&#39;s Blog">
<meta property="og:description" content="前言 Transformer解析博客分为上下两篇，您现在阅读的是下篇——关于Transformer的详解，在阅读本篇博客之前了解过Encoder-Decoder和Attention机制可能会更有帮助，因此可以参考我的上篇博客，里面我也做了详细讲解和简单举例，上篇博客地址：Transformer解析，适合没有NLP基础的小白入门 （上） 网上可以学习到Transformer的资料很多，大佬们大">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2023-01-11T09:00:10.000Z">
<meta property="article:modified_time" content="2023-01-19T06:51:15.409Z">
<meta property="article:author" content="龚泽颖">
<meta property="article:tag" content="科研">
<meta property="article:tag" content="项目">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://serika-onoe.github.io/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"W5J7GO3HT8","apiKey":"53b7d5d068605c78d1a20d7f3671629a","indexName":"test_serika","hits":{"per_page":3},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer解析，适合没有NLP基础的小白入门 （下）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-01-19 14:51:15'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/CodeByZach/pace/themes/green/pace-theme-flash.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Kung's Blog" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-language"></i><span> English</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Kung's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div><div class="menus_item"><a class="site-page" href="/en/"><i class="fa-fw fas fa-language"></i><span> English</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transformer解析，适合没有NLP基础的小白入门 （下）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-11T09:00:10.000Z" title="发表于 2023-01-11 17:00:10">2023-01-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-01-19T06:51:15.409Z" title="更新于 2023-01-19 14:51:15">2023-01-19</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transformer解析，适合没有NLP基础的小白入门 （下）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="前言">前言</h1>
<p>Transformer解析博客分为上下两篇，您现在阅读的是下篇——关于Transformer的详解，在阅读本篇博客之前了解过Encoder-Decoder和Attention机制可能会更有帮助，因此可以参考我的上篇博客，里面我也做了详细讲解和简单举例，上篇博客地址：<a
href="https://serika-onoe.github.io/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%20%EF%BC%88%E4%B8%8A%EF%BC%89/">Transformer解析，适合没有NLP基础的小白入门
（上）</a></p>
<p>网上可以学习到Transformer的资料很多，大佬们大多写得都很专业，不过对于像我这样的初学者来说，很容易就感觉自己掉进了专业术语的坑里爬不上来了。所以借此机会，我总结出了这篇博客，在一边梳理脉络的同时，也希望能够帮到屏幕面前的读者。</p>
<h1 id="transformer之前的通用模型">Transformer之前的通用模型</h1>
<p>在Transformer之前，递归神经网络(RNN)一直是处理序列数据的首选方法，大家做机器翻译用的最多的就是基于RNN的Encoder-Decoder模型。</p>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-02.jpg" />
<center>
<code>图1: RNN的工作方式</code>
</center>
<p>输入:</p>
<ul>
<li><p>输入向量 <span class="math inline">\(\vec{x_t}\)</span>
(编码词)</p></li>
<li><p>隐藏状态向量 <span
class="math inline">\(\vec{h_{t-1}}\)</span>（包含当前块之前的序列状态)</p></li>
</ul>
<p>输出：</p>
<ul>
<li>输出向量 <span class="math inline">\(\vec{o_t}\)</span></li>
</ul>
<p>权重：</p>
<ul>
<li><p><span class="math inline">\({W}\)</span>—— <span
class="math inline">\(\vec{x_t}\)</span> 和 <span
class="math inline">\(\vec{h_t}\)</span> 之间的权重</p></li>
<li><p><span class="math inline">\({V}\)</span>—— <span
class="math inline">\(\vec{ h_{t-1} }\)</span> 和 <span
class="math inline">\(\vec{h_t}\)</span> 之间的权重</p></li>
<li><p><span class="math inline">\({U}\)</span>—— <span
class="math inline">\(\vec{h_t}\)</span> 和 <span
class="math inline">\(\vec{o_t}\)</span> 之间的权重</p></li>
</ul>
<p>RNN的工作方式类似于前馈神经网络，它会将输入序列一个接一个地读取。因此在基于RNN的Encoder-Decoder模型中，编码器的目标是从顺序输入中提取数据，并将其编码为向量（即输入的表示形式）。而解码器代替输出固定长度向量的分类器，与单独使用输入中的每个符号的编码器一样，解码器在多个时间步长内生成每个输出符号。</p>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-03.jpg" />
<center>
<code>图2: Encoder-Decoder进行英-法翻译的例子</code>
</center>
<p><br/><br/>
例如，在机器翻译中，输入是英文句子，输出是翻译出的法语句子。Encoder将按顺序展开每个单词，并形成输入英文句子的固定长度向量表示（也就是上篇博客中的<span
class="math inline">\(C\)</span>）。然后Decoder将固定长度的向量表示作为输入，依次产生每个法语单词，形成翻译后的法语句子。</p>
<h2 id="原有模型的缺陷">原有模型的缺陷</h2>
<p>原有的模型，即基于RNN的Encoder-Decoder存在一些问题：</p>
<ol type="1">
<li><p>训练速度慢：输入数据需要一个接一个地顺序处理，这种串行的循环过程不适用于擅长并行计算的GPU。</p></li>
<li><p>难以处理长序列：</p>
<ul>
<li>如果输入序列太长，会出现梯度消失和爆炸问题。一般在训练过程中会在loss中看到NaN（Not
a Number）。这些也称为 RNN 中的长期依赖问题。</li>
<li>上下文向量长度固定，使用固定长度的向量表示输入序列来解码一个全新的句子是很困难的。如果输入序列很大，则上下文向量无法存储所有信息。此外，也很难区分具有相似单词但具有不同含义的句子。</li>
</ul></li>
</ol>
<p>第1个问题很好理解，不过第2个问题中，关于梯度消失和爆炸的问题，可以先看看以下的补充说明。</p>
<h3 id="什么是梯度消失和梯度爆炸">什么是梯度消失和梯度爆炸</h3>
<p>高中数学有教过，我们可以利用微分的方法来求函数的最大值与最小值。在机器学习中，梯度是一个向量，它表示网络误差函数关于所有权重的偏导数。梯度优化算法就是<strong>通过不断计算梯度，并使用梯度优化算法来调整权重，使得代价函数Cost
Function (预测结果究竟与实际答案差了多少)
越来越小</strong>，也意味着网络能够更好地适应训练数据。当网络的Cost
Function达到最小值时，网络就能对新的数据进行较好的预测。</p>
<p>当我们的代价函数是线性函数时，我们就能够用梯度下降法(Gradient
Descent)来快速的求出代价函数（在图中记为<span
class="math inline">\(J(w)\)</span>）的最小值，如图：</p>
<img src="https://i.imgur.com/EFs14Nt.png" />
<center>
<code>图3: 梯度下降法的示意图</code>
</center>
<p><br/><br/>
而梯度消失和梯度爆炸是深度神经网络训练中的两种典型问题。</p>
<p>梯度消失（vanishing
gradient）指在深层网络训练中，由于梯度的较小值逐层传递，导致较深层的权值参数的更新量非常小，趋近于0。这样会导致较深层网络的参数无法得到有效更新，从而使整个网络无法学习。</p>
<p>梯度爆炸（exploding
gradient）指在深层网络训练中，由于梯度的较大值逐层传递，导致较深层的权值参数的更新量非常大，甚至无限大。这样会导致较深层网络的参数更新量过大，从而使整个网络无法学习。</p>
<p>如果有疑问请看下面RNN的例子：</p>
<h3 id="rnn梯度消失和爆炸的原理">RNN梯度消失和爆炸的原理</h3>
<p><strong>RNN的统一定义为</strong></p>
<p><span class="math display">\[
\begin{equation}h_t = f\left(x_t, h_{t-1};\theta\right)\end{equation}
\]</span></p>
<ul>
<li><span
class="math inline">\(h_t\)</span>是每一步的输出，也就是隐藏状态，由当前输入<span
class="math inline">\(x_t\)</span>和前一时刻的输出<span
class="math inline">\(h_{t-1}\)</span>共同决定</li>
<li><span class="math inline">\(\theta\)</span>则是可训练的参数</li>
</ul>
<p>(在做基本分析时，我们可以假设<span
class="math inline">\(h_t,x_t,\theta\)</span>都是一维的，这可以让我们获得最直观的理解，并且其结果对高维情形仍有参考价值。)</p>
<p><strong>RNN梯度的表达式为</strong></p>
<p><span class="math display">\[
\begin{equation}\frac{d h_t}{d\theta} = \frac{\partial h_t}{\partial
h_{t-1}}\frac{d h_{t-1}}{d\theta} + \frac{\partial h_t}{\partial
\theta}\end{equation}
\]</span></p>
<p>这个公式的意思是，我们可以递推地计算出每一个时间步的隐藏状态对于参数的偏导数，也就是梯度。这样就可以用梯度下降算法来更新网络中的参数，使得网络能够更好地适应训练数据。</p>
<p>可以看到，其实RNN的梯度也是一个RNN，当前时刻梯度<span
class="math inline">\(\frac{d h_t}{d\theta}\)</span>是前一时刻梯度<span
class="math inline">\(\frac{d
h_{t-1}}{d\theta}\)</span>与当前运算梯度<span
class="math inline">\(\frac{\partial h_t}{\partial
\theta}\)</span>的函数。同时，从上式我们就可以看出，其实梯度消失或者梯度爆炸现象几乎是必然存在的：</p>
<ul>
<li>当<span class="math inline">\(\left|\frac{\partial h_t}{\partial
h_{t-1}}\right| &lt;
1\)</span>时，意味着历史的梯度信息逐步衰减，因此步数多了梯度必然消失（好比<span
class="math inline">\(\lim\limits_{n\to\infty} 0.9^n \to
0\)</span>）；</li>
<li>当<span class="math inline">\(\left|\frac{\partial h_t}{\partial
h_{t-1}}\right| &gt;
1\)</span>时，意味着历史的梯度信息逐步增强，因此步数多了梯度必然爆炸（好比<span
class="math inline">\(\lim\limits_{n\to\infty} 1.1^n \to
\infty\)</span>）</li>
<li>也有可能有些时刻大于1，有些时刻小于1，最终稳定在1附近，但这样概率很小，需要很精巧地设计模型和参数才行。</li>
</ul>
<img
src="https://editor.analyticsvidhya.com/uploads/51317greatlearning.png" />
<center>
<code>图4: 梯度爆炸和梯度消失的示意图</code>
</center>
<p><br/><br/></p>
<blockquote>
<p>梯度消失就是梯度变成零吗？</p>
</blockquote>
<p>并不是，我们刚刚说梯度消失是<span
class="math inline">\(\left|\frac{\partial h_t}{\partial
h_{t-1}}\right|\)</span>一直小于1，历史梯度不断衰减，但不意味着总的梯度就为0了，具体来说，一直迭代下去，我们有</p>
<p><span class="math display">\[
\begin{equation}\begin{aligned}\frac{d h_t}{d\theta} =&amp;
\frac{\partial h_t}{\partial h_{t-1}}\frac{d h_{t-1}}{d\theta} +
\frac{\partial h_t}{\partial \theta}\\
=&amp; \frac{\partial h_t}{\partial \theta}+\frac{\partial h_t}{\partial
h_{t-1}}\frac{\partial h_{t-1}}{\partial \theta}+\frac{\partial
h_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partial
h_{t-2}}\frac{\partial h_{t-2}}{\partial \theta}+\dots\\
\end{aligned}\end{equation}
\]</span></p>
<p>显然，其实只要<span class="math inline">\(\frac{\partial
h_t}{\partial
\theta}\)</span>不为0，那么总梯度为0的概率其实是很小的；但是一直迭代下去的话，那么<span
class="math inline">\(\frac{\partial h_1}{\partial
\theta}\)</span>这一项前面的稀疏就是<span
class="math inline">\(t-1\)</span>项的连乘<span
class="math inline">\(\frac{\partial h_t}{\partial
h_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}}\cdots\frac{\partial
h_2}{\partial
h_1}\)</span>，如果它们的绝对值都小于1，那么结果就会趋于0，这样一来，<span
class="math inline">\(\frac{d
h_t}{d\theta}\)</span>几乎就没有包含最初的梯度<span
class="math inline">\(\frac{\partial h_1}{\partial
\theta}\)</span>的信息了。</p>
<p>这才是RNN中梯度消失的含义：距离当前时间步越长，那么其反馈的梯度信号越不显著，最后可能完全没有起作用，这就意味着RNN对长距离语义的捕捉能力失效了。<strong>说白了，优化过程都跟长距离的反馈没关系，那我们怎么保证学习出来的模型能有效捕捉长距离呢？</strong></p>
<p>所以对于一般的RNN模型来说，步数多了，梯度消失或爆炸几乎都是不可避免的，我们只能通过让RNN执行有限的步数来缓解这个问题。直到上世纪末提出的LSTM极大地改进了这个问题。</p>
<h3 id="针对梯度消失爆炸的改进lstm">针对梯度消失/爆炸的改进：LSTM</h3>
<p><a
target="_blank" rel="noopener" href="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">[论文1]Hochreiter
&amp; Schmidhuber. 1997. Long Short-Term Memory</a> 引入了长短期记忆
(LSTM)
网络，其明确设计用于避免长期依赖问题。每个LSTM单元允许过去的信息跳过当前单元的所有处理并移动到下一个单元；这允许内存保留更长时间，并使数据能够不变地与其一起流动。LSTM
由一个决定要存储哪些新信息的输入门和一个决定要删除哪些信息的遗忘门组成。</p>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-05.jpg" />
<center>
<code>图5: LSTM的工作方式</code>
</center>
<p><br/><br/> 当然，LSTM 具有改进的记忆力，能够处理比 RNN
更长的序列。然而，由于LSTM更加复杂，使得LSTM与RNN相比运行更慢。</p>
<h3
id="针对上下文向量长度固定的改进attention">针对上下文向量长度固定的改进：Attention</h3>
<figure>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-06.jpg"
alt="使用固定长度的向量来表示输入序列。" />
<figcaption
aria-hidden="true">使用固定长度的向量来表示输入序列。</figcaption>
</figure>
<center>
<code>图6: 长序列输入到原有模型的例子</code>
</center>
<p><br/><br/>
假设有一段长文本（输入），将其记忆下来（转换为固定长度的向量），然后在不回顾这段文本的情况下，按顺序翻译出整段文本（输出）。这很难，也不是我们的目标做法。相反，当我们翻译一句话时，我们会一部分一部分地看，逐段关注句子的某一部分，从而保证翻译的准确性，这就引入了下文所提到的Attention机制。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">[论文2]Dzmitry Bahdanau.
2014. Neural Machine Translation by Jointly Learning to Align and
Translate</a>
提出了一种在编码器-解码器模型中搜索与预测目标词相关的源句子部分的方法，也就是
Attention
机制，我们可以使用Attention机制翻译相对较长的句子而不影响其性能。例如，翻译成“noir”（在法语中意为“黑色”），注意力机制将关注单词“black”和可能的“cat”，而忽略句子中的其他单词。</p>
<img src="https://i.imgur.com/5y6SCvU.png" />
<center>
<code>图7: Attention机制让输出单词关注相关的输入部分</code>
</center>
<img
src="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-07.jpg" />
<center>
<code>图8: Bahadanau的论文模型在测试集上长序列的 BLEU 分数明显更优</code>
</center>
<p><br/><br/>
由此可见，Attention机制提高了编码器-解码器网络的性能，但速度瓶颈仍然是RNN必须逐字处理的工作机制。</p>
<p>很自然的，我们会接着考虑：<strong>可以用更好的模型替换掉RNN这种顺序结构的模型吗？</strong></p>
<p>答案是：<strong>Yes, attention is all you need!</strong></p>
<p>哈哈，有点一语双关的感觉（说不定Google
Brain团队当时起名的时候就是这么想的）。在2017年，我们得到了一个令人满意的答案，一款名为Transformer的“神器”横空出世，至今风头不减当年。它也是我们这篇文章要探讨的主题。</p>
<h1 id="transformer简介">Transformer简介</h1>
<p>正如刚刚提到的，<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">[论文3]Ashish Vaswani., 2017 .
Attention Is All You Need</a>
第一次正式介绍了一款在翻译领域超越了RNN的新模型Transformer，<strong>Transformer是一种Encoder-Decoder架构，使用Attention机制来处理输入和生成输出。</strong></p>
<h1 id="transformer对比rnn">Transformer对比RNN</h1>
<p>Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成，更准确地讲，<strong>Transformer由且仅由Self
Attenion和Feed Forward Neural
Network组成</strong>。采用Attention机制是因为考虑到RNN（或者LSTM，GRU等）的计算限制是顺序处理的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：</p>
<ol type="1">
<li><p>时间<span class="math inline">\(t\)</span>的计算依赖<span
class="math inline">\(t-1\)</span>时刻的计算结果，这样限制了模型的并行能力。</p></li>
<li><p>顺序计算的过程中RNN会因为长期依赖导致信息丢失问题。</p></li>
</ol>
<p>Transformer的提出解决了上面两个问题：</p>
<ol type="1">
<li><p>不采用类似RNN的顺序结构，而是具有并行性，符合现有的GPU框架。</p></li>
<li><p>使用了Attention机制，将序列中的任意两个位置之间的距离缩小为一个常量，采用了和LSTM不同的思路，从而允许处理不同长度的输入序列，并且摆脱了长期依赖的影响。</p></li>
</ol>
<p>这也就是为什么它在机器翻译任务种打败了以前基于RNN的Encoder-Decoder模型，并且在其他应用中也非常受欢迎的原因。</p>
<h1 id="transformer原理">Transformer原理</h1>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg" width = "50%" />
<center>
<code>图9: Transformer模型的整体架构</code>
</center>
<p><br/><br/>
上图是Transformer的整体架构图，结构上看起来和Encoder-Decoder模型很相似，左边是Encoder部分，右边是Decoder部分。为了方便理解，下面把Transformer分成四个部分进行详细说明:</p>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts.jpg" width = "50%" />
<center>
<code>图10: Transformer模型可分为4个组成部分</code>
</center>
<p>简单介绍一下各部分的任务：</p>
<ul>
<li><p><strong>Input</strong>：输入是单词的Embedding再加上位置编码，然后进入编码器或解码器。</p></li>
<li><p><strong>Encoder</strong>：这个结构可以循环很多次（N次），也就是说有很多层（N层）。每一层又可以分成Attention层和全连接层，再额外加了一些处理，比如Skip
Connection，做跳跃连接，然后还加了Normalization层。其实它本身的模型还是很简单的。</p></li>
<li><p><strong>Decoder</strong>：同样可以循环N次，第一次输入是前缀信息，之后的就是上一次产出的Embedding，加入位置编码，然后进入一个可以重复很多次的模块。该模块可以分成三块来看，第一块也是Attention层，第二块是Cross
Attention，不是Self
Attention，第三块是全连接层。也用了跳跃连接和Normalization。</p></li>
<li><p><strong>Output</strong>：最后的输出要通过Linear层（全连接层），再通过Softmax做预测。</p></li>
</ul>
<p>以英-中翻译为例：假设我们输入<code>"Why do we work?"</code>，输出可以是<code>"为什么我们要工作？"</code>。那么Transformer的工作步骤是：</p>
<ol type="1">
<li>输入自然语言序列到编码器: Why do we work?</li>
<li>编码器输出的隐藏层, 再输入到解码器;</li>
<li>输入<span
class="math inline">\(&lt;start&gt;\)</span>(起始)符号到解码器;</li>
<li>得到第一个字"为";</li>
<li>将得到的第一个字"为"落下来再输入到解码器;</li>
<li>得到第二个字"什";</li>
<li>将得到的第二字再落下来,
重复5、6步的相关动作依次生成“么”、“我”、“们”、“要”、“工”、“作”、“
？”，直到解码器输出<span
class="math inline">\(&lt;end&gt;\)</span>(终止符),
则代表序列生成完成。</li>
</ol>
<p>我们可以对Transformer的工作过程1~6进行可视化，如下所示:</p>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_process.png" width = "80%" />
<center>
<code>图11: 图解Transformer的工作过程</code>
</center>
<h2 id="input">Input</h2>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(1).png" width = "100%" />
<center>
<code>图12: Transformer模型的输入部分</code>
</center>
<p><br/><br/> 我们同样以上篇采用过的英-中翻译例子，我们输入"Tom chase
Jerry"，期待输出的翻译结果为"汤姆追逐杰瑞"。</p>
<p>Transformer的Decoder的输入与Encoder的输入处理方法步骤是一样的，一个接受source数据，一个接受target数据，对应例子里面就是：Encoder接受英文"Tom
chase
Jerry"，Decoder接受中文"汤姆追逐杰瑞"。当然对Decoder来说，只是在有target数据时（也就是在进行有监督训练时）才会接受Outputs
Embedding，进行预测时则不会接收。那么下面，以Encoder为例，描述输入过程和细节分析：</p>
<ol type="1">
<li><p>首先，向Transformer输入文本 <code>"Tom chase Jerry"</code>
。</p></li>
<li><p>随后，Transformer会将原始的英文句子"Tom chase
Jerry"进行分词(tokenization)，比如得到单词序列['[START]', 'Tom',
'chase', 'Jerry',
'[END]']。请注意，分词后的token包括“[START]”和“[END]”标记。</p></li>
<li><p>接下来，将每个单词映射到对应的词向量上。<strong>实际上Transformer使用的是512维的向量</strong>，那么假设我们使用4维的词向量表示单词，那么对于单词'[START]',
'Tom', 'chase', 'Jerry', '[END]'，它们的词向量可能是：</p></li>
</ol>
<p>        <span class="math inline">\(v_{[START]} :
[-0.1,0.2,-0.3,0.4]\)</span></p>
<p>        <span class="math inline">\(v_{Tom} : [0.5, 0.2, -0.1,
0.3]\)</span></p>
<p>        <span class="math inline">\(v_{chase} : [-0.2, 0.4, 0.1,
0.6]\)</span></p>
<p>        <span class="math inline">\(v_{Jerry} : [-0.2, 0.3, 0.1,
-0.5]\)</span></p>
<p>        <span class="math inline">\(v_{[END]} :
[0.3,-0.2,0.1,-0.4]\)</span></p>
<ol start="4" type="1">
<li>通过使用sin和cos函数来生成位置向量，这种方式可以向模型描述各个单词之间的顺序关系，并且能够在维度空间上均匀分布位置向量。可以假设'[START]'的位置编号为1，"Tom"的位置编号为2，"chase"的位置编号为3，"Jerry"的位置编号为4，"[END]"的位置编号为5。那么它们位置向量可能是：</li>
</ol>
<p>        <span class="math inline">\(p_1 =
[sin(\frac{1}{10000^{2*\frac{1}{4}}}),
cos(\frac{1}{10000^{2*\frac{1}{4}}}),sin(\frac{2}{10000^{2*\frac{1}{4}}}),
cos(\frac{2}{10000^{2*\frac{1}{4}}}) ]\)</span></p>
<p>        <span class="math inline">\(p_2 =
[sin(\frac{2}{10000^{2*\frac{1}{4}}}),
cos(\frac{2}{10000^{2*\frac{1}{4}}}),sin(\frac{3}{10000^{2*\frac{1}{4}}}),
cos(\frac{3}{10000^{2*\frac{1}{4}}}) ]\)</span></p>
<p>        <span class="math inline">\(p_3 =
[sin(\frac{3}{10000^{2*\frac{1}{4}}}),
cos(\frac{3}{10000^{2*\frac{1}{4}}}),sin(\frac{4}{10000^{2*\frac{1}{4}}}),
cos(\frac{4}{10000^{2*\frac{1}{4}}}) ]\)</span></p>
<p>        <span class="math inline">\(p_4 =
[sin(\frac{4}{10000^{2*\frac{1}{4}}}),
cos(\frac{4}{10000^{2*\frac{1}{4}}}),sin(\frac{5}{10000^{2*\frac{1}{4}}}),
cos(\frac{5}{10000^{2*\frac{1}{4}}}) ]\)</span></p>
<p>        <span class="math inline">\(p_5 =
[sin(\frac{5}{10000^{2*\frac{1}{4}}}),
cos(\frac{5}{10000^{2*\frac{1}{4}}}),sin(\frac{6}{10000^{2*\frac{1}{4}}}),
cos(\frac{6}{10000^{2*\frac{1}{4}}}) ]\)</span></p>
<ol start="5" type="1">
<li>最后，输入到Transformer中的序列就是由词向量和位置向量相加得到的，例如，“Tom
chase Jerry”的输入序列可能是:</li>
</ol>
<p>        [<span class="math inline">\(v_ {[START]}\)</span> + <span
class="math inline">\(p_1\)</span> , <span
class="math inline">\(v_{Tom}\)</span> + <span
class="math inline">\(p_2\)</span>, <span
class="math inline">\(v_{chase}\)</span> + <span
class="math inline">\(p_3\)</span>, <span
class="math inline">\(v_{Jerry}\)</span> + <span
class="math inline">\(p_4\)</span>, <span class="math inline">\(v_
{[END]}\)</span> + <span class="math inline">\(p_5\)</span>]</p>
<p>下面分别介绍过程中相关的知识点：</p>
<h3 id="分词tokenization">分词(Tokenization)</h3>
<blockquote>
<p>什么是分词</p>
</blockquote>
<p>Transformer
模型的输入通常是序列数据，如文本、语音等。这些数据在输入之前需要进行预处理，其中一个重要的步骤就是分词。<strong>分词是获取词向量之前的一个必要步骤。</strong></p>
<p>分词是NLP的一个重要概念，<strong>表示将文本(text)切分成符号(token)的过程。</strong>token可以是以下三种类型：</p>
<ol type="1">
<li><p>单词 (word) —— 例如，短语“dogs like
cats”由三个词标记组成：“dogs”、“like”和“cats”。</p></li>
<li><p>字符 (character) —— 例如，短语“your
fish”由九个字符标记组成。<strong>（请注意，空格算作标记之一）</strong></p></li>
<li><p>子词 (subword) ——
其中单个词可以是单个标记或多个标记。子词由词根、前缀或后缀组成。例如，使用子词作为标记的语言模型可能会将单词“dogs”视为两个标记（词根“dog”和复数后缀“s”）。相同的语言模型可能会将单个词“更高”视为两个子词（词根“high”和后缀“er”）。</p></li>
</ol>
<blockquote>
<p>分词的作用？</p>
</blockquote>
<p>通过分词，模型可以将文本分成若干个单独的词汇单元，可以减少翻译系统需要处理的信息量，提高翻译效率和准确性，同时更好地维护语言的语法结构。</p>
<h3 id="嵌入embedding">嵌入(Embedding)</h3>
<p>NLP中，<strong>使用分词后的词向量作为模型输入</strong>是常见的做法，而词向量是一种特定类型的嵌入。</p>
<blockquote>
<p>什么是嵌入？</p>
</blockquote>
<p>嵌入是NLP中使用的一种技术，以机器学习模型能够理解的数字格式表示单词、短语甚至整个句子。而其中的词向量用于在多维空间中表示单词。</p>
<blockquote>
<p>嵌入的作用？</p>
</blockquote>
<p>嵌入的目标在于<strong>通过机器学习模型能够理解的方式捕捉一个词的含义和上下文，并将词语的语义信息转化为数字，使得它们可以被计算机理解。</strong></p>
<p>当然从另一种角度来说，也可以认为嵌入是一种降维的形式。</p>
<p>比如除了嵌入之外，单词或句子也可以被表示为one
hot编码向量，它是高维和稀疏的，当然它虽然避免了线性不可分的问题，但也意味着向量中的大多数元素都是零。这可能会导致计算效率低下，而且机器学习模型也难以通过学习来理解单词或句子的含义和相互关系。</p>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/onehot.png" width = "80%" />
<center>
<code>图13: 图解one hot的编码方式</code>
</center>
<p><br/><br/>
相反，嵌入是一种以较低维度的密集格式表示单词或句子的方法，更适合机器学习模型。<strong>通过将单词或句子映射到一个较低的维度空间，嵌入可以捕捉到单词或句子的含义和相互关系，同时舍弃不太重要的信息。</strong></p>
<p><strong>常见的词向量编码方式是word2vec。</strong>相比于one-hot编码只允许我们将单词作为单个不同的条目来解释，word2vec允许我们寻找每个单词和其他单词的关系，从而创建更好的特征表示。</p>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/word2.png" width = "80%" />
<center>
<code>图14: 图解word2vec的编码方式</code>
</center>
<p><br/><br/>
word2vec模型使用神经网络来学习一个词的向量表示。它将每个词映射到一个高维向量，其中语义相似的词在向量空间中是紧密相连的。例如，在word2vec模型中，"banana"这个词的向量表示可能是[-0.2,
0.5, 0.1, 0.3,
...]，代表这个词的含义和上下文，(一个词向量往往是300-1000维，向量中的每个元素代表这个词的意义或上下文的一个维度，单个数字的含义本身不可解释，只有在与其他词或句子的向量相关时才有意义)。</p>
<blockquote>
<p>输入的embedding是否需要经过训练</p>
</blockquote>
<p>将单词<span
class="math inline">\(x\)</span>的embedding输入encoder，有两种常见的选择：</p>
<ol type="1">
<li><p>使用Pre-trained的<strong>embeddings并固化</strong>，这种情况下embedding取自一个预先训练好的模型，在训练过程中不更新。实际就是一个Lookup
Table（查找表）。</p></li>
<li><p>对其进行随机初始化（当然也可以选择Pre-trained的结果），但<strong>设为Trainable</strong>。这样在training过程中不断地对embeddings进行改进。即End2End（端到端）训练方法，意味着模型从头到尾都被训练，所有的参数，包括嵌入都在训练过程中被更新。<strong>这也是Transformer选择的做法。</strong></p></li>
</ol>
<p><strong>源码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="built_in">super</span>(Embeddings, self).__init__()</span><br><span class="line">        self.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        self.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.lut(x) * math.sqrt(self.d_model)</span><br></pre></td></tr></table></figure>
<p>这段代码定义了一个名为Embeddings的类，继承自PyTorch中nn.Module类。</p>
<p>在初始化方法__init__中，首先使用nn.Embedding类创建了一个名为lut的词嵌入层，vocab参数表示词汇表大小，d_model参数表示词向量维度。同时还有一个d_model变量记录了词向量维度。</p>
<p>在forward方法中，将输入x经过lut层，乘上根号d_model的值，再返回。这里的乘法操作是为了缩放词向量的维度，这是Transformer模型中的常规做法。</p>
<h3 id="位置编码positional-encoding">位置编码(Positional Encoding)</h3>
<blockquote>
<p>为什么需要知道每个单词的位置，并且添加位置编码呢？</p>
</blockquote>
<p>首先，咱们知道，一句话中同一个词如果的出现位置不同，意思可能发生翻天覆地的变化，就比如：我欠他100W
和
他欠我100W。这两句话的意思一个地狱一个天堂。可见获取词语出现在句子中的位置信息是一件很重要的事情。</p>
<p>而Transformer没有用RNN也没有卷积，它使用的注意力机制(主要是由于self
attention)，不能获取词语位置信息，就算打乱一句话中词语的位置，每个词还是能与其他词之间计算attention值。所以为了让模型能利用序列的顺序，必须输入序列中词的位置，所以Transformer采用的方法是给每一个词向量，包括包括“[START]”和“[END]”都需要添加位置编码。</p>
<blockquote>
<p>怎么得到positional encoding呢？</p>
</blockquote>
<p>Transformer使用的是正余弦位置编码。位置编码通过使用不同频率的正弦、余弦函数生成，然后和对应位置的词向量相加，位置向量维度必须和词向量的维度一致。过程如上图，PE（positional
encoding）计算公式如下：</p>
<p><span class="math display">\[PE_{(pos,2i)} =
sin(\frac{pos}{10000^{\frac{2 i}{d_{model}}}})\]</span> <span
class="math display">\[PE_{(pos,2i+1)} = cos(\frac{pos}{10000^{\frac{2
i}{d_{model}}}})\]</span></p>
<p>解释一下上面的公式：</p>
<ul>
<li><p><span
class="math inline">\(pos\)</span>表示单词在句子中的绝对位置，<span
class="math inline">\(pos=0, 1, 2, \dots\)</span>，例如：Jerry在"Tom
chase Jerry"中的pos=2；</p></li>
<li><p><span
class="math inline">\(d_{model}\)</span>表示词向量的维度，一般<span
class="math inline">\(d_{model}\)</span>=512；2i和2i+1表示奇偶性，i表示词向量中的第几维，例如这里<span
class="math inline">\(d_{model}\)</span>=512，故<span
class="math inline">\(i=0, 1, 2, \dots, 255\)</span>。</p></li>
</ul>
<p>至于上面两个公式是怎么得来的，其实不重要，很有可能是作者根据经验自己造的，而且公式也不唯一，后续Google在Bert中的采用类似词向量的方法通过训练PE，说明<strong>这种求位置向量的方法还是存在一定问题滴</strong>。</p>
<blockquote>
<p>为什么是将positional encoding与词向量相加，而不是拼接呢？</p>
</blockquote>
<p>事实上，拼接或者相加都可以，只是词向量本身的维度（512维）就已经蛮大了。再拼接一个512维的位置向量（变成1024维）这样训练起来会相对慢一些，影响学习效率。两者既然效果差不多，那当然是选择学习难度较小的相加了。</p>
<p><strong>源码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implement the PE function.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Compute the positional encodings once in log space.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>) <span class="comment">#pe.unsqueeze(0) 这行代码是把pe变量在第0维加上一个维度，</span></span><br><span class="line">        <span class="comment">#这样pe变量就变成了一个形状为(1, max_len, d_model)的张量。</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe) <span class="comment">#将pe变量注册为一个buffer</span></span><br><span class="line">        <span class="comment">#这样pe变量的值就不会在计算梯度时被统计了。</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<p>这段代码实现了Transformer模型中的位置编码，主要用于确定输入序列中每个单词的位置信息问题。</p>
<ul>
<li>d_model: 定义词向量的维度。</li>
<li>dropout: 一种正则化方式，随机让部分网络参数为0，以防过拟合。</li>
<li>max_len: 输入句子的最大长度。</li>
</ul>
<p>在初始化中，首先使用nn.Dropout类创建了一个dropout层。然后根据论文中的公式，预先计算出位置编码，并将其存储在pe变量中。</p>
<p>在forward函数中，将输入x加上pe变量中对应位置的位置编码，最后进行dropout操作（一种正则化方式，在训练时会随机将部分网络参数设置为0，从而防止过拟合）。</p>
<p>需要注意的是，这里的<strong>位置编码是在计算时预先计算好了并存储下来，而不是在运行时动态计算，这样可以减少计算量</strong>。</p>
<h2 id="encoder">Encoder</h2>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(2).png" width = "40%" />
<center>
<code>图15: Transformer模型的编码器部分</code>
</center>
<p><br/><br/>
在论文中，有6层编码器，即“Nx”的N=6。编码器的每一层是由4个sub-layer（子层）组成的：
第一个子层是多头注意力机制 (multi-head self-attention
mechanism)，它负责计算输入序列之间的关系并生成新的表示。
第二个子层是残差连接 (residual connection)
，它将第一个子层的输出与输入相加。 第三个子层是归一化
(normalization)，它对第二个子层的输出进行归一化。
第四个子层是全连接前馈层（feed-forward
layer），它对第三个子层的输出进行非线性变换。</p>
<p><strong>源码如下：</strong> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;Apply residual connection to any sublayer with the same size.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>
这段代码实现了一个transformer编码器中的一个编码层EncoderLayer。这个编码层由两部分组成，分别是自注意力机制self-attn和前馈网络feed_forward。在初始化时，通过传入size、self_attn、feed_forward和dropout参数来初始化编码层。</p>
<p>在前向传播过程中，首先使用自注意力机制对输入x进行处理，然后将处理后的结果经过前馈网络进一步处理，最后返回处理结果。</p>
<p>具体来说，使用sublayer<a
href="x,%20lambda%20x:%20self.self_attn(x,%20x,%20x,%20mask)">0</a>
调用SublayerConnection类对输入进行处理，进行自注意力机制。
之后使用sublayer<a
href="x,%20self.feed_forward">1</a>将结果经过前馈网络进一步处理。</p>
<h3 id="multi-head-attention">Multi-Head Attention</h3>
<p><strong>Multi-Head Attention是Self
Attention机制的进一步细化，因此先从Self Attention讲起：</strong></p>
<h4 id="从self-attention讲起">从Self Attention讲起</h4>
<p>假设下面的句子是我们要翻译的输入句子：</p>
<p><code>The animal didn't cross the street because it was too tired</code></p>
<p>这句话中的“it”指的是什么？指的是街道还是动物？这对人来说是一个简单的问题，但对算法模型来说却不那么简单。</p>
<img
src="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png" />
<center>
<code>图17: 对单词“it”编码时Attention大部分集中在“The animal”上</code>
</center>
<p>在该例子中，当模型处理“it”这个词时，self attention
允许它把“it”和“animal”联系起来。而广泛地说，<strong>当模型处理每个单词（输入序列中的每个位置）时，自注意力允许它查看输入序列中的其他位置以寻找有助于更好地编码该单词的线索。</strong></p>
<p>而上篇我们详细讲解过，Attention的本质，这里我们简单描述下：<strong>Attention实际上做的就是数据库中的检索操作，本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。</strong></p>
<p>这个类似于搜索引擎或者推荐系统的基本原理，以谷歌Google为例:</p>
<ul>
<li>用户给定需查询的问题(Query)</li>
<li>Google后台有各种文章标题(Key)和文章本身(Value)</li>
</ul>
<p>通过定义并计算问题Query与文章标题Key的相关性，用户就可以找到最匹配的文章Value。当然Attention的目标不是通过Query得到Value，而是通过Query得到Value的加权和。</p>
<p>那么回到计算Self Attention的过程上来，这次我们以新的输入“Thinking
Machines”为例进行过程描述：</p>
<p><strong>单词级别-第一步：</strong></p>
<p>从每个编码器的输入向量（词向量+位置编码）创建三个向量：一个Query查询向量、一个Key键向量和一个Value值向量。</p>
<img
src="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png" /><br />

<center>
<code>图18: qkv向量是通过将embedding分别乘以训练的三类权重矩阵而创建的</code>
</center>
<p><br/><br/> 比如产生"Thinking"的三个向量的过程如下：</p>
<ol type="1">
<li>将<span class="math inline">\(X_1\)</span>乘以<span
class="math inline">\(W^Q\)</span>权重矩阵产生<span
class="math inline">\(q_1\)</span>，即与该词关联的Query向量。</li>
<li>将<span class="math inline">\(X_1\)</span>乘以<span
class="math inline">\(W^K\)</span>权重矩阵产生<span
class="math inline">\(k_1\)</span>，即与该词关联的Key向量。</li>
<li>将<span class="math inline">\(X_1\)</span>乘以<span
class="math inline">\(W^V\)</span>权重矩阵产生<span
class="math inline">\(v_1\)</span>，即与该词关联的Value向量。</li>
</ol>
<p>请注意，qkv向量的维度小于embedding向量。它们的维数是
64，而嵌入和编码器输入/输出向量的维数是
512。它们不必更小，这是一种使multi-head
attention（大部分）计算保持不变的架构选择。</p>
<p><strong>单词级别-第二步</strong></p>
<p>计算每个单词的分数，分数是通过Query向量与当前正在评分单词的Key向量的点积计算得出的。<strong>当我们在特定位置对单词进行编码时，分数决定了将多少注意力放在输入句子的其他部分。</strong></p>
<p>假设我们正在计算本例中第一个词“Thinking”的自注意力。我们需要根据这个词对输入句子的每个词进行评分:</p>
<img
src="https://jalammar.github.io/images/t/transformer_self_attention_score.png" /><br />

<center>
<code>图19: 单词的分数是对应位置Query向量和Key向量的点积</code>
</center>
<p><strong>单词级别-第三步</strong></p>
<p>将分数除以8，这个数字是论文中使用的关键向量维度64的平方根。</p>
<blockquote>
<p>上式为什么要除以<span
class="math inline">\(\sqrt{d_k}\)</span>呢？</p>
</blockquote>
<p>因为为了防止维数过高时<span
class="math inline">\(QK^T\)</span>的值过大导致softmax函数反向传播时发生梯度消失。</p>
<blockquote>
<p>为什么是<span class="math inline">\(\sqrt{d_k}\)</span>而不是<span
class="math inline">\(d_k\)</span>呢？</p>
</blockquote>
<p>这就是个经验值，从理论上来说，就是还需要让<span
class="math inline">\(QK^T\)</span>的值适度增加，但不能过度增加，如果是<span
class="math inline">\(d_k\)</span>的话，可能就不增加了。</p>
<p><strong>单词级别-第四步</strong></p>
<p>然后通过 softmax 操作传递结果。softmax
对分数进行归一化处理，使它们都为正且加起来为 1。</p>
<p>softmax
分数决定了在这个位置上，输入句子的每个单词会被投入的注意力占比。</p>
<img
src="https://jalammar.github.io/images/t/self-attention_softmax.png" /><br />

<center>
<code>图20: 缩小维度并进行归一化处理。</code>
</center>
<p><strong>单词级别-第五步</strong></p>
<p>将每个Value向量乘以 softmax
分数（准备将它们相加）。这里的直觉是保持我们想要关注的单词的值不变，并淹没不相关的单词（例如，通过将它们乘以像
0.001
这样的小数字）。然后是对加权值向量求和。这会在该位置产生自注意层的输出。</p>
<img
src="https://jalammar.github.io/images/t/self-attention-output.png" /><br />

<center>
<code>图21: 输出第一个单词的self attention计算结果</code>
</center>
<p><br/><br/>
自注意力计算到此结束。生成的向量是我们可以发送到前馈神经网络的向量。然而，在实际实现中，此计算以矩阵形式完成。</p>
<blockquote>
<p>为什么实际要用矩阵而不是神经网络呢？</p>
</blockquote>
<p>因为矩阵运算能用GPU加速，会更快，同时参数量更少，更节省空间。</p>
<p>既然我们已经看到了单词级别的计算过程，那么让我们来看看Self
Attention实际使用的矩阵计算：</p>
<p><strong>矩阵计算-第一步</strong></p>
<p>计算Query,
Key和Value共计三个矩阵。为此，我们将嵌入打包到矩阵X中，然后将其乘以我们训练过的权重矩阵
<span class="math inline">\((W^Q、W^K、W^V)\)</span>。</p>
<img
src="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png" />
<center>
<code>图22: 计算QKV矩阵</code>
</center>
<p><strong>矩阵计算-第二步</strong></p>
<p>由于我们处理的是矩阵，我们可以将单词形式的第二步到第五步压缩为一个公式来计算自注意力层的输出。</p>
<img
src="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" /><br />

<center>
<code>图23: 矩阵形式的self attention计算公式</code>
</center>
<h4 id="multi-head-attention原理">Multi-head Attention原理</h4>
<p>该论文通过添加一种称为Multi-head
Attention的机制，进一步细化了自注意力层。主要体现在两个方面：</p>
<ol type="1">
<li><p>它扩展了模型关注不同位置的能力。比如要翻译像“The animal didn't
cross the street because it was too
tired”这样的句子，知道“it”指的是哪个词会很有用。</p></li>
<li><p>它为注意力层提供了多个representation subspaces
(表示子空间)。正如我们接下来将看到的，对于Multi-head
Attention，我们有多组QKV权重矩阵，其中的每一个都是随机初始化的。训练之后，每个集合用于将输入（初始输入或来自较低编码器/解码器的向量）投影到不同的表示子空间中。</p></li>
</ol>
<img
src="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png" /><br />

<center>
<code>图24: Multi-head Attention每个头各产生不同的 QKV 矩阵</code>
</center>
<p><br/><br/></p>
<img
src="https://jalammar.github.io/images/t/transformer_attention_heads_z.png" /><br />

<center>
<code>图25: 通过相同的计算过程，8个attention head最终会得到8个不同的Z矩阵</code>
</center>
<p><br/><br/></p>
<p>这给我们带来了一些挑战。前馈层不需要8个矩阵——它需要一个矩阵（每个单词一个向量）。所以我们需要一种方法将这8个压缩成一个矩阵。因此我们连接这些矩阵，然后将它们乘以一个额外的权重矩阵
<span class="math inline">\(W^O\)</span>。</p>
<img
src="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png" /><br />

<center>
<code>图26: 将多个Z矩阵通过矩阵乘法合并成总的Z矩阵</code>
</center>
<p><br/><br/> <img
src="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" /></p>
<p>现在我们已经谈到了attention
head，让我们重新审视我们之前的例子，看看当我们在示例句子中对单词“it”进行编码时，不同的attention
head集中在什么地方：</p>
<img
src="https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png" /><br />

<center>
<code>图28: 不同attention head对it的注意力权重不同</code>
</center>
<p><br/><br/> 当我们对“it”这个词进行编码时，一个attention head最关注“the
animal”，而另一个attention head则关注“tired”。我认为这说明不同attention
head很可能是从不同角度来理解it和其他单词的关系，比如it的指代的对象是“the
animal”，而这个对象所处的状态是“tired”。因为Attention是注意力的意思而不是表示相等的意思，那么从不同角度看待同一个事物，得到不同的答案自然也是没问题的。</p>
<h3 id="add-normalize">Add &amp; Normalize</h3>
<p>可以注意到编码器/解码器的每个子层（比如self attention,
ffnn）之后都带有一个 <strong>Add &amp; Normalize</strong>。</p>
<img
src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png" /><br />

<center>
<code>图29: 每个编码器中都有2个Add &amp; Normalize子层</code>
</center>
<p><br/><br/></p>
<p>我之前没有听说过残差连接，因此看着这张图好久也没看出residual这个词体现在哪里，问了“Chat老师”才明白，Add表示残差连接，Norm表示LayerNorm，残差连接来源于<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">[论文4]Kaiming He. 2015. Deep
Residual Learning for Image Recognition</a>，LayerNorm来源于<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">[论文5]Jimmy Lei Ba,. 2016.
Layer Normalization</a>。</p>
<p>Encoder端和Decoder端每个子模块实际的输出为：<span
class="math inline">\(LayerNorm(x+Sublayer(x))\)</span>，其中<span
class="math inline">\({Sublayer}(x)\)</span>为子模块的输出。这样做有助于模型更好地捕捉长期依赖关系。</p>
<p>关于这一部分的更多技术细节我以问答的形式展示在下面。</p>
<p><strong>问题一</strong></p>
<blockquote>
<p>什么是残差连接</p>
</blockquote>
<p>残差连接（residual
connections）是一种网络设计方法，就是在每一层的输入和输出之间添加一个残差连接，并对结果进行标准化。（其实也就是Add
&amp;
Normolize）这种方法能够有效地缓解深度网络中的梯度消失问题，帮助模型更好地捕捉长期依赖关系。</p>
<p>例如在Transformer中，设计一个多头注意力机制的层，输入为x，输出为y，残差连接就是
y=norm(x+multi-head-attention(x))</p>
<p>这样做的好处就是，当网络层数较深时，残差连接能够有效减少梯度消失问题，简化网络训练过程，使网络能够更好地捕捉长期依赖关系。</p>
<p><strong>问题二</strong></p>
<blockquote>
<p>Add操作做了什么</p>
</blockquote>
<p>Add，就是在Z的基础上加了一个残差块X。</p>
<p><img
src="https://img-blog.csdnimg.cn/94d2b8dbc1ea432c9ca85f62e29cb454.png#pic_center" />
上图就是构造的一个残差块。</p>
<ul>
<li>X是这一层残差块的输入值</li>
<li>F(X)是经过第一层线性变化并激活后的输出，也称为残差</li>
</ul>
<p>该图表示在残差网络中，第二层进行线性变化之后、激活之前，F(X)加入了这一层输入值X，然后再进行激活后输出。在第二层输出值激活前加入X，这条路径称作shortcut连接。</p>
<p>加入残差块X的目的是为了防止在深度神经网络训练中发生退化问题，退化的意思就是深度神经网络通过增加网络的层数，Loss逐渐减小，然后趋于稳定达到饱和，然后再继续增加网络层数，Loss反而增大。</p>
<p><strong>问题三</strong></p>
<blockquote>
<p>为什么深度神经网络会发生退化？</p>
</blockquote>
<p>举个例子：假如某个神经网络的最优网络层数是18层，但是我们在设计的时候并不知道到底多少层是最优解，本着层数越深越好的理念，我们设计了32层，那么32层神经网络中有14层其实是多余地，我们要想达到18层神经网络的最优效果，必须保证这多出来的14层网络必须进行恒等映射，恒等映射的意思就是说，输入什么，输出就是什么，可以理解成F(x)=x这样的函数，因为只有进行了这样的恒等映射咱们才能保证这多出来的14层神经网络不会影响我们最优的效果。<br />
但现实是神经网络的参数都是训练出来地，要想保证训练出来地参数能够很精确的完成F(x)=x的恒等映射其实是很困难地。多余的层数较少还好，对效果不会有很大影响，但多余的层数一多，可能结果就不是很理想了。这个时候大神们就提出了ResNet
残差神经网络来解决神经网络退化的问题。</p>
<p><strong>问题四</strong></p>
<blockquote>
<p>为什么添加了残差块能防止神经网络退化问题呢？</p>
</blockquote>
<p>咱们再来看看添加了残差块后，咱们之前说的要完成恒等映射的函数变成什么样子了。是不是就变成h(X)=F(X)+X，我们要让h(X)=X，那么是不是相当于只需要让F(X)=0就可以了，这里就巧妙了！神经网络通过训练变成0是比变成X容易很多地，因为大家都知道咱们一般初始化神经网络的参数的时候就是设置的[0,1]之间的随机数嘛。所以经过网络变换后很容易接近于0。举个例子：</p>
<p><img
src="https://img-blog.csdnimg.cn/20200326001443472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /></p>
<p>假设该网络只经过线性变换，没有bias也没有激活函数。我们发现因为随机初始化权重一般偏向于0，那么经过该网络的输出值为[0.6
0.6]，很明显会更接近与[0 0]，而不是[2
1]，相比与学习h(x)=x，模型要更快到学习F(x)=0。<br />
并且ReLU能够将负数激活为0，过滤了负数的线性变化，也能够更快的使得F(x)=0。这样当网络自己决定哪些网络层为冗余层时，使用ResNet的网络很大程度上解决了学习恒等映射的问题，用学习残差F(x)=0更新该冗余层的参数来代替学习h(x)=x更新冗余层的参数。</p>
<p>这样当网络自行决定了哪些层为冗余层后，通过学习残差F(x)=0来让该层网络恒等映射上一层的输入，使得有了这些冗余层的网络效果与没有这些冗余层的网络效果相同，这样很大程度上解决了网络的退化问题。</p>
<p>到这里，关于Add中为什么需要加上一个X，要进行残差网络中的shortcut你清楚了吗？Transformer中加上的X也就是Multi-Head
Attention的输入，X矩阵。</p>
<p><strong>问题五</strong></p>
<blockquote>
<p>为什么要进行Normalize呢？<br />
在神经网络进行训练之前，都需要对于输入数据进行Normalize归一化，目的有二：1，能够加快训练的速度。2.提高训练的稳定性。</p>
</blockquote>
<p><strong>问题六</strong></p>
<blockquote>
<p>为什么使用Layer Normalization（LN）而不使用Batch
Normalization（BN）呢？</p>
</blockquote>
<p><img
src="https://img-blog.csdnimg.cn/20200326202939489.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /><br />
先看图，LN是在同一个样本中不同神经元之间进行归一化，而BN是在同一个batch中不同样本之间的同一位置的神经元之间进行归一化。<br />
BN是对于相同的维度进行归一化，但是咱们NLP中输入的都是词向量，一个300维的词向量，单独去分析它的每一维是没有意义地，在每一维上进行归一化也是适合地，因此这里选用的是LayerNorm。</p>
因此，我们可以可视化Add &amp; Normalize操作，如下图所示： <img
src="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png" /><br />

<center>
<code>图30: 可视化Add &amp; Normalize操作</code>
</center>
<p><strong>源码如下：</strong> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;Apply residual connection to any sublayer with the same size.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br></pre></td></tr></table></figure>
这段代码实现了transformer编码器中一个残差连接的类
SublayerConnection。</p>
<p>在初始化时，通过传入size和dropout参数来初始化类的norm和dropout层。</p>
<p>在前向传播过程中，首先对输入x使用norm层进行归一化处理，再将其传入子层sublayer进行处理。最后使用dropout层将处理后的结果进行dropout操作，最后使用残差连接将dropout后的结果与原始输入x相加。</p>
<p>具体来说，使用self.norm(x)对输入x进行归一化处理，之后使用sublayer(self.norm(x))将归一化后的x经过子层进行处理，最后使用self.dropout(sublayer(self.norm(x)))进行dropout操作，最后使用x
+
self.dropout(sublayer(self.norm(x)))使用残差连接将dropout后的结果与原始输入x相加。最后返回残差连接后的结果。</p>
<h3 id="feed-forward">Feed Forward</h3>
<p>每一层经过self attention之后，还会有一个Feed Forward
Network(FFN)，这个FFN的作用就是空间变换。FFN包含了2层linear
transformation层，中间的激活函数是ReLu。</p>
<p><span class="math display">\[FFN(x) = \max(0, xW_1 + b_1 )W_2 +
b_2\]</span></p>
<blockquote>
<p>attention层的output最后会和 <span class="math inline">\(W_O\)</span>
相乘，为什么这里又要增加一个2层的FFN网络？</p>
</blockquote>
<p>这是因为FFN的加入引入了非线性(ReLu激活函数)，变换了attention
output的空间,
从而增加了模型的表现能力。把FFN去掉模型也是可以用的，但是效果差了很多。</p>
<p><strong>源码如下：</strong> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implements FFN equation.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br></pre></td></tr></table></figure></p>
<p>这段代码实现了一个全连接前馈层（feed-forward
layer）。这个层的输入是一个 <span class="math inline">\(d_model\)</span>
维的向量 x，输出也是一个 <span class="math inline">\(d_model\)</span>
维的向量。代码中，它包含了两个线性变换和一个dropout，它们分别是 <span
class="math inline">\(w_1\)</span>, <span
class="math inline">\(w_2\)</span> 和 dropout。</p>
<p><span class="math inline">\(w_1\)</span>是一个线性变换，它将输入 x
转换成一个 d_ff 维的向量。</p>
<p><span class="math inline">\(w_2\)</span>是一个线性变换，它将 <span
class="math inline">\(w_1\)</span> 的输出转换成 d_model 维的向量。</p>
<p>dropout是为了防止过拟合。</p>
<p>F.relu是激活函数，它会把w_1的输出中小于0的数转换成0.</p>
<p>最后，输出是<span
class="math inline">\(w_2\)</span>的输出和dropout的结果。</p>
<p>总而言之，这个类实现了一个全连接层，它接受 <span
class="math inline">\(d_model\)</span>
维的输入，经过两个线性变换和一个激活函数，最终输出 <span
class="math inline">\(d_model\)</span> 维的向量。</p>
<h3 id="skip-connection图中的虚线输入">Skip
Connection——图中的虚线输入</h3>
<p>skip
connection最早是在计算机视觉的ResNet里面提到的，是微软亚洲研究院的何凯明做的，主要是想解决当网络很深时，误差向后传递会越来越弱，训练就很困难，那如果产生跳跃连接，如果有误差，可以从不同路径传到早期的网络层，这样的话误差就会比较明确地传回来。这就是跳跃层的来历。</p>
<p>跳跃层不是必须的，但在Transformer中，作者建议这样做，在Self
Attention的前后和每一个Feed
Forwar前后都用了跳跃层，如下图中的虚线所示。</p>
<img
src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKszwql2x03nnGJe%2Fskip-connection-and-layer-normalization.jpg?generation=1583677013154775&amp;alt=media" />
<center>
<code>图30: 图中的虚线代表Skip Connection</code>
</center>
<h2 id="decoder">Decoder</h2>
<img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(3).png" width = "50%" />
<center>
<code>图31: Transformer模型的编码器部分</code>
</center>
<p>论文中Decoder也是N=6层堆叠的结构。被分为3个sub-layer，Encoder与Decoder有<strong>三大主要的不同</strong>：</p>
<ol type="1">
<li><p>Decoder sub-layer-1使用的是“<strong>Masked</strong>” Multi-Headed
Attention机制，<strong>防止为了模型看到要预测的数据，防止泄露</strong>。</p></li>
<li><p>sub-layer-2是一个Encoder-Decoder Multi-head Attention。</p></li>
<li><p>LinearLayer和SoftmaxLayer作用于sub-layer-3的输出后面，来预测对应的word的概率。</p></li>
</ol>
<p>如果你弄懂了Encoder部分，Decoder部分也就没有那么可怕了：</p>
<ul>
<li>输入都是 embedding + positional Encoding。</li>
<li>Decoder也是N=6层堆叠的结构。被分为3个sub-layer，具体细节方面：
<ol type="1">
<li>masked multi-head
attention：由于在机器翻译中，Decode的过程是一个顺序的过程，也就是当解码第k个位置时，我们只能看到第k
- 1
及其之前的解码结果，因此<strong>加了mask，防止模型看到要预测的数据。这点和Encoder不同</strong></li>
<li>Encoder-Decoder Multi-Head
Attention：和Encoder的类似，每一层Decoder都会接受Encoder最后一层输出作为key和value，而当前解码器输出作为query。然后计算输入序列和目标序列中每个位置之间的相似度，最后将所有头的结果拼接在一起得到最终的输出。</li>
<li>FeedForward：和Encoder一样</li>
</ol></li>
<li>最后都连接了LinearLayer和SoftmaxLayer</li>
</ul>
<img
src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKtDCvJ7TGjfZPuP%2Fencoder-decoder-2.jpg?generation=1583677008527428&amp;alt=media" />
<center>
<code>图33: Transformer每一层Decoder都会分别接受Encoder最后一层输出</code>
</center>
<p><strong>关于Decoder的定义，源码如下：</strong> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure></p>
<p>这段代码实现了一个transformer编码器中的解码器Decoder。在初始化时，通过传入layer和N参数来初始化解码器。其中layer参数表示解码器中的单个层，N参数表示解码器中层的数量。</p>
<p>在前向传播过程中，通过使用for循环对解码器中的每一层进行处理。在每一层中，传入x,
memory, src_mask
和tgt_mask进行处理。循环结束后将最后的结果经过norm层进行归一化处理，最后返回归一化后的结果。</p>
<p>具体来说，使用for循环遍历self.layers，在每一次循环中使用layer(x,
memory, src_mask, tgt_mask)对x进行处理，最后使用self.norm(x)
将最终的结果进行归一化处理，最后返回归一化后的结果。</p>
<p>由此可见，<strong>只有masked multi-head
attention需要详细讲解，其余的在encoder处都已经掌握了。</strong></p>
<h3 id="masked-multi-head-attention">Masked Multi-Head-Attention</h3>
<p>Masked
Multi-Head-Attention则是在传统的多头注意力层的基础上，在计算过程中添加了一个遮挡（mask）机制。这个遮挡机制可以避免解码器在生成目标序列时看到未来的信息。</p>
<p>具体来说，在计算解码器在当前位置的输出值时，如果该位置对应的输入位置在目标序列中出现的位置在当前位置之后，那么这个输入位置就会被遮挡，不会被用来计算输出值。这样做能够避免解码器在生成目标序列时看到未来的信息，提高模型的效果。</p>
<p><strong>问题一</strong></p>
<blockquote>
<p>什么是mask</p>
</blockquote>
<p>mask表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer模型里面涉及两种mask，分别是
padding mask和sequence mask。 其中，padding mask在所有的scaled
dot-product attention 里面都需要用到，而sequence
mask只有在Decoder的Self-Attention里面用到。</p>
<p><strong>问题二</strong></p>
<blockquote>
<p>什么是padding mask？</p>
</blockquote>
<p>因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的Attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。</p>
<p>具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过softmax，这些位置的概率就会接近0！
而我们的padding mask
实际上是一个张量，每个值都是一个Boolean，值为false的地方就是我们要进行处理的地方。</p>
<p><strong>问题三</strong></p>
<blockquote>
<p>什么是sequence mask 文章前面也提到，sequence
mask是为了使得Decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。
那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p>
</blockquote>
<p>sequence mask的目的是防止Decoder “seeing the
future”，就像防止考生偷看考试答案一样。这里mask是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0。下面是个10维度的下三角矩阵：
$ [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],</p>
<p>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0],</p>
<p>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0],</p>
<p>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],</p>
<p>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],</p>
<p>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],</p>
<p>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],</p>
<p>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],</p>
<p>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],</p>
<p>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]] $</p>
<p>对于Decoder的Self-Attention，里面使用到的scaled dot-product
attention，同时需要padding mask和sequence
mask作为attn_mask，具体实现就是两个mask相加作为attn_mask。</p>
<p>其他情况，attn_mask一律等于padding mask。</p>
<p>举个例子：</p>
<p>假设最大允许的序列长度为10，先令padding mask为</p>
<p>[0 0 0 0 0 0 0 0 0 0]</p>
<p>然后假设当前句子一共有5个单词（加一个起始标识），在输入第三个单词的时候，前面有一个开始标识和两个单词，则此刻的sequence
mask为</p>
<p>[1 1 1 0 0 0]</p>
<p>然后padding mask和sequence mask相加，得</p>
<p>[1 1 1 0 0 0 0 0 0 0]</p>
<p><strong>问题四</strong></p>
<blockquote>
<p>为什么在模型训练阶段，Decoder的初始输入需要整体右移（Shifted
Right）一位？</p>
</blockquote>
<p>因为<span class="math inline">\(T-1\)</span>时刻需要预测<span
class="math inline">\(T\)</span>时刻的输出，所以Decoder的输入需要整体后移一位</p>
<p>举例说明：<code>汤姆追逐杰瑞</code> →
<code>Tom chase Jerry</code></p>
<p>位置关系： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0-“Tom”</span><br><span class="line">1-“chase”</span><br><span class="line">2-“Jerry”</span><br></pre></td></tr></table></figure> 操作：整体右移一位（Shifted Right）
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0-&lt;/s&gt;【起始符】目的是为了预测下一个Token</span><br><span class="line">1-“Tom”</span><br><span class="line">2-“chase”</span><br><span class="line">3-“Jerry”</span><br></pre></td></tr></table></figure></p>
<img
src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKtPv2OdAZhNaaw3%2Ftransformer-process-2.gif?generation=1583677021471399&amp;alt=media" />
<center>
<code>图31: 另一个例子说明Transformer模型的解码工作过程(省略了&lt;/s&gt;)</code>
</center>
<h2 id="output">Output</h2>
<img
src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(4).png" />
<center>
<code>图34: Decoder之后的输出部分</code>
</center>
<p>Decoder最终输出的结果是一个浮点型数据的向量，我们要如何把这个向量转为一个单词呢？这个就是Linear和softmax要做的事情了。</p>
<p>Linear层是一个全连接的神经网络，输出神经元个数一般等于我们的词汇表大小。Decoder输出的结果会输入到Linear层，然后再用softmax进行转换，得到的是词汇表大小的向量，向量的每个值对应的是当前Decoder是对应的这个词的概率，我们只要取概率最大的词，就是当前词语Decoder的结果了。</p>
<p>也就是说，Decoder的输出值首先经过一次线性变换，然后Softmax得到输出的概率分布，然后通过词典，输出概率最大的对应的单词作为我们的预测输出。</p>
<p><strong>关于Decoder的定义，源码如下：</strong> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br></pre></td></tr></table></figure>
在这段代码中，类Generator继承了nn.Module，定义了一个标准的线性+softmax生成步骤。类中定义了一个线性层self.proj,将输入的d_model维度转化为vocab维度,最后在forward函数中，将self.proj(x)通过F.log_softmax()函数计算出对数概率分布，在计算softmax的时候指定维度为-1(最后一维)，最终得到每个词的对数概率。这样计算出来的对数概率可以用来计算模型最终的损失函数。</p>
<p>总的来说，这段代码就是Transformer输出部分的代码，它负责将解码器的输出转化为最终的词的概率分布。</p>
<h1 id="transformer训练tricks">Transformer训练Tricks</h1>
<p>这里有两个训练小技巧，第一个是label平滑，第二个就是学习率要有个worm
up过程，然后再下降。</p>
<p>1、Label Smoothing（regularization）</p>
<p>由传统的 <span class="math display">\[
\begin{equation}
P_i=
\begin{cases}
1&amp; \text{ $ i = y $ } \\
0&amp; \text{ $ i \neq y $ }
\end{cases}
\end{equation}
\]</span></p>
<p>变为</p>
<p><span class="math display">\[
\begin{equation}
P_i=
\begin{cases}
1−ϵ&amp; \text{ $ i = y $ } \\
\frac{ϵ}{K−1}&amp; \text{ $ i \neq y $ }
\end{cases}
\end{equation}
\]</span></p>
<p>注：<span class="math inline">\(K\)</span>表示多分类的类别总数，<span
class="math inline">\(\epsilon\)</span>是一个较小的超参数。</p>
<p>2、<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.10464.pdf">[论文6] Mikel
Artetxe &amp; Holger Schwenk. 2018. Massively Multilingual Sentence
Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</a></p>
<p>Noam learning rate
schedule是一种在训练深度学习模型时调整学习率的方法。这种方法是由Google
AI团队的Noam
Shazeer在2018年提出的。它的基本思想是，随着模型的训练进程，学习率应该逐渐降低。具体来说，学习率是根据训练步数的对数来调整的。这个方法可以帮助模型在训练初期快速收敛，并在训练后期更稳定地优化。</p>
<p>学习率不按照Noam Learning Rate
Schedule，可能就得不到一个好的Transformer。</p>
<p><span
class="math display">\[lr=d_{model}^{−0.5}⋅min(step_{num}^{−0.5}, step_{num}\cdot
warmup\_steps^{−1.5})\]</span></p>
<p>公式表示学习率随着训练步数的增加而逐渐降低，在训练的前<span
class="math inline">\(warmup_steps\)</span>步中学习率是线性增长的,
之后学习率是指数下降的。如图所示：</p>
<img
src="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKtJ7Yme6Ll5rgo9%2Flr-worm-up.jpg?generation=1583677009478369&amp;alt=media" />
<center>
<code>图32: Noam learning rate schedule学习率随着训练步数的增加先上升后下降</code>
</center>
<h1 id="transformer特点">Transformer特点</h1>
<p><strong>优点</strong></p>
<ul>
<li><p>每层计算<strong>复杂度比RNN要低</strong>。</p></li>
<li><p>可以进行<strong>并行计算</strong>。</p></li>
<li><p>从计算一个序列长度为n的信息要经过的路径长度来看,
CNN需要增加卷积层数来扩大视野，RNN需要从1到n逐个进行计算，而Self-attention只需要一步矩阵计算就可以。Self-Attention可以比RNN<strong>更好地解决长时依赖问题</strong>。当然如果计算量太大，比如序列长度N大于序列维度D这种情况，也可以用窗口限制Self-Attention的计算数量。</p></li>
<li><p>从作者在附录中给出的例子可以看出，Self-Attention<strong>模型更可解释，Attention结果的分布表明了该模型学习到了一些语法和语义信息</strong>。</p></li>
</ul>
<p><strong>缺点</strong></p>
<p>在原文中没有提到缺点，是后来在Universal
Transformers中指出的，主要是两点：</p>
<ul>
<li><p>有些RNN轻易可以解决的问题Transformer没做到，比如复制String，或者推理时碰到的sequence长度比训练时更长（因为碰到了没见过的position
embedding）</p></li>
<li><p>理论上：transformers不是computationally
universal(图灵完备)，而RNN图灵。完备，这种非RNN式的模型是非图灵完备的的，<strong>无法单独完成NLP中推理、决策等计算问题</strong>（包括使用transformer的bert模型等等）。</p></li>
</ul>
<h1 id="参考链接">参考链接</h1>
<h2 id="整体代码实现">整体代码实现</h2>
<ul>
<li><p>哈佛大学NLP组的notebook，很详细文字和代码描述，用pytorch实现</p>
<p><a
target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p></li>
<li><p>Google的TensorFlow官方的，用tf keas实现</p>
<p><a
target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/text/transformer">https://www.tensorflow.org/tutorials/text/transformer</a></p></li>
</ul>
<h2 id="参考网站">参考网站</h2>
<ol type="1">
<li><p>Self-Attention和Transformer</p>
<p><a
target="_blank" rel="noopener" href="https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#%E8%AF%8D%E5%90%91%E9%87%8FEmbedding%E8%BE%93%E5%85%A5">https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#%E8%AF%8D%E5%90%91%E9%87%8FEmbedding%E8%BE%93%E5%85%A5</a></p></li>
<li><p>史上最小白之Transformer详解:</p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/Tink1995/article/details/105080033">https://blog.csdn.net/Tink1995/article/details/105080033</a></p></li>
<li><p>史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！:</p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148656446">https://zhuanlan.zhihu.com/p/148656446</a></p></li>
<li><p>The Illustrated Transformer:</p>
<p><a
target="_blank" rel="noopener" href="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p></li>
<li><p>A Brief Overview of Recurrent Neural Networks (RNN):</p>
<p><a
target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/">https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/</a></p></li>
<li><p>Practical PyTorch: Translation with a Sequence to Sequence
Network and Attention:</p>
<p><a
target="_blank" rel="noopener" href="https://notebook.community/spro/practical-pytorch/seq2seq-translation/seq2seq-translation-batched">https://notebook.community/spro/practical-pytorch/seq2seq-translation/seq2seq-translation-batched</a></p></li>
<li><p>也来谈谈RNN的梯度消失/爆炸问题:</p>
<p><a
target="_blank" rel="noopener" href="https://kexue.fm/archives/7888">https://kexue.fm/archives/7888</a></p></li>
<li><p>Transformer 架构逐层功能介绍和详细解释:</p>
<p><a
target="_blank" rel="noopener" href="https://avoid.overfit.cn/post/a895d880dab245609c177db7598446d4">https://avoid.overfit.cn/post/a895d880dab245609c177db7598446d4</a></p></li>
<li><p>Pytorch中 nn.Transformer的使用详解与Transformer的黑盒讲解:</p>
<p><a
target="_blank" rel="noopener" href="https://blog.51cto.com/u_11466419/5530949">https://blog.51cto.com/u_11466419/5530949</a></p></li>
<li><p>【深度学习】Attention is All You Need : Transformer模型:</p>
<p><a
target="_blank" rel="noopener" href="https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/">https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/</a></p></li>
<li><p>Bert前篇：手把手带你详解Transformer原理:</p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/364659780">https://zhuanlan.zhihu.com/p/364659780</a></p></li>
<li><p>ChatGPT3：</p>
<p><a
target="_blank" rel="noopener" href="https://chat.openai.com/chat">https://chat.openai.com/chat</a></p></li>
<li><p>一文搞懂one-hot和embedding：</p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/Alex_81D/article/details/114287498">https://blog.csdn.net/Alex_81D/article/details/114287498</a></p></li>
<li><p>残差网络(Residual Network)：</p>
<p><a
target="_blank" rel="noopener" href="https://www.cnblogs.com/gczr/p/10127723.html">https://www.cnblogs.com/gczr/p/10127723.html</a></p></li>
<li><p>【经典精读】万字长文解读Transformer模型和Attention机制</p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/104393915?utm_source=ZHShareTargetIDMore">https://zhuanlan.zhihu.com/p/104393915?utm_source=ZHShareTargetIDMore</a></p></li>
</ol>
<h2 id="参考文献">参考文献</h2>
<ul>
<li><p><a
target="_blank" rel="noopener" href="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">[论文1]Hochreiter
&amp; Schmidhuber. 1997. Long Short-Term Memory</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">[论文2]Dzmitry
Bahdanau. 2014. Neural Machine Translation by Jointly Learning to Align
and Translate</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">[论文3]Ashish
Vaswani., 2017 . Attention Is All You Need</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">[论文4]Kaiming He.
2015. Deep Residual Learning for Image Recognition</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">[论文5]Jimmy Lei Ba,.
2016. Layer Normalization</a></p></li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1812.10464.pdf">[论文6] Mikel
Artetxe &amp; Holger Schwenk. 2018. Massively Multilingual Sentence
Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</a></p></li>
</ul>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94/">科研</a><a class="post-meta__tags" href="/tags/%E9%A1%B9%E7%9B%AE/">项目</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/12/31/FEDformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"><img class="next-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">FEDformer论文阅读</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/12/09/%E6%88%91%E7%9A%84%E7%A7%91%E7%A0%94%E7%BB%8F%E5%8E%86/" title="我的项目经历"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Experience/uav3(1).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-09</div><div class="title">我的项目经历</div></div></a></div><div><a href="/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8A%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （上）"><img class="cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-14</div><div class="title">Transformer解析，适合没有NLP基础的小白入门 （上）</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">龚泽颖</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/serika-onoe" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:zgong313@connect.hkust-gz.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">雪融化了，是春天！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer%E4%B9%8B%E5%89%8D%E7%9A%84%E9%80%9A%E7%94%A8%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.</span> <span class="toc-text">Transformer之前的通用模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%9F%E6%9C%89%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BC%BA%E9%99%B7"><span class="toc-number">2.1.</span> <span class="toc-text">原有模型的缺陷</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8"><span class="toc-number">2.1.1.</span> <span class="toc-text">什么是梯度消失和梯度爆炸</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rnn%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E7%88%86%E7%82%B8%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-number">2.1.2.</span> <span class="toc-text">RNN梯度消失和爆炸的原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%92%88%E5%AF%B9%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%88%86%E7%82%B8%E7%9A%84%E6%94%B9%E8%BF%9Blstm"><span class="toc-number">2.1.3.</span> <span class="toc-text">针对梯度消失&#x2F;爆炸的改进：LSTM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%92%88%E5%AF%B9%E4%B8%8A%E4%B8%8B%E6%96%87%E5%90%91%E9%87%8F%E9%95%BF%E5%BA%A6%E5%9B%BA%E5%AE%9A%E7%9A%84%E6%94%B9%E8%BF%9Battention"><span class="toc-number">2.1.4.</span> <span class="toc-text">针对上下文向量长度固定的改进：Attention</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer%E7%AE%80%E4%BB%8B"><span class="toc-number">3.</span> <span class="toc-text">Transformer简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer%E5%AF%B9%E6%AF%94rnn"><span class="toc-number">4.</span> <span class="toc-text">Transformer对比RNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer%E5%8E%9F%E7%90%86"><span class="toc-number">5.</span> <span class="toc-text">Transformer原理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#input"><span class="toc-number">5.1.</span> <span class="toc-text">Input</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E8%AF%8Dtokenization"><span class="toc-number">5.1.1.</span> <span class="toc-text">分词(Tokenization)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B5%8C%E5%85%A5embedding"><span class="toc-number">5.1.2.</span> <span class="toc-text">嵌入(Embedding)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81positional-encoding"><span class="toc-number">5.1.3.</span> <span class="toc-text">位置编码(Positional Encoding)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#encoder"><span class="toc-number">5.2.</span> <span class="toc-text">Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#multi-head-attention"><span class="toc-number">5.2.1.</span> <span class="toc-text">Multi-Head Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%8Eself-attention%E8%AE%B2%E8%B5%B7"><span class="toc-number">5.2.1.1.</span> <span class="toc-text">从Self Attention讲起</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#multi-head-attention%E5%8E%9F%E7%90%86"><span class="toc-number">5.2.1.2.</span> <span class="toc-text">Multi-head Attention原理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#add-normalize"><span class="toc-number">5.2.2.</span> <span class="toc-text">Add &amp; Normalize</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#feed-forward"><span class="toc-number">5.2.3.</span> <span class="toc-text">Feed Forward</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#skip-connection%E5%9B%BE%E4%B8%AD%E7%9A%84%E8%99%9A%E7%BA%BF%E8%BE%93%E5%85%A5"><span class="toc-number">5.2.4.</span> <span class="toc-text">Skip
Connection——图中的虚线输入</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#decoder"><span class="toc-number">5.3.</span> <span class="toc-text">Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#masked-multi-head-attention"><span class="toc-number">5.3.1.</span> <span class="toc-text">Masked Multi-Head-Attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#output"><span class="toc-number">5.4.</span> <span class="toc-text">Output</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer%E8%AE%AD%E7%BB%83tricks"><span class="toc-number">6.</span> <span class="toc-text">Transformer训练Tricks</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#transformer%E7%89%B9%E7%82%B9"><span class="toc-number">7.</span> <span class="toc-text">Transformer特点</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">8.</span> <span class="toc-text">参考链接</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">8.1.</span> <span class="toc-text">整体代码实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E7%BD%91%E7%AB%99"><span class="toc-number">8.2.</span> <span class="toc-text">参考网站</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">8.3.</span> <span class="toc-text">参考文献</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （下）"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer解析，适合没有NLP基础的小白入门 （下）"/></a><div class="content"><a class="title" href="/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （下）">Transformer解析，适合没有NLP基础的小白入门 （下）</a><time datetime="2023-01-11T09:00:10.000Z" title="发表于 2023-01-11 17:00:10">2023-01-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/31/FEDformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="FEDformer论文阅读"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="FEDformer论文阅读"/></a><div class="content"><a class="title" href="/2022/12/31/FEDformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" title="FEDformer论文阅读">FEDformer论文阅读</a><time datetime="2022-12-31T03:47:23.000Z" title="发表于 2022-12-31 11:47:23">2022-12-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/20/CS231n-Assignment-1/" title="CS231n Assignment 1 (Updating)"><img src="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/cs231n_assignment1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CS231n Assignment 1 (Updating)"/></a><div class="content"><a class="title" href="/2022/12/20/CS231n-Assignment-1/" title="CS231n Assignment 1 (Updating)">CS231n Assignment 1 (Updating)</a><time datetime="2022-12-20T14:49:13.000Z" title="发表于 2022-12-20 22:49:13">2022-12-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8A%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （上）"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer解析，适合没有NLP基础的小白入门 （上）"/></a><div class="content"><a class="title" href="/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8A%EF%BC%89/" title="Transformer解析，适合没有NLP基础的小白入门 （上）">Transformer解析，适合没有NLP基础的小白入门 （上）</a><time datetime="2022-12-14T01:42:13.000Z" title="发表于 2022-12-14 09:42:13">2022-12-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/" title="强化学习初探"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="强化学习初探"/></a><div class="content"><a class="title" href="/2022/12/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/" title="强化学习初探">强化学习初探</a><time datetime="2022-12-12T15:17:01.000Z" title="发表于 2022-12-12 23:17:01">2022-12-12</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 龚泽颖</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script defer src="https://cdn.jsdelivr.net/gh/CodeByZach/pace/pace.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>