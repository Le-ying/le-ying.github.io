<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4） | 空之影的技术博客</title><meta name="author" content="空之影"><meta name="copyright" content="空之影"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="导航 这篇文章是基于 Pytorch 的 Transformer 简易版复现教程。  本博客的 Transformer 系列文章共计四篇，导航如下：   Transformer 零基础解析教程，从Encoder-Decoder架构说起（1&#x2F;4） Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2&#x2F;4） Transformer 零基础解析教程，牛刀小试Pyt">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3&#x2F;4）">
<meta property="og:url" content="https://serika-onoe.github.io/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/index.html">
<meta property="og:site_name" content="空之影的技术博客">
<meta property="og:description" content="导航 这篇文章是基于 Pytorch 的 Transformer 简易版复现教程。  本博客的 Transformer 系列文章共计四篇，导航如下：   Transformer 零基础解析教程，从Encoder-Decoder架构说起（1&#x2F;4） Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2&#x2F;4） Transformer 零基础解析教程，牛刀小试Pyt">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/serika-onoe/web-img/raw/main/Transformer/simpletransformers-logo.png">
<meta property="article:published_time" content="2023-01-31T09:00:10.000Z">
<meta property="article:modified_time" content="2023-02-14T11:53:27.563Z">
<meta property="article:author" content="空之影">
<meta property="article:tag" content="科研">
<meta property="article:tag" content="transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/serika-onoe/web-img/raw/main/Transformer/simpletransformers-logo.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://serika-onoe.github.io/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"W5J7GO3HT8","apiKey":"53b7d5d068605c78d1a20d7f3671629a","indexName":"test_serika","hits":{"per_page":3},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-14 19:53:27'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/CodeByZach/pace/themes/green/pace-theme-flash.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="空之影的技术博客" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/coder.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://github.com/serika-onoe/web-img/raw/main/Transformer/simpletransformers-logo.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">空之影的技术博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友情链接</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-31T09:00:10.000Z" title="发表于 2023-01-31 17:00:10">2023-01-31</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-14T11:53:27.563Z" title="更新于 2023-02-14 19:53:27">2023-02-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0-%E6%95%99%E7%A8%8B/">笔记, 教程</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="导航">导航</h1>
<p><strong>这篇文章是基于 Pytorch 的 Transformer
简易版复现教程。</strong></p>
<blockquote>
<p>本博客的 Transformer 系列文章共计四篇，导航如下：</p>
</blockquote>
<ul>
<li><p><a
href="https://serika-onoe.github.io/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/">Transformer
零基础解析教程，从Encoder-Decoder架构说起（1/4）</a></p></li>
<li><p><a
href="https://serika-onoe.github.io/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/">Transformer
零基础解析教程，剥洋葱般层层剖析内在原理（2/4）</a></p></li>
<li><p><a
href="https://serika-onoe.github.io/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/">Transformer
零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）<strong>本篇</strong></a></p></li>
<li><p><a
href="https://serika-onoe.github.io/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/">Transformer
零基础解析教程，完整版代码最终挑战（4/4）</a></p></li>
</ul>
<h1 id="前言">前言</h1>
<p><strong>该教程代码初始来源于Jeff
Jung，我阅读了一些博客和视频后做了大量的注释和修改，更加方便阅读和复现。</strong></p>
<p>代码中为了加快可读性和运行速度，并没有用到大型的数据集，而是手动输入了两对中文→英语的句子，还有每个字的索引也是手动硬编码上去的，主要是为了降低代码执行速度和阅读难度，哪怕用普通的笔记本CPU也能在1分钟以内完成，从而方便读者<strong>把重点放到模型实现的部分！</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ======================================</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612, modify by shwei</span></span><br><span class="line"><span class="string">Reference: https://github.com/jadore801120/attention-is-all-you-need-pytorch</span></span><br><span class="line"><span class="string">           https://github.com/JayParks/transformer</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ====================================================================================================</span></span><br></pre></td></tr></table></figure>
<h1 id="数据预处理">数据预处理</h1>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">device = <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="comment"># device = &#x27;cuda&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># transformer epochs</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"><span class="comment"># epochs = 1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里我没有用什么大型的数据集，而是手动输入了两对中文→英语的句子</span></span><br><span class="line"><span class="comment"># 还有每个字的索引也是我手动硬编码上去的，主要是为了降低代码阅读难度</span></span><br><span class="line"><span class="comment"># S: Symbol that shows starting of decoding input</span></span><br><span class="line"><span class="comment"># E: Symbol that shows starting of decoding output</span></span><br><span class="line"><span class="comment"># P: Symbol that will fill in blank sequence if current batch data size is shorter than time steps</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="comment"># 中文和英语的单词个数不要求相同</span></span><br><span class="line">    <span class="comment"># enc_input                dec_input           dec_output</span></span><br><span class="line">    [<span class="string">&#x27;我 有 一 个 女 朋 友&#x27;</span>, <span class="string">&#x27;S i have a girl friend . &#x27;</span>, <span class="string">&#x27;i have a girl friend . E&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;我 有 零 个 好 朋 友&#x27;</span>, <span class="string">&#x27;S i have zero good friend .&#x27;</span>, <span class="string">&#x27;i have zero good friend . E&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中文和英语的单词要分开建立词库</span></span><br><span class="line"><span class="comment"># Padding Should be Zero</span></span><br><span class="line">src_vocab = &#123;</span><br><span class="line">            <span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;我&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;有&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;一&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;个&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;好&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;朋&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;友&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;零&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;女&#x27;</span>: <span class="number">9</span>&#125;</span><br><span class="line">src_idx2word = &#123;</span><br><span class="line">            i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(src_vocab)&#125;</span><br><span class="line">src_vocab_size = <span class="built_in">len</span>(src_vocab)</span><br><span class="line"></span><br><span class="line">tgt_vocab = &#123;</span><br><span class="line">            <span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;have&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;good&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;friend&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;zero&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;girl&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;S&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">9</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line">idx2word = &#123;</span><br><span class="line">            i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(tgt_vocab)&#125;</span><br><span class="line">tgt_vocab_size = <span class="built_in">len</span>(tgt_vocab)</span><br><span class="line"></span><br><span class="line">src_len = <span class="number">8</span>  <span class="comment"># （源句子的长度）enc_input max sequence length</span></span><br><span class="line">tgt_len = <span class="number">7</span>  <span class="comment"># dec_input(=dec_output) max sequence length</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Transformer Parameters</span></span><br><span class="line">d_model = <span class="number">512</span>  <span class="comment"># Embedding Size（token embedding和position编码的维度）</span></span><br><span class="line">d_ff = <span class="number">2048</span>  <span class="comment"># FeedForward dimension (两次线性层中的隐藏层 512-&gt;2048-&gt;512，线性层是用来做特征提取的），当然最后会再接一个projection层</span></span><br><span class="line">d_k = d_v = <span class="number">64</span>  <span class="comment"># dimension of K(=Q), V（Q和K的维度需要相同，这里为了方便让K=V）</span></span><br><span class="line">n_layers = <span class="number">6</span>  <span class="comment"># number of Encoder of Decoder Layer（Block的个数）</span></span><br><span class="line">n_heads = <span class="number">8</span>  <span class="comment"># number of heads in Multi-Head Attention（有几套头）</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==============================================================================================</span></span><br><span class="line"><span class="comment"># 数据构建</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentences</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;把单词序列转换为数字序列&quot;&quot;&quot;</span></span><br><span class="line">    enc_inputs, dec_inputs, dec_outputs = [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences)):</span><br><span class="line">        enc_input = [[src_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">0</span>].split()]]  <span class="comment"># [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]</span></span><br><span class="line">        dec_input = [[tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">1</span>].split()]]  <span class="comment"># [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]</span></span><br><span class="line">        dec_output = [[tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">2</span>].split()]]  <span class="comment"># [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]</span></span><br><span class="line"></span><br><span class="line">        enc_inputs.extend(enc_input)</span><br><span class="line">        dec_inputs.extend(dec_input)</span><br><span class="line">        dec_outputs.extend(dec_output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataSet</span>(Data.Dataset):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义DataLoader&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_inputs, dec_inputs, dec_outputs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDataSet, self).__init__()</span><br><span class="line">        self.enc_inputs = enc_inputs</span><br><span class="line">        self.dec_inputs = dec_inputs</span><br><span class="line">        self.dec_outputs = dec_outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.enc_inputs.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 这行代码创建了一个PyTorch数据加载器，用于在训练机器学习模型时加载数据。DataLoader是PyTorch的核心库torch.utils.data中的函数。它的作用是将</span></span><br><span class="line"><span class="comment"># 数据集（此处为MyDataSet类的实例，传入的enc_inputs、dec_inputs和dec_outputs作为数据）拆分成小批次以提高加载效率。</span></span><br><span class="line"><span class="comment"># 具体参数说明：</span></span><br><span class="line"><span class="comment"># batch_size: 2，每批加载的样本数。</span></span><br><span class="line"><span class="comment"># shuffle: True，是否打乱数据顺序。</span></span><br></pre></td></tr></table></figure>
<p>上面都比较简单，下面开始涉及到模型就比较复杂了，因此我会将模型拆分成以下几个部分进行讲解</p>
<ul>
<li>Positional Encoding</li>
<li>Pad
Mask（序列本身固定长度，不够长的序列需要填充（pad），也就是'P'）</li>
<li>Subsequence Mask（Decoder input 不能看到未来时刻单词信息，因此需要
mask）</li>
<li>ScaledDotProductAttention</li>
<li>Multi-Head Attention</li>
<li>FeedForward Layer</li>
<li>Encoder Layer</li>
<li>Encoder</li>
<li>Decoder Layer</li>
<li>Decoder</li>
<li>Transformer</li>
</ul>
<p>关于代码中的注释，如果值为 <code>src_len</code> 或者
<code>tgt_len</code> 的，我一定会写清楚，但是有些函数或者类，Encoder 和
Decoder 都有可能调用，因此就不能确定究竟是 <code>src_len</code> 还是
<code>tgt_len</code>，对于不确定的，我会记作 <code>seq_len</code></p>
<h1 id="模型构建">模型构建</h1>
<h2 id="positional-encoding">Positional Encoding</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在dropout函数中，参数p是指丢弃元素的概率。它决定了输入张量的元素在丢弃操作中被设置为零的比率。</span></span><br><span class="line"><span class="string">        例如，如果p=0.1，那么10%的元素将在丢弃操作中被设置为零。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        通过调用self.register_buffer(&#x27;pe&#x27;, pe)，我们将这个张量注册为模型的一个可学习参数，</span></span><br><span class="line"><span class="string">        这意味着这个张量在模型训练过程中不需要更新其梯度，即不需要在损失函数的计算中考虑这个张量的梯度。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: [seq_len, batch_size, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure>
<h2 id="pad-mask">Pad Mask</h2>
<p>由于在 Encoder 和 Decoder 中都需要进行 mask
操作，因此就无法确定这个函数的参数中 <code>seq_len</code> 的值，如果是在
Encoder 中调用的，<code>seq_len</code> 就等于
<code>src_len</code>；如果是在 Decoder 中调用的，<code>seq_len</code>
就有可能等于 <code>src_len</code>，也有可能等于
<code>tgt_len</code>（因为 Decoder 有两次 mask）</p>
<p><strong>pad
mask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量。</strong></p>
<p>这个函数最核心的一句代码是
<code>seq_k.data.eq(0)</code>，这句的作用是返回一个大小和
<code>seq_k</code> 一样的 tensor，只不过里面的值只有 True 和 False。如果
<code>seq_k</code> 某个位置的值等于 0，那么对应位置就是 True，否则即为
False。举个例子，输入为
<code>seq_data = [1, 2, 3, 4, 0]</code>，<code>seq_data.data.eq(0)</code>
就会返回 <code>[False, False, False, False, True]</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span><br><span class="line">    <span class="comment"># pad mask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;这里的q,k表示的是两个序列（跟注意力机制的q,k没有关系），例如encoder_inputs (x1,x2,..xm)和encoder_inputs (x1,x2..xm)</span></span><br><span class="line"><span class="string">    encoder和decoder都可能调用这个函数，所以seq_len视情况而定</span></span><br><span class="line"><span class="string">    seq_q: [batch_size, seq_len]</span></span><br><span class="line"><span class="string">    seq_k: [batch_size, seq_len]</span></span><br><span class="line"><span class="string">    seq_len could be src_len or it could be tgt_len</span></span><br><span class="line"><span class="string">    seq_len in seq_q and seq_len in seq_k maybe not equal</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    batch_size, len_q = seq_q.size()  <span class="comment"># 这个seq_q只是用来expand维度的</span></span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    <span class="comment"># eq(zero) is PAD token</span></span><br><span class="line">    <span class="comment"># 例如:seq_k = [[1,2,3,4,0], [1,2,3,5,0]]</span></span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># [batch_size, 1, len_k], True is masked</span></span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k)  <span class="comment"># [batch_size, len_q, len_k] 构成一个立方体(batch_size个这样的矩阵)</span></span><br></pre></td></tr></table></figure>
<h2 id="subsequence-mask">Subsequence Mask</h2>
<p><strong>Subsequence Mask 只有 Decoder
会用到，主要作用是屏蔽未来时刻单词的信息。</strong></p>
<p>这段代码实现了获得一个注意力子序列掩码。它使用Numpy函数np.triu生成一个上三角矩阵，并用np.ones初始化这个矩阵。attn_shape变量储存了这个矩阵的形状，它是一个三维数组，分别是batch_size、tgt_len、tgt_len。然后，np.triu将这个矩阵初始化为上三角形，并通过参数k=1使对角线上的元素为0。最后，将这个矩阵转换为PyTorch
tensor，并返回该张量。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequence_mask</span>(<span class="params">seq</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;建议打印出来看看是什么样的输出（一目了然）</span></span><br><span class="line"><span class="string">    seq: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]</span><br><span class="line">    <span class="comment"># attn_shape: [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">    subsequence_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>)  <span class="comment"># 生成一个上三角矩阵</span></span><br><span class="line">    subsequence_mask = torch.from_numpy(subsequence_mask).byte()</span><br><span class="line">    <span class="keyword">return</span> subsequence_mask  <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br></pre></td></tr></table></figure>
<h2 id="scaleddotproductattention">ScaledDotProductAttention</h2>
<p><strong>"Scaled Dot-Product Attention"
是一种用于自注意力机制的注意力机制方法，通过将输入矩阵Q、K和V与自身转置进行点积运算，得到关于每一个词的注意力分数。</strong></p>
<p>使用Q、K、V三个变量作为输入，经过一系列矩阵运算后得到context和attn，其中context是计算出的注意力张量，attn是对应的注意力稀疏矩阵，并返回这两个结果。</p>
<p>具体地，在forward函数中，使用Q、K做矩阵乘法得到scores矩阵，scores中的每一个元素都是对应Q中词与K中词的相似程度。</p>
<p>下一步，通过使用mask矩阵对scores中的元素进行赋值，将与mask矩阵中值为1的元素相对应的scores元素赋值为-1e9，使其不被softmax计算。</p>
<p>最后，使用softmax对scores最后一维（也就是v）做软归一化，得到注意力稀疏矩阵attn。最后，使用attn矩阵对V做矩阵乘法，得到context矩阵，其中每一行对应一个词的向量表示。</p>
<p>（matmul函数是矩阵乘法，它返回两个矩阵的点积，即将两个矩阵对应元素相乘并相加。它对应的矩阵乘法操作是：C
= A * B，其中C是乘积矩阵，A是左矩阵，B是右矩阵。）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line"><span class="string">        K: [batch_size, n_heads, len_k, d_k]</span></span><br><span class="line"><span class="string">        V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line"><span class="string">        attn_mask: [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line"><span class="string">        说明：在encoder-decoder的Attention层中len_q(q1,..qt)和len_k(k1,...km)可能不同</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(d_k)  <span class="comment"># scores : [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        <span class="comment"># mask矩阵填充scores（用-1e9填充scores中与attn_mask中值为1位置相对应的元素）</span></span><br><span class="line">        scores.masked_fill_(attn_mask, -<span class="number">1e9</span>)  <span class="comment"># Fills elements of self tensor with value where mask is True.</span></span><br><span class="line"></span><br><span class="line">        attn = nn.Softmax(dim=-<span class="number">1</span>)(scores)  <span class="comment"># 对最后一个维度(v)做softmax</span></span><br><span class="line">        <span class="comment"># scores : [batch_size, n_heads, len_q, len_k] * V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line">        context = torch.matmul(attn, V)  <span class="comment"># context: [batch_size, n_heads, len_q, d_v]</span></span><br><span class="line">        <span class="comment"># context：[[z1,z2,...],[...]]向量, attn注意力稀疏矩阵（用于可视化的）</span></span><br><span class="line">        <span class="keyword">return</span> context, attn</span><br></pre></td></tr></table></figure>
<h2 id="multiheadattention">MultiHeadAttention</h2>
<p>完整代码中一定会有三处地方调用
<code>MultiHeadAttention()</code>，Encoder Layer 调用一次，传入的
<code>input_Q</code>、<code>input_K</code>、<code>input_V</code>
全部都是 <code>enc_inputs</code>；Decoder Layer
中两次调用，第一次传入的全是 <code>dec_inputs</code>，第二次传入的分别是
<code>dec_outputs</code>，<code>enc_outputs</code>，<code>enc_outputs</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;这个Attention类可以实现:</span></span><br><span class="line"><span class="string">    Encoder的Self-Attention</span></span><br><span class="line"><span class="string">    Decoder的Masked Self-Attention</span></span><br><span class="line"><span class="string">    Encoder-Decoder的Attention</span></span><br><span class="line"><span class="string">    输入：seq_len x d_model</span></span><br><span class="line"><span class="string">    输出：seq_len x d_model</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)  <span class="comment"># q,k必须维度相同，不然无法做点积</span></span><br><span class="line">        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 这个全连接层可以保证多头attention的输出仍然是seq_len x d_model</span></span><br><span class="line">        self.fc = nn.Linear(n_heads * d_v, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        nn.Linear 函数是 PyTorch 模型中的一种全连接层 (fully connected layer) 的实现。</span></span><br><span class="line"><span class="string">        它的作用是对输入数据进行线性变换，即 y = Wx + b，其中 W 是线性变换的系数矩阵，b 是偏移量，x 是输入数据。</span></span><br><span class="line"><span class="string">        torch.nn.Linear(in_features, # 输入的神经元个数</span></span><br><span class="line"><span class="string">           out_features, # 输出神经元个数</span></span><br><span class="line"><span class="string">           bias=True # 是否包含偏置</span></span><br><span class="line"><span class="string">           )</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_Q, input_K, input_V, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        input_Q: [batch_size, len_q, d_model]</span></span><br><span class="line"><span class="string">        input_K: [batch_size, len_k, d_model]</span></span><br><span class="line"><span class="string">        input_V: [batch_size, len_v(=len_k), d_model]</span></span><br><span class="line"><span class="string">        attn_mask: [batch_size, seq_len, seq_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        residual, batch_size = input_Q, input_Q.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 下面的多头的参数矩阵是放在一起做线性变换的，然后再拆成多个头，这是工程实现的技巧</span></span><br><span class="line">        <span class="comment"># B: batch_size, S:seq_len, D: dim</span></span><br><span class="line">        <span class="comment"># (B, S, D) -proj-&gt; (B, S, D_new) -split-&gt; (B, S, Head, W) -trans-&gt; (B, Head, S, W)</span></span><br><span class="line">        <span class="comment">#           线性变换               拆成多头</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line">        Q = self.W_Q(input_Q).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># K: [batch_size, n_heads, len_k, d_k] # K和V的长度一定相同，维度可以不同</span></span><br><span class="line">        K = self.W_K(input_K).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line">        V = self.W_V(input_V).view(batch_size, -<span class="number">1</span>, n_heads, d_v).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 因为是多头，所以mask矩阵要扩充成4维的</span></span><br><span class="line">        <span class="comment"># attn_mask: [batch_size, seq_len, seq_len] -&gt; [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, n_heads, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)</span><br><span class="line">        <span class="comment"># 下面将不同头的输出向量拼接在一起</span></span><br><span class="line">        <span class="comment"># context: [batch_size, n_heads, len_q, d_v] -&gt; [batch_size, len_q, n_heads * d_v]</span></span><br><span class="line">        context = context.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(batch_size, -<span class="number">1</span>, n_heads * d_v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这个全连接层可以保证多头attention的输出仍然是seq_len x d_model</span></span><br><span class="line">        output = self.fc(context)  <span class="comment"># [batch_size, len_q, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_model).to(device)(output + residual), attn</span><br></pre></td></tr></table></figure>
<h2 id="feedforward-layer">FeedForward Layer</h2>
<p><strong>这段代码非常简单，就是做两次线性变换，残差连接后再跟一个
Layer Norm。用于实现Transformer模型中的前馈网络。</strong></p>
<p>该网络由两个全连接层（nn.Linear）和一个 ReLU
激活函数（nn.ReLU）组成。第一个全连接层将输入从 d_model 维度转换到 d_ff
维度，第二个全连接层将输入从 d_ff 维度转换回 d_model 维度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PoswiseFeedForwardNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(PoswiseFeedForwardNet, self).__init__()</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(d_model, d_ff, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(d_ff, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        inputs: [batch_size, seq_len, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        residual = inputs</span><br><span class="line">        output = self.fc(inputs)</span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_model).to(device)(output + residual)  <span class="comment"># [batch_size, seq_len, d_model]</span></span><br></pre></td></tr></table></figure>
<h2 id="encoder-layer-encoder">Encoder Layer &amp; Encoder</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.enc_self_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, enc_self_attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len, d_model]</span></span><br><span class="line"><span class="string">        enc_self_attn_mask: [batch_size, src_len, src_len]  mask矩阵(pad mask or sequence mask)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">        <span class="comment"># 第一个enc_inputs * W_Q = Q</span></span><br><span class="line">        <span class="comment"># 第二个enc_inputs * W_K = K</span></span><br><span class="line">        <span class="comment"># 第三个enc_inputs * W_V = V</span></span><br><span class="line">        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs,</span><br><span class="line">                                               enc_self_attn_mask)  <span class="comment"># enc_inputs to same Q,K,V（未线性变换前）</span></span><br><span class="line">        enc_outputs = self.pos_ffn(enc_outputs)</span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs, attn</span><br></pre></td></tr></table></figure>
<p><code>nn.ModuleList()</code> 列表里面存了 <code>n_layers</code> 个
Encoder Layer。由于我们控制好了 Encoder Layer
的输入和输出维度相同，所以可以直接用个 for 循环以嵌套的方式，将上一次
Encoder Layer 的输出作为下一次 Encoder Layer 的输入。</p>
<p><strong>将<code>n_layers</code>个（本文为6个）EncoderLayer组件逐个拼起来，就是一个完整的Encoder。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.src_emb = nn.Embedding(src_vocab_size, d_model)  <span class="comment"># token Embedding</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Embedding解释</span></span><br><span class="line"><span class="string">        例如：如果你有一个词语表(vocabulary)，其中包含了3个词语：&quot;dog&quot;, &quot;cat&quot;, &quot;bird&quot;。并且你指定了src_vocab_size = 3</span></span><br><span class="line"><span class="string">        和d_model = 5，那么这个Embedding层就可以将每一个词语表示成一个5维的实数向量，比如：&quot;dog&quot;</span></span><br><span class="line"><span class="string">        可以表示为[0.1, 0.2, 0.3, 0.4, 0.5]。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)  <span class="comment"># Transformer中位置编码时固定的，不需要学习</span></span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs)  <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        enc_outputs = self.pos_emb(enc_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        <span class="comment"># Encoder输入序列的pad mask矩阵</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)  <span class="comment"># [batch_size, src_len, src_len]</span></span><br><span class="line">        enc_self_attns = []  <span class="comment"># 在计算中不需要用到，它主要用来保存你接下来返回的attention的值（这个主要是为了你画热力图等，用来看各个词之间的关系</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:  <span class="comment"># for循环访问nn.ModuleList对象</span></span><br><span class="line">            <span class="comment"># 上一个block的输出enc_outputs作为当前block的输入</span></span><br><span class="line">            <span class="comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">            enc_outputs, enc_self_attn = layer(enc_outputs,</span><br><span class="line">                                               enc_self_attn_mask)  <span class="comment"># 传入的enc_outputs其实是input，传入mask矩阵是因为你要做self attention</span></span><br><span class="line">            enc_self_attns.append(enc_self_attn)  <span class="comment"># 这个只是为了可视化</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs, enc_self_attns</span><br></pre></td></tr></table></figure>
<h1 id="decoder-layer-decoder">Decoder Layer &amp; Decoder</h1>
<p>在 Decoder Layer 中会调用两次
<code>MultiHeadAttention</code>，第一次是计算 Decoder Input 的
self-attention，得到输出 <code>dec_outputs</code>。然后将
<code>dec_outputs</code> 作为生成 Q 的元素，<code>enc_outputs</code>
作为生成 K 和 V 的元素，再调用一次
<code>MultiHeadAttention</code>，得到的是 Encoder 和 Decoder Layer
之间的 context vector。最后将 <code>dec_outptus</code>
做一次维度变换，然后返回。</p>
<p><strong>将<code>n_layers</code>个（本文为6个）DecoderLayer组件逐个拼起来，就是一个完整的Decoder。</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.dec_self_attn = MultiHeadAttention()</span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len, d_model]</span></span><br><span class="line"><span class="string">        enc_outputs: [batch_size, src_len, d_model]</span></span><br><span class="line"><span class="string">        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"><span class="string">        dec_enc_attn_mask: [batch_size, tgt_len, src_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]</span></span><br><span class="line">        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs,</span><br><span class="line">                                                        dec_self_attn_mask)  <span class="comment"># 这里的Q,K,V全是Decoder自己的输入</span></span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span></span><br><span class="line">        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs,</span><br><span class="line">                                                      dec_enc_attn_mask)  <span class="comment"># Attention层的Q(来自decoder) 和 K,V(来自encoder)</span></span><br><span class="line">        dec_outputs = self.pos_ffn(dec_outputs)  <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attn, dec_enc_attn  <span class="comment"># dec_self_attn, dec_enc_attn这两个是为了可视化的</span></span><br></pre></td></tr></table></figure>
<p>Decoder 中不仅要把 "pad"mask 掉，还要 mask
未来时刻的信息，因此就有了下面这三行代码，其中
<code>torch.gt(a, value)</code> 的意思是，将 a 中各个位置上的元素和
value 比较，若大于 value，则该位置取 1，否则取 0</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)  <span class="comment"># Decoder输入的embed词表</span></span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])  <span class="comment"># Decoder的blocks</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        enc_outputs: [batch_size, src_len, d_model]   # 用在Encoder-Decoder Attention层</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs)  <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_outputs = self.pos_emb(dec_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>).to(</span><br><span class="line">            device)  <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="comment"># Decoder输入序列的pad mask矩阵（这个例子中decoder是没有加pad的，实际应用中都是有pad填充的）</span></span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).to(device)  <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">        <span class="comment"># Masked Self_Attention：当前时刻是看不到未来的信息的</span></span><br><span class="line">        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).to(</span><br><span class="line">            device)  <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Decoder中把两种mask矩阵相加（既屏蔽了pad的信息，也屏蔽了未来时刻的信息）</span></span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask),</span><br><span class="line">                                      <span class="number">0</span>).to(device)  <span class="comment"># [batch_size, tgt_len, tgt_len]; torch.gt比较两个矩阵的元素，大于则返回1，否则返回0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这个mask主要用于encoder-decoder attention层</span></span><br><span class="line">        <span class="comment"># get_attn_pad_mask主要是enc_inputs的pad mask矩阵(因为enc是处理K,V的，求Attention时是用v1,v2,..vm去加权的，要把pad对应的v_i的相关系数设为0，这样注意力就不会关注pad向量)</span></span><br><span class="line">        <span class="comment">#                       dec_inputs只是提供expand的size的</span></span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)  <span class="comment"># [batc_size, tgt_len, src_len]</span></span><br><span class="line"></span><br><span class="line">        dec_self_attns, dec_enc_attns = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span></span><br><span class="line">            <span class="comment"># Decoder的Block是上一个Block的输出dec_outputs（变化）和Encoder网络的输出enc_outputs（固定）</span></span><br><span class="line">            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask,</span><br><span class="line">                                                             dec_enc_attn_mask)</span><br><span class="line">            dec_self_attns.append(dec_self_attn)</span><br><span class="line">            dec_enc_attns.append(dec_enc_attn)</span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure>
<h2 id="transformer">Transformer</h2>
<p><strong>这段代码实现了Transformer类，用到的三种架构--前面定义过的Encoder和Decoder，以及下面新定义的投影层Projection（projection实现的是decoder后面的linear，之所以没有实现softmax是因为后续的贪婪解码器替代了softmax层的工作，直接得到概率最大值的词表索引并输出）。</strong></p>
<p>在输入经过Encoder网络和Decoder网络处理后，得到的输出分别是enc_outputs和dec_outputs。最后再经过一个投影层，将dec_outputs映射成dec_logits，表示每个单词的词概率分布。返回
dec_logits, enc_self_attns, dec_self_attns, dec_enc_attns。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        <span class="comment"># Transformer类继承自PyTorch的nn.Module类。</span></span><br><span class="line">        <span class="comment"># 通过在Transformer的构造函数中调用super(Transformer, self).init()，</span></span><br><span class="line">        <span class="comment"># 可以调用nn.Module的构造函数，以便初始化nn.Module的一些内部状态，以及设置Transformer类对象的一些公共属性。</span></span><br><span class="line">        self.encoder = Encoder().to(device)</span><br><span class="line">        self.decoder = Decoder().to(device)</span><br><span class="line">        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=<span class="literal">False</span>).to(device)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, dec_inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Transformers的输入：两个序列</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># tensor to store decoder outputs</span></span><br><span class="line">        <span class="comment"># outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">        <span class="comment"># 经过Encoder网络后，得到的输出还是[batch_size, src_len, d_model]</span></span><br><span class="line">        enc_outputs, enc_self_attns = self.encoder(enc_inputs)</span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]</span></span><br><span class="line">        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)</span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model] -&gt; dec_logits: [batch_size, tgt_len, tgt_vocab_size]</span></span><br><span class="line">        dec_logits = self.projection(dec_outputs)</span><br><span class="line">        <span class="keyword">return</span> dec_logits.view(-<span class="number">1</span>, dec_logits.size(-<span class="number">1</span>)), enc_self_attns, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure>
<p><strong>view函数说明</strong></p>
<p>view 方法是 PyTorch 中 tensor 的一种 reshape 操作。它将一个 tensor 的
shape 变成给定的形状。</p>
<p>在这段代码中， dec_logits.view(-1, dec_logits.size(-1)) 表示将
dec_logits tensor 从原来的 shape 变成了一个新的 shape，其中第一维是
-1，这意味着该维的长度是自动计算的（其他维度的长度已经确定了），第二维是
dec_logits.size(-1)，这是一个数字，代表 dec_logits tensor
的最后一维的长度。</p>
<p><strong>forward函数说明：</strong></p>
<p>通过进行一次前向传播的操作（比如 output = model(input)）时，PyTorch
内部会对模型中每一个模块（包括 Encoder 和 EncoderLayer）中的 forward
函数进行调用，以计算输出结果。</p>
<p>如果不手动定义 forward 函数，那么模型将不会被调用。因此，forward
函数是必须被定义的，用于计算模型的前向传播过程。</p>
<h1 id="模型调用-损失函数-优化器">模型调用 &amp; 损失函数 &amp;
优化器</h1>
<p>这段代码调用了Transformer模型，并设置了交叉熵损失函数（将ignore_index参数设置为0），优化器使用随机梯度下降（SGD）算法。（优化器将使用model.parameters()作为参数进行优化，学习率为1e-3，动量为0.99）。</p>
<p>ignore_index参数被设置为0，这样损失计算将忽略任何索引为0的输入，这通常是为NLP模型中的padding
token保留的。</p>
<p>使用Adam算法对于较小的数据量效果很差</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Transformer().to(device)</span><br><span class="line"><span class="comment"># 这里的损失函数里面设置了一个参数 ignore_index=0，因为 &quot;pad&quot; 这个单词的索引为 0，这样设置以后，就不会计算 &quot;pad&quot; 的损失（因为本来 &quot;pad&quot; 也没有意义，不需要计算）</span></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-3</span>, momentum=<span class="number">0.99</span>)</span><br><span class="line"><span class="comment"># optimizer = optim.Adam(model.parameters(), lr=1e-9) # 用adam的话效果不好</span></span><br></pre></td></tr></table></figure>
<h1 id="训练">训练</h1>
<p><strong>最后三行代码是在进行一次反向传播迭代的操作。</strong></p>
<p>分三步执行：
optimizer.zero_grad()：对梯度进行初始化，因为pytorch的梯度是累加的，所以每次计算前需要把梯度归零。
loss.backward()：计算当前损失函数的梯度，并且完成反向传播。
optimizer.step()：执行优化器的更新操作，根据梯度对模型参数进行更新。</p>
<p>总的来说，这三步代码是完成一次机器学习模型的参数优化的核心过程。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> enc_inputs, dec_inputs, dec_outputs <span class="keyword">in</span> loader:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        dec_outputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_inputs, dec_inputs, dec_outputs = enc_inputs.to(device), dec_inputs.to(device), dec_outputs.to(device)</span><br><span class="line">        <span class="comment"># outputs: [batch_size * tgt_len, tgt_vocab_size]</span></span><br><span class="line">        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)</span><br><span class="line">        loss = criterion(outputs, dec_outputs.view(-<span class="number">1</span>))  <span class="comment"># dec_outputs.view(-1):[batch_size * tgt_len * tgt_vocab_size]</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (epoch + <span class="number">1</span>), <span class="string">&#x27;loss =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>
<p>训练输出如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># OUTPUT</span><br><span class="line">    Epoch: 0001 loss = 2.752963</span><br><span class="line">    Epoch: 0002 loss = 2.625792</span><br><span class="line">    Epoch: 0003 loss = 2.508290</span><br><span class="line">    Epoch: 0004 loss = 2.260287</span><br><span class="line">    Epoch: 0005 loss = 2.026300</span><br><span class="line">    Epoch: 0006 loss = 1.715278</span><br><span class="line">    Epoch: 0007 loss = 1.559423</span><br><span class="line">    Epoch: 0008 loss = 1.339594</span><br><span class="line">    Epoch: 0009 loss = 1.082600</span><br><span class="line">    Epoch: 0010 loss = 0.945365</span><br><span class="line">    Epoch: 0011 loss = 0.758650</span><br><span class="line">    Epoch: 0012 loss = 0.581669</span><br><span class="line">    Epoch: 0013 loss = 0.461126</span><br><span class="line">    Epoch: 0014 loss = 0.340352</span><br><span class="line">    Epoch: 0015 loss = 0.252243</span><br><span class="line">    Epoch: 0016 loss = 0.190971</span><br><span class="line">    Epoch: 0017 loss = 0.161172</span><br><span class="line">    Epoch: 0018 loss = 0.130554</span><br><span class="line">    Epoch: 0019 loss = 0.101027</span><br><span class="line">    Epoch: 0020 loss = 0.093038</span><br><span class="line">    Epoch: 0021 loss = 0.079294</span><br><span class="line">    Epoch: 0022 loss = 0.070799</span><br><span class="line">    Epoch: 0023 loss = 0.062795</span><br><span class="line">    Epoch: 0024 loss = 0.044259</span><br><span class="line">    Epoch: 0025 loss = 0.056274</span><br><span class="line">    Epoch: 0026 loss = 0.033928</span><br><span class="line">    Epoch: 0027 loss = 0.037328</span><br><span class="line">    Epoch: 0028 loss = 0.035663</span><br><span class="line">    Epoch: 0029 loss = 0.032550</span><br><span class="line">    Epoch: 0030 loss = 0.029425</span><br><span class="line">    Epoch: 0031 loss = 0.028057</span><br><span class="line">    Epoch: 0032 loss = 0.024588</span><br><span class="line">    Epoch: 0033 loss = 0.019545</span><br><span class="line">    Epoch: 0034 loss = 0.025953</span><br><span class="line">    Epoch: 0035 loss = 0.018335</span><br><span class="line">    Epoch: 0036 loss = 0.028104</span><br><span class="line">    Epoch: 0037 loss = 0.015952</span><br><span class="line">    Epoch: 0038 loss = 0.014356</span><br><span class="line">    Epoch: 0039 loss = 0.015536</span><br><span class="line">    Epoch: 0040 loss = 0.013210</span><br><span class="line">    Epoch: 0041 loss = 0.015791</span><br><span class="line">    Epoch: 0042 loss = 0.013085</span><br><span class="line">    Epoch: 0043 loss = 0.011149</span><br><span class="line">    Epoch: 0044 loss = 0.009110</span><br><span class="line">    Epoch: 0045 loss = 0.007416</span><br><span class="line">    Epoch: 0046 loss = 0.005960</span><br><span class="line">    Epoch: 0047 loss = 0.006156</span><br><span class="line">    Epoch: 0048 loss = 0.004907</span><br><span class="line">    Epoch: 0049 loss = 0.004867</span><br><span class="line">    Epoch: 0050 loss = 0.005042</span><br><span class="line">    Epoch: 0051 loss = 0.005796</span><br><span class="line">    Epoch: 0052 loss = 0.005398</span><br><span class="line">    Epoch: 0053 loss = 0.004669</span><br><span class="line">    Epoch: 0054 loss = 0.004401</span><br><span class="line">    Epoch: 0055 loss = 0.003372</span><br><span class="line">    Epoch: 0056 loss = 0.002630</span><br><span class="line">    Epoch: 0057 loss = 0.002565</span><br><span class="line">    Epoch: 0058 loss = 0.002309</span><br><span class="line">    Epoch: 0059 loss = 0.003040</span><br><span class="line">    Epoch: 0060 loss = 0.002470</span><br><span class="line">    Epoch: 0061 loss = 0.002096</span><br><span class="line">    Epoch: 0062 loss = 0.002189</span><br><span class="line">    Epoch: 0063 loss = 0.002061</span><br><span class="line">    Epoch: 0064 loss = 0.001174</span><br><span class="line">    Epoch: 0065 loss = 0.001599</span><br><span class="line">    Epoch: 0066 loss = 0.001527</span><br><span class="line">    Epoch: 0067 loss = 0.001685</span><br><span class="line">    Epoch: 0068 loss = 0.001565</span><br><span class="line">    Epoch: 0069 loss = 0.001718</span><br><span class="line">    Epoch: 0070 loss = 0.001291</span><br><span class="line">    Epoch: 0071 loss = 0.001259</span><br><span class="line">    Epoch: 0072 loss = 0.001222</span><br><span class="line">    Epoch: 0073 loss = 0.001179</span><br><span class="line">    Epoch: 0074 loss = 0.000965</span><br><span class="line">    Epoch: 0075 loss = 0.001888</span><br><span class="line">    Epoch: 0076 loss = 0.001052</span><br><span class="line">    Epoch: 0077 loss = 0.000888</span><br><span class="line">    Epoch: 0078 loss = 0.001349</span><br><span class="line">    Epoch: 0079 loss = 0.000916</span><br><span class="line">    Epoch: 0080 loss = 0.001315</span><br><span class="line">    Epoch: 0081 loss = 0.001191</span><br><span class="line">    Epoch: 0082 loss = 0.001341</span><br><span class="line">    Epoch: 0083 loss = 0.001674</span><br><span class="line">    Epoch: 0084 loss = 0.001122</span><br><span class="line">    Epoch: 0085 loss = 0.001133</span><br><span class="line">    Epoch: 0086 loss = 0.000839</span><br><span class="line">    Epoch: 0087 loss = 0.001059</span><br><span class="line">    Epoch: 0088 loss = 0.001204</span><br><span class="line">    Epoch: 0089 loss = 0.001092</span><br><span class="line">    Epoch: 0090 loss = 0.000943</span><br><span class="line">    Epoch: 0091 loss = 0.000699</span><br><span class="line">    Epoch: 0092 loss = 0.001015</span><br><span class="line">    Epoch: 0093 loss = 0.000730</span><br><span class="line">    Epoch: 0094 loss = 0.000795</span><br><span class="line">    Epoch: 0095 loss = 0.000926</span><br><span class="line">    Epoch: 0096 loss = 0.000948</span><br><span class="line">    Epoch: 0097 loss = 0.000945</span><br><span class="line">    Epoch: 0098 loss = 0.000730</span><br><span class="line">    Epoch: 0099 loss = 0.000747</span><br><span class="line">    Epoch: 0100 loss = 0.000749</span><br></pre></td></tr></table></figure>
<h1 id="测试">测试</h1>
<p>这段代码是一个贪心解码器(greedy
decoder)的实现，其作用是在给定编码输入(enc_input)和起始符号(start_symbol)的情况下，根据给定的模型(model)预测出目标序列(greedy_dec_predict)。</p>
<p>首先，编码器(encoder)对编码输入(enc_input)进行处理，生成编码输出(enc_outputs)和注意力权值(enc_self_attns)。</p>
<p>然后初始化解码器(decoder)的输入(dec_input)为一个空的tensor。</p>
<p>接着，在没有到达终止符的情况下，不断执行以下步骤：</p>
<ol type="1">
<li>将解码器的输入(dec_input)拼接上当前的符号(next_symbol)。</li>
<li>解码器(decoder)对拼接后的输入(dec_input)、编码输入(enc_input)和编码输出(enc_outputs)进行处理，生成解码输出(dec_outputs)。</li>
<li>投影层(projection)将解码输出(dec_outputs)映射到词表上，生成预测概率分布(projected)。</li>
<li>根据预测概率分布(projected)，选择概率最大的下一个词，并将其作为下一个符号(next_symbol)。</li>
<li>如果下一个符号是终止符，终止循环。</li>
</ol>
<p>最后，返回除开初始符号以外的预测的目标序列(greedy_dec_predict)。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decoder</span>(<span class="params">model, enc_input, start_symbol</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;贪心编码</span></span><br><span class="line"><span class="string">    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don&#x27;t know the</span></span><br><span class="line"><span class="string">    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.</span></span><br><span class="line"><span class="string">    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding</span></span><br><span class="line"><span class="string">    :param model: Transformer Model</span></span><br><span class="line"><span class="string">    :param enc_input: The encoder input</span></span><br><span class="line"><span class="string">    :param start_symbol: The start symbol. In this example it is &#x27;S&#x27; which corresponds to index 8</span></span><br><span class="line"><span class="string">    :return: The target input</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    enc_outputs, enc_self_attns = model.encoder(enc_input)</span><br><span class="line">    dec_input = torch.zeros(<span class="number">1</span>, <span class="number">0</span>).type_as(enc_input.data)  <span class="comment"># 初始化一个空的tensor: tensor([], size=(1, 0), dtype=torch.int64)</span></span><br><span class="line">    terminal = <span class="literal">False</span></span><br><span class="line">    next_symbol = start_symbol</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> terminal:</span><br><span class="line">        <span class="comment"># 预测阶段：dec_input序列会一点点变长（每次添加一个新预测出来的单词）</span></span><br><span class="line">        dec_input = torch.cat([dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],</span><br><span class="line">                              -<span class="number">1</span>)</span><br><span class="line">        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)</span><br><span class="line">        projected = model.projection(dec_outputs)</span><br><span class="line">        prob = projected.squeeze(<span class="number">0</span>).<span class="built_in">max</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">False</span>)[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 这行代码替代了softmax层的工作，直接得到概率最大值的词表索引</span></span><br><span class="line">        <span class="comment"># 1. 从 tensor 中删除所有维度大小为1的维：projected.squeeze(0)；</span></span><br><span class="line">        <span class="comment"># 2. 通过 dim=-1 参数，在最后一维（即维度的索引为 -1）上，找到最大的值的索引：max(dim=-1)；</span></span><br><span class="line">        <span class="comment"># 3. 通过 keepdim=False 参数，将最后一维的维度删除，同时返回结果的最大值的索引：[1]。</span></span><br><span class="line">        <span class="comment"># 因此，该行代码得到prob 变量的结果，存储了 projected 处理后得到的最大值的索引。</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 增量更新（我们希望重复单词预测结果是一样的）</span></span><br><span class="line">        <span class="comment"># 我们在预测时会选择性忽略重复的预测的词，只摘取最新预测的单词拼接到输入序列中</span></span><br><span class="line">        next_word = prob.data[-<span class="number">1</span>]  <span class="comment"># 拿出当前预测的单词(数字)。我们用x_t对应的输出z_t去预测下一个单词的概率，不用z_1,z_2..z_&#123;t-1&#125;</span></span><br><span class="line">        next_symbol = next_word</span><br><span class="line">        <span class="keyword">if</span> next_symbol == tgt_vocab[<span class="string">&quot;E&quot;</span>]:</span><br><span class="line">            terminal = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># print(next_word)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># greedy_dec_predict = torch.cat(</span></span><br><span class="line">    <span class="comment">#     [dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],</span></span><br><span class="line">    <span class="comment">#     -1)</span></span><br><span class="line">    greedy_dec_predict = dec_input[:, <span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">return</span> greedy_dec_predict</span><br></pre></td></tr></table></figure>
<p>这段代码实现了一个简单的预测（因为数据量较小，因此测试集选用训练集中的一句）</p>
<ul>
<li>测试集（希望transformer能达到的效果）</li>
<li>输入："我 有 一 个 女 朋 友"</li>
<li>输出："i have a girl friend"</li>
</ul>
<p>过程如下：</p>
<ol type="1">
<li>定义了一个句子列表sentences，其中包含一个中文句子和对应的空的英文句子。</li>
<li>使用make_data函数处理句子列表，获得编码句子、解码句子的输入和输出。</li>
<li>创建一个数据加载器test_loader，用于加载处理后的句子数据。</li>
<li>使用next函数从数据加载器中读取一个批次的数据，并将其分别赋给编码句子的输入。</li>
<li>对于每个编码句子，调用greedy_decoder函数，通过训练好的Transformer模型，将其翻译成英文句子。</li>
<li>最后，输出编码句子和对应的解码句子，以及它们的中英文词语对应关系。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ==========================================================================================</span></span><br><span class="line"><span class="comment"># 预测阶段</span></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="comment"># enc_input                dec_input           dec_output</span></span><br><span class="line">    [<span class="string">&#x27;我 有 一 个 女 朋 友&#x27;</span>, <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line">test_loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line">enc_inputs, _, _ = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_loader))</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用训练好的Transformer模型将中文句子&#x27;我 有 一 个 女 朋 友&#x27; 翻译成英文句子: &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(enc_inputs)):</span><br><span class="line">    greedy_dec_predict = greedy_decoder(model, enc_inputs[i].view(<span class="number">1</span>, -<span class="number">1</span>).to(device), start_symbol=tgt_vocab[<span class="string">&quot;S&quot;</span>])</span><br><span class="line">    <span class="built_in">print</span>(enc_inputs[i], <span class="string">&#x27;-&gt;&#x27;</span>, greedy_dec_predict.squeeze())</span><br><span class="line">    <span class="built_in">print</span>([src_idx2word[t.item()] <span class="keyword">for</span> t <span class="keyword">in</span> enc_inputs[i]], <span class="string">&#x27;-&gt;&#x27;</span>,</span><br><span class="line">          [idx2word[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> greedy_dec_predict.squeeze()])</span><br></pre></td></tr></table></figure>
<pre><code>==============================
利用训练好的Transformer模型将中文句子&#39;我 有 一 个 女 朋 友&#39; 翻译成英文句子: 
tensor([1, 2, 3, 4, 9, 6, 7]) -&gt; tensor([ 1,  2,  3,  7,  5, 10])
[&#39;我&#39;, &#39;有&#39;, &#39;一&#39;, &#39;个&#39;, &#39;女&#39;, &#39;朋&#39;, &#39;友&#39;] -&gt; [&#39;i&#39;, &#39;have&#39;, &#39;a&#39;, &#39;girl&#39;, &#39;friend&#39;, &#39;.&#39;]</code></pre>
<p><strong>next 函数说明：</strong></p>
<p>"next"
函数用于返回迭代器的下一个项目，即从迭代器中获取下一个数据项。在这段代码中，使用
next(iter(test_loader)) 获取第一个批次的数据，赋值给 enc_inputs, _ , _
三个变量。</p>
<p><strong>因为数据量较小，如果测试集选用新的句子，那么结果就不尽人意</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ==========================================================================================</span></span><br><span class="line"><span class="comment"># 预测阶段</span></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="comment"># enc_input                dec_input           dec_output</span></span><br><span class="line">    [<span class="string">&#x27;我 有 一 个 好 朋 友&#x27;</span>, <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line">test_loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line">enc_inputs, _, _ = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_loader))</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用训练好的Transformer模型将中文句子&#x27;我 有 一 个 好 朋 友&#x27; 翻译成英文句子: &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(enc_inputs)):</span><br><span class="line">    greedy_dec_predict = greedy_decoder(model, enc_inputs[i].view(<span class="number">1</span>, -<span class="number">1</span>).to(device), start_symbol=tgt_vocab[<span class="string">&quot;S&quot;</span>])</span><br><span class="line">    <span class="built_in">print</span>(enc_inputs[i], <span class="string">&#x27;-&gt;&#x27;</span>, greedy_dec_predict.squeeze())</span><br><span class="line">    <span class="built_in">print</span>([src_idx2word[t.item()] <span class="keyword">for</span> t <span class="keyword">in</span> enc_inputs[i]], <span class="string">&#x27;-&gt;&#x27;</span>,</span><br><span class="line">          [idx2word[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> greedy_dec_predict.squeeze()])</span><br></pre></td></tr></table></figure>
<pre><code>==============================
利用训练好的Transformer模型将中文句子&#39;我 有 一 个 好 朋 友&#39; 翻译成英文句子: 
tensor([1, 2, 3, 4, 5, 6, 7]) -&gt; tensor([ 1,  2,  3,  7,  5, 10])
[&#39;我&#39;, &#39;有&#39;, &#39;一&#39;, &#39;个&#39;, &#39;好&#39;, &#39;朋&#39;, &#39;友&#39;] -&gt; [&#39;i&#39;, &#39;have&#39;, &#39;a&#39;, &#39;girl&#39;, &#39;friend&#39;, &#39;.&#39;]</code></pre>
<h1 id="参考链接">参考链接</h1>
<h2 id="本文详细代码">本文详细代码</h2>
<ol type="1">
<li><p>Colab:</p>
<p><a
target="_blank" rel="noopener" href="https://github.com/serika-onoe/transformer_reproduction/blob/main/Transformer_Pytorch.ipynb">https://github.com/serika-onoe/transformer_reproduction/blob/main/Transformer_Pytorch.ipynb</a></p></li>
<li><p>Github:</p>
<p><a
target="_blank" rel="noopener" href="https://github.com/serika-onoe/transformer_reproduction">https://github.com/serika-onoe/transformer_reproduction</a></p></li>
</ol>
<h2 id="参考网站">参考网站</h2>
<ol type="1">
<li><p>Transformer 的 PyTorch 实现:</p>
<p><a
target="_blank" rel="noopener" href="https://wmathor.com/index.php/archives/1455/">https://wmathor.com/index.php/archives/1455/</a></p></li>
<li><p>手把手教你用Pytorch代码实现Transformer模型（超详细的代码解读）:</p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43827595/article/details/120394042">https://blog.csdn.net/qq_43827595/article/details/120394042</a></p></li>
</ol>
<h2 id="完整代码实现">完整代码实现</h2>
<ul>
<li><p>哈佛大学NLP组的notebook，很详细文字和代码描述，用pytorch实现</p>
<p><a
target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p></li>
<li><p>Google的TensorFlow官方的，用tf keras实现</p>
<p><a
target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/text/transformer">https://www.tensorflow.org/tutorials/text/transformer</a></p></li>
</ul>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94/">科研</a><a class="post-meta__tags" href="/tags/transformer/">transformer</a></div><div class="post_share"><div class="social-share" data-image="https://github.com/serika-onoe/web-img/raw/main/Transformer/simpletransformers-logo.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/"><img class="prev-cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/harvard.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Transformer 零基础解析教程，完整版代码最终挑战（4/4）</div></div></a></div><div class="next-post pull-right"><a href="/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/"><img class="next-cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/" title="Transformer 零基础解析教程，从Encoder-Decoder架构说起（1&#x2F;4）"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-14</div><div class="title">Transformer 零基础解析教程，从Encoder-Decoder架构说起（1&#x2F;4）</div></div></a></div><div><a href="/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/" title="Transformer 零基础解析教程，完整版代码最终挑战（4&#x2F;4）"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/harvard.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-05</div><div class="title">Transformer 零基础解析教程，完整版代码最终挑战（4&#x2F;4）</div></div></a></div><div><a href="/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/" title="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2&#x2F;4）"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-11</div><div class="title">Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2&#x2F;4）</div></div></a></div><div><a href="/2022/12/09/%E6%88%91%E7%9A%84%E7%A7%91%E7%A0%94%E7%BB%8F%E5%8E%86/" title="我的项目经历"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Experience/uav3(1).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-09</div><div class="title">我的项目经历</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/coder.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">空之影</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">12</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/serika-onoe" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:zgong313@connect.hkust-gz.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Transformer 解读系列共计四篇文章，均已大功告成，请放心享用！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AF%BC%E8%88%AA"><span class="toc-number">1.</span> <span class="toc-text">导航</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">2.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">数据预处理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="toc-number">4.</span> <span class="toc-text">模型构建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#positional-encoding"><span class="toc-number">4.1.</span> <span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pad-mask"><span class="toc-number">4.2.</span> <span class="toc-text">Pad Mask</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#subsequence-mask"><span class="toc-number">4.3.</span> <span class="toc-text">Subsequence Mask</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scaleddotproductattention"><span class="toc-number">4.4.</span> <span class="toc-text">ScaledDotProductAttention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#multiheadattention"><span class="toc-number">4.5.</span> <span class="toc-text">MultiHeadAttention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#feedforward-layer"><span class="toc-number">4.6.</span> <span class="toc-text">FeedForward Layer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#encoder-layer-encoder"><span class="toc-number">4.7.</span> <span class="toc-text">Encoder Layer &amp; Encoder</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#decoder-layer-decoder"><span class="toc-number">5.</span> <span class="toc-text">Decoder Layer &amp; Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#transformer"><span class="toc-number">5.1.</span> <span class="toc-text">Transformer</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%B0%83%E7%94%A8-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">6.</span> <span class="toc-text">模型调用 &amp; 损失函数 &amp;
优化器</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">7.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-number">8.</span> <span class="toc-text">测试</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">9.</span> <span class="toc-text">参考链接</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E6%96%87%E8%AF%A6%E7%BB%86%E4%BB%A3%E7%A0%81"><span class="toc-number">9.1.</span> <span class="toc-text">本文详细代码</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E7%BD%91%E7%AB%99"><span class="toc-number">9.2.</span> <span class="toc-text">参考网站</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">9.3.</span> <span class="toc-text">完整代码实现</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/" title="Transformer 零基础解析教程，完整版代码最终挑战（4/4）"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/harvard.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 零基础解析教程，完整版代码最终挑战（4/4）"/></a><div class="content"><a class="title" href="/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/" title="Transformer 零基础解析教程，完整版代码最终挑战（4/4）">Transformer 零基础解析教程，完整版代码最终挑战（4/4）</a><time datetime="2023-02-05T03:53:10.000Z" title="发表于 2023-02-05 11:53:10">2023-02-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/" title="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/simpletransformers-logo.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）"/></a><div class="content"><a class="title" href="/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/" title="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）">Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）</a><time datetime="2023-01-31T09:00:10.000Z" title="发表于 2023-01-31 17:00:10">2023-01-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/" title="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）"/></a><div class="content"><a class="title" href="/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/" title="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）">Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）</a><time datetime="2023-01-11T09:00:10.000Z" title="发表于 2023-01-11 17:00:10">2023-01-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/20/CS231n-Assignment-1/" title="CS231n Assignment 1 逐行解析"><img src="https://s1.ax1x.com/2023/02/04/pSyTy3q.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CS231n Assignment 1 逐行解析"/></a><div class="content"><a class="title" href="/2022/12/20/CS231n-Assignment-1/" title="CS231n Assignment 1 逐行解析">CS231n Assignment 1 逐行解析</a><time datetime="2022-12-20T14:49:13.000Z" title="发表于 2022-12-20 22:49:13">2022-12-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/" title="Transformer 零基础解析教程，从Encoder-Decoder架构说起（1/4）"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 零基础解析教程，从Encoder-Decoder架构说起（1/4）"/></a><div class="content"><a class="title" href="/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/" title="Transformer 零基础解析教程，从Encoder-Decoder架构说起（1/4）">Transformer 零基础解析教程，从Encoder-Decoder架构说起（1/4）</a><time datetime="2022-12-14T01:42:13.000Z" title="发表于 2022-12-14 09:42:13">2022-12-14</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By 空之影</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script defer src="https://cdn.jsdelivr.net/gh/CodeByZach/pace/pace.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>