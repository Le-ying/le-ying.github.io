<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Transformer 零基础解析教程，完整版代码最终挑战（4/4） | 空之影的技术博客</title><meta name="author" content="空之影"><meta name="copyright" content="空之影"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="导航 这篇文章是 Transformer 完整版复现代码的深度解析。  本博客的 Transformer 系列文章共计四篇，导航如下：   Transformer 零基础解析教程，从Encoder-Decoder架构说起（1&#x2F;4） Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2&#x2F;4） Transformer 零基础解析教程，牛刀小试Pytorch简易版">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer 零基础解析教程，完整版代码最终挑战（4&#x2F;4）">
<meta property="og:url" content="https://zeying-gong.github.io/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/index.html">
<meta property="og:site_name" content="空之影的技术博客">
<meta property="og:description" content="导航 这篇文章是 Transformer 完整版复现代码的深度解析。  本博客的 Transformer 系列文章共计四篇，导航如下：   Transformer 零基础解析教程，从Encoder-Decoder架构说起（1&#x2F;4） Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2&#x2F;4） Transformer 零基础解析教程，牛刀小试Pytorch简易版">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/serika-onoe/web-img/raw/main/Transformer/harvard.png">
<meta property="article:published_time" content="2023-02-05T03:53:10.000Z">
<meta property="article:modified_time" content="2023-02-26T07:15:44.987Z">
<meta property="article:author" content="空之影">
<meta property="article:tag" content="科研">
<meta property="article:tag" content="transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/serika-onoe/web-img/raw/main/Transformer/harvard.png"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://zeying-gong.github.io/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"W5J7GO3HT8","apiKey":"53b7d5d068605c78d1a20d7f3671629a","indexName":"test_serika","hits":{"per_page":3},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer 零基础解析教程，完整版代码最终挑战（4/4）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-26 15:15:44'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/CodeByZach/pace/themes/green/pace-theme-flash.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="空之影的技术博客" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/coder.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://github.com/serika-onoe/web-img/raw/main/Transformer/harvard.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">空之影的技术博客</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 总览</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Transformer 零基础解析教程，完整版代码最终挑战（4/4）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-02-05T03:53:10.000Z" title="发表于 2023-02-05 11:53:10">2023-02-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-02-26T07:15:44.987Z" title="更新于 2023-02-26 15:15:44">2023-02-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%AC%94%E8%AE%B0-%E6%95%99%E7%A8%8B/">笔记, 教程</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transformer 零基础解析教程，完整版代码最终挑战（4/4）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="导航">导航</h1>
<p><strong>这篇文章是 Transformer
完整版复现代码的深度解析。</strong></p>
<blockquote>
<p>本博客的 Transformer 系列文章共计四篇，导航如下：</p>
</blockquote>
<ul>
<li><p><a
target="_blank" rel="noopener" href="https://serika-onoe.github.io/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/">Transformer
零基础解析教程，从Encoder-Decoder架构说起（1/4）</a></p></li>
<li><p><a
target="_blank" rel="noopener" href="https://serika-onoe.github.io/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/">Transformer
零基础解析教程，剥洋葱般层层剖析内在原理（2/4）</a></p></li>
<li><p><a
target="_blank" rel="noopener" href="https://serika-onoe.github.io/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/">Transformer
零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）</a></p></li>
<li><p><a
target="_blank" rel="noopener" href="https://serika-onoe.github.io/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/">Transformer
零基础解析教程，完整版代码最终挑战（4/4）<strong>本篇</strong></a></p></li>
</ul>
<h1 id="前言">前言</h1>
<p>由哈佛的NLP组撰写的 <a
target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">The
Annotated Transformer</a>，用代码对应论文<a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">《Attention is all you
need》</a>的各个部分基本复现和还原了论文模型中初始版本的
Transformer，并给出了两个机器翻译的例子。而本文内容是在 The Annotated
Transformer 的 Colab 版基础上，进一步精读 《Attention is all you
need》中的 Transformer
结构和源码。作者之所以选择它进行解读，是因为它是一个单独的ipynb文件，如果我们要实际使用只需要全部执行单元格就行了。而一些比较大的库（比如Tensor2Tensor和OpenNMT等等）尽管也包含了Transformer的实现，但是对于理解论文和跑代码来说不够方便，为什么不选择更简单实用的现成品呢？</p>
<p>本文完整版及执行结果可见我在Google Colab的注释版副本 <a
target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1LrYYeXvIS_LJURYZFp_MDdJvRJmDACwL#scrollTo=uNDgJW-gFn_g">The
Annotated Transformer by Harvard NLP</a>
.同时告诉大家一个小trick：不妨试着用Chatgpt进行代码讲解，本系列文章有相当一部分代码讲解都是经过Chatgpt辅助我理解消化的。读者可以尝试一下<a
target="_blank" rel="noopener" href="https://chat.openai.com/">ChatGPT</a>，我一般尊称它为
<strong>“Chat老师”</strong>，它是一个非常耐心的老师，（最重要的，它还是免费哒！）这让我想起了柏拉图或者孔夫子的教学方式——通过青年问禅师的对话体，来回答读者的困惑，并启发更深层次的哲学思考。</p>
<h1 id="背景">背景</h1>
<p>The goal of reducing sequential computation also forms the foundation
of the Extended Neural GPU, ByteNet and ConvS2S, all of which use
convolutional neural networks as basic building block, computing hidden
representations in parallel for all input and output positions. In these
models, the number of operations required to relate signals from two
arbitrary input or output positions grows in the distance between
positions, linearly for ConvS2S and logarithmically for ByteNet. This
makes it more difficult to learn dependencies between distant positions.
In the Transformer this is reduced to a constant number of operations,
albeit at the cost of reduced effective resolution due to averaging
attention-weighted positions, an effect we counteract with Multi-Head
Attention.</p>
<p>Self-attention, sometimes called intra-attention is an attention
mechanism relating different positions of a single sequence in order to
compute a representation of the sequence. Self-attention has been used
successfully in a variety of tasks including reading comprehension,
abstractive summarization, textual entailment and learning
task-independent sentence representations. End-to-end memory networks
are based on a recurrent attention mechanism instead of sequencealigned
recurrence and have been shown to perform well on simple-language
question answering and language modeling tasks.</p>
<p>To the best of our knowledge, however, the Transformer is the first
transduction model relying entirely on self-attention to compute
representations of its input and output without using sequence aligned
RNNs or convolution.</p>
<blockquote>
<p>翻译</p>
</blockquote>
<p>减少序列计算（<code>sequential computation</code>）的目标也是<code>Extended Neural GPU、ByteNet</code>、以及<code>ConvS2S</code>等模型的基础，所有这些都是使用CNN作为基础块，对于所有的输入输出位置并行计算隐层表示。在这些模型当中，模型<code>ConvS2S</code>将任意输入输出信号联系起来要求操作的次数与这两者位置间的距离呈线性关系，而模型<code>ByteNet</code>则呈对数关系。
这使得学习远距离的依赖变得更加困难。然而在Transformer中，这个复杂度被减至一个常数操作，尽管由于平均
<code>attention-weighted</code>
位置在以减少有效地解析为代价，但我们提出一种
<code>Multi-head Attention</code> 用于抵消这种影响。</p>
<p>自我注意（<code>self-attention</code>），有时也称为内部注意，是一个注意力机制，这种注意力机制：将单个句子不同的位置联系起来，用于计算一个序列的表示。自我注意已经被成功应用在个各种各样的任务中，诸如阅读理解，抽象总结，文本蕴含，以及学习独立任务的句子表示中。端到端的记忆网络是基于递归注意力机制，而不是对齐序列递归，同时在单一语言问答系统以及语言建模任务中，端到端的网络已经被证明效果很好。</p>
<p>然而，据我们所知，Transformer
是第一个完全依赖于自我注意的推导模型。在接下来的部分中，我们将描述Transformer，自驱动的自我注意力机制，并且讨论它们的优缺点。</p>
<h1 id="第一部分-模型架构">第一部分: 模型架构</h1>
<p>Most competitive neural sequence transduction models have an
encoder-decoder structure <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">(cite)</a>. Here, the encoder
maps an input sequence of symbol representations <span
class="math inline">\((x_1, ..., x_n)\)</span> to a sequence of
continuous representations <span class="math inline">\(\mathbf{z} =
(z_1, ..., z_n)\)</span>. Given <span
class="math inline">\(\mathbf{z}\)</span>, the decoder then generates an
output sequence <span class="math inline">\((y_1,...,y_m)\)</span> of
symbols one element at a time. At each step the model is auto-regressive
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1308.0850">(cite)</a>, consuming the
previously generated symbols as additional input when generating the
next.</p>
<blockquote>
<p>翻译</p>
</blockquote>
<p>大多数的神经序列推导模型有一个 <code>encoder-decoder</code>
结构。这里，<code>encoder</code> 将一个表示输入的字符序列 (<span
class="math inline">\(x_1\)</span>,<span
class="math inline">\(x_2\)</span>,...<span
class="math inline">\(x_n\)</span>) 映射成另一种连续表征序列 Z=(<span
class="math inline">\(z_1\)</span>,<span
class="math inline">\(z_2\)</span>,...<span
class="math inline">\(z_m\)</span>)。 基于Z，<code>decoder</code>
每次产生一个元素 <span class="math inline">\(y_i\)</span>
最终输出一个序列 Y=(<span class="math inline">\(y_1\)</span>,...<span
class="math inline">\(y_m\)</span>)
。<code>Decoder</code>中的每个步骤都是自回归的 ——
消耗之前产生的表征作为下次的输入。</p>
<p>在这里，"消耗
"意味着模型使用或吸收上一步生成的符号作为输入。换句话说，上一步生成的输出成为当前步骤的输入的一部分。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. </span></span><br><span class="line"><span class="string">    Base for this and many other models.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, </span></span><br><span class="line"><span class="params">      src_embed, tgt_embed, generator</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        <span class="comment"># Encoder对象</span></span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        <span class="comment"># Decoder对象</span></span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        <span class="comment"># 源语言序列的编码，包括词嵌入和位置编码</span></span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        <span class="comment"># 目标语言序列的编码，包括词嵌入和位置编码</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        <span class="comment"># 生成器（Decoder后面 Linear + Softmax 部分）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Take in and process masked src and target sequences.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">        <span class="comment"># 先对源语言序列进行编码，</span></span><br><span class="line">        <span class="comment"># 结果作为memory传递给目标语言的编码器</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        <span class="comment"># src = (batch.size, seq.length)</span></span><br><span class="line">        <span class="comment"># src_mask 负责对src加掩码</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">        <span class="comment"># 对源语言序列进行编码，得到的结果为</span></span><br><span class="line">        <span class="comment"># (batch.size, seq.length, 512)的tensor</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), </span><br><span class="line">          memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="comment"># 对目标语言序列进行编码，得到的结果为</span></span><br><span class="line">        <span class="comment"># (batch.size, seq.length, 512)的tensor</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Define standard linear + softmax generation step.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="comment"># d_model=512， vocab = 目标语言词表大小</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line">        <span class="comment"># 定义一个全连接层，可训练参数个数是(512 * trg_vocab_size) + trg_vocab_size </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># x 类似于 (batch.size, sequence.length, 512)</span></span><br><span class="line">        <span class="comment"># -&gt; proj 全连接层 (30, 10, trg_vocab_size) = logits</span></span><br><span class="line">        <span class="comment"># 对最后一个维度执行log_soft_max 得到(30, 10, trg_vocab_size)</span></span><br></pre></td></tr></table></figure>
<p>The Transformer follows this overall architecture using stacked
self-attention and point-wise, fully connected layers for both the
encoder and decoder, shown in the left and right halves of Figure 1,
respectively.</p>
<blockquote>
<p>翻译</p>
</blockquote>
<p><code>Transformer</code>
遵循了这种总体的架构：在encoder和decoder中都使用数层<code>self-attention</code>
和 <code>point-wise</code>，全连接层。相对的各个部分正如Figure
1中左右两部分描述的一样。</p>
<p><img
src="https://github.com/harvardnlp/annotated-transformer/blob/master/images/ModalNet-21.png?raw=1" /></p>
<h2 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h2>
<h3 id="encoder">Encoder</h3>
<p>The encoder is composed of a stack of <span
class="math inline">\(N=6\)</span> identical layers.</p>
<blockquote>
<p>翻译</p>
</blockquote>
<p>Encoder 由6个相同的层叠加而成。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 clones 函数</p>
</blockquote>
<p>实现一个网络的深copy，也就是说copy一个新的对象，和原来的对象，完全分离，不分享任何存储空间，从而保证可训练参数，都有自己的取值或梯度。</p>
<p>比如可以用在copy N 个 EncoderLayer 组成 Encoder</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Core encoder is a stack of N layers&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="comment"># layer = one EncoderLayer object, N=6</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N) </span><br><span class="line">        <span class="comment"># 深copy，N=6，</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        <span class="comment"># 定义一个LayerNorm，layer.size=d_model=512</span></span><br><span class="line">        <span class="comment"># 其中有两个可训练参数a_2和b_2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="comment"># x is alike (30, 10, 512)</span></span><br><span class="line">        <span class="comment"># (batch.size, sequence.len, d_model)</span></span><br><span class="line">        <span class="comment"># mask是类似于(batch.size, 10, 10)的矩阵</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">            <span class="comment"># 进行6次EncoderLayer操作</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line">        <span class="comment"># 最后做一次LayerNorm，最后的输出也是(30, 10, 512)</span></span><br></pre></td></tr></table></figure>
<p>Each layer has two sub-layers. The first is a multi-head
self-attention mechanism, and the second is a simple, position-wise
fully connected feed-forward network.</p>
<p>We employ a residual connection <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.03385">(cite)</a> around each of the
two sub-layers, followed by layer normalization <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.06450">(cite)</a>.</p>
<p>That is, the output of each sub-layer is <span
class="math inline">\(\mathrm{LayerNorm}(x +
\mathrm{Sublayer}(x))\)</span>, where <span
class="math inline">\(\mathrm{Sublayer}(x)\)</span> is the function
implemented by the sub-layer itself. We apply dropout <a
target="_blank" rel="noopener" href="http://jmlr.org/papers/v15/srivastava14a.html">(cite)</a> to the
output of each sub-layer, before it is added to the sub-layer input and
normalized.</p>
<p>To facilitate these residual connections, all sub-layers in the
model, as well as the embedding layers, produce outputs of dimension
<span class="math inline">\(d_{\text{model}}=512\)</span>.</p>
<blockquote>
<p>翻译</p>
</blockquote>
<p>Encoder
由6个相同的层叠加而成。每层又分成两个子层。第一层是<code>multi-head self-attention</code>机制，第二层则是简单的<code>position-wise fully connected feed-forward network</code>。</p>
<p>在每个子层中，我们都使用残差网络，然后紧接着一个
<code>layer normalization</code>。</p>
<p>也就是说：其实每个子层的实际输出是
<code>LayerNorm(x+Sublayer(x))</code>，其中<code>Sublayer(x)</code>是由sub-layer层实现的。</p>
<p>为了简化这个残差连接，模型中的所有子层都与 embedding
层相同，输出的结果维度都是 <span
class="math inline">\(d_{model}\)</span> = 512。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Construct a layernorm module (See citation for details).&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="comment"># features=d_model=512, eps=epsilon 用于分母的非0化平滑</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        <span class="comment"># a_2 是一个可训练参数向量，(512)</span></span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        <span class="comment"># b_2 也是一个可训练参数向量, (512)</span></span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x 的形状为(batch.size, sequence.len, 512)</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">        <span class="comment"># 对x的最后一个维度，取平均值，得到tensor (batch.size, seq.len)</span></span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 对x的最后一个维度，取标准差，得(batch.size, seq.len)</span></span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br><span class="line">        <span class="comment"># 本质上类似于（x-mean)/std，不过这里加入了两个可训练向量a_2 and b_2</span></span><br><span class="line">        <span class="comment"># 以及分母上增加一个极小值epsilon，用来防止std为0的时候，除法溢出</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="comment"># size=d_model=512; dropout=0.1</span></span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size) <span class="comment"># (512)，用来定义a_2和b_2</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;Apply residual connection to any sublayer with the &quot;</span></span><br><span class="line">        <span class="string">&quot;same size.&quot;</span></span><br><span class="line">        <span class="comment"># x is alike (batch.size, sequence.len, 512)</span></span><br><span class="line">        <span class="comment"># sublayer是一个具体的MultiHeadAttention或者PositionwiseFeedForward对象</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line">        <span class="comment"># x (30, 10, 512) -&gt; norm (LayerNorm) -&gt; (30, 10, 512)</span></span><br><span class="line">        <span class="comment"># -&gt; sublayer (MultiHeadAttention or PositionwiseFeedForward)</span></span><br><span class="line">        <span class="comment"># -&gt; (30, 10, 512) -&gt; dropout -&gt; (30, 10, 512)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 然后输入的x（没有走sublayer) + 和走sublayer后的结果，即实现了残差相加的功能</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Encoder is made up of self-attn and &quot;</span></span><br><span class="line">    <span class="string">&quot;feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="comment"># size = d_model = 512</span></span><br><span class="line">        <span class="comment"># self_attn = MultiHeadAttention对象, first sublayer</span></span><br><span class="line">        <span class="comment"># feed_forward = PositionwiseFeedForward对象，second sublayer</span></span><br><span class="line">        <span class="comment"># dropout = 0.1 (e.g.)</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 使用深度克隆方法，完整地复制出来两个SublayerConnection</span></span><br><span class="line">        self.size = size </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (left) for connections.&quot;</span></span><br><span class="line">        <span class="comment"># x shape = (30, 10, 512)</span></span><br><span class="line">        <span class="comment"># mask 是(batch.size, 10,10)的矩阵，类似于当前一个词w，有哪些词是w可见的</span></span><br><span class="line">        <span class="comment"># 源语言的序列的话，所有其他词都可见，除了&quot;&lt;blank&gt;&quot;这样的填充；</span></span><br><span class="line">        <span class="comment"># 目标语言的序列的话，所有w的左边的词，都可见。</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, </span><br><span class="line">          <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="comment"># lambda 匿名函数，说明见下文</span></span><br><span class="line">        <span class="comment"># x (30, 10, 512) -&gt; self_attn (MultiHeadAttention) </span></span><br><span class="line">        <span class="comment"># shape is same (30, 10, 512) -&gt; SublayerConnection -&gt; (30, 10, 512)</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line">        <span class="comment"># x 和feed_forward对象一起，给第二个SublayerConnection</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 SublayerConnection 实现和论文间的差异</p>
</blockquote>
<p>论文中的执行顺序是 LayerNorm(x +
Sublayer(x))，即把LayerNorm放残差和的外边了，对应的代码应该是return
self.norm(x + self.dropout(sublayer(x)))。</p>
<p>这里的实现是先把x进行layernorm，然后扔给sublayer(例如multi-head
self-attention, position-wise
feed-forward)，也有一定的道理（效果上并不差），类似于在复杂操作前，先layernorm。</p>
<blockquote>
<p>关于 EncoderLayer 中，匿名函数 lambda 的说明</p>
</blockquote>
<p>稍微难理解的是 EncoderLayer 的 forward
方法使用了lambda来定义一个匿名函数。这是因为之前我们定义的self_attn函数需要4个参数(Query的输入,Key的输入,Value的输入和Mask)。</p>
<p>因此这里我们使用lambda的技巧把它变成一个参数x的函数，mask可以看成已知的数。（<code>lambda x</code>
表示 lambda 的形参也叫
x），如果看到这里觉得难以理解的话，我们不妨把改写的函数抽离出来，记作
<code>self_attn_lambda</code>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">self_attn_lambda</span>(<span class="params">x, mask</span>):</span><br><span class="line">    <span class="keyword">return</span> self.self_attn(x, x, x, mask)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">    x = self.sublayer[<span class="number">0</span>](x, self_attn_lambda(x, mask))</span><br><span class="line">    <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure>
<h3 id="decoder">Decoder</h3>
<p>The decoder is also composed of a stack of <span
class="math inline">\(N=6\)</span> identical layers.</p>
<p>In addition to the two sub-layers in each encoder layer, the decoder
inserts a third sub-layer, which performs multi-head attention over the
output of the encoder stack. Similar to the encoder, we employ residual
connections around each of the sub-layers, followed by layer
normalization.</p>
<p>We also modify the self-attention sub-layer in the decoder stack to
prevent positions from attending to subsequent positions. This masking,
combined with fact that the output embeddings are offset by one
position, ensures that the predictions for position <span
class="math inline">\(i\)</span> can depend only on the known outputs at
positions less than <span class="math inline">\(i\)</span>.</p>
<blockquote>
<p>翻译</p>
</blockquote>
<p>decoder 同样是由 N=6 个相同layer组成的栈。</p>
<p>除了encoder layer中的两个子层之外，decoder
还插入了第三个子层，这层的功能就是：利用 encoder
的输出，执行一个<code>multi-head attention</code>。与encoder相似，在每个子层中，我们都使用一个残差连接，接着在其后跟一个layer
normalization。</p>
<p>为了防止当前位置看到后序的位置，我们同样修改了decoder
栈中的self-attention
子层。这个masking，是基于这样一种事实：输出embedding
偏移了一个位置，确保对位置i的预测仅仅依赖于位置小于i的、已知的输出。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="comment"># layer = DecoderLayer object</span></span><br><span class="line">        <span class="comment"># N = 6</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        <span class="comment"># 深度copy 6次DecoderLayer</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        <span class="comment"># 初始化1个LayerNorm</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">            <span class="comment"># 执行6次DecoderLayer</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line">        <span class="comment"># 执行1次LayerNorm</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Decoder is made of self-attn, src-attn, &quot;</span></span><br><span class="line">    <span class="string">&quot;and feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, src_attn, </span></span><br><span class="line"><span class="params">      feed_forward, dropout</span>):</span><br><span class="line">      <span class="comment"># size = d_model=512,</span></span><br><span class="line">      <span class="comment"># self_attn = one MultiHeadAttention object，目标语言序列的注意力层</span></span><br><span class="line">      <span class="comment"># src_attn = second MultiHeadAttention object, 目标语言序列和源语言序列之间的注意力层</span></span><br><span class="line">      <span class="comment"># feed_forward 全连接层</span></span><br><span class="line">      <span class="comment"># dropout = 0.1</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size <span class="comment"># 512</span></span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># 需要三个SublayerConnection, 分别在</span></span><br><span class="line">        <span class="comment"># self.self_attn, self.src_attn 和 self.feed_forward 的后边</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (right) for connections.&quot;</span></span><br><span class="line">        m = memory <span class="comment"># (batch.size, sequence.len, 512) </span></span><br><span class="line">        <span class="comment"># 来自源语言序列的Encoder之后的输出</span></span><br><span class="line">        <span class="comment"># 作为memory供目标语言的序列检索匹配</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, </span><br><span class="line">          <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        <span class="comment"># 通过一个匿名函数，来实现目标序列的自注意力编码</span></span><br><span class="line">        <span class="comment"># 结果扔给sublayer[0]:SublayerConnection</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, </span><br><span class="line">          <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="comment"># 通过第二个匿名函数，来实现目标序列和源序列的注意力计算</span></span><br><span class="line">        <span class="comment"># 结果扔给sublayer[1]:SublayerConnection</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br><span class="line">        <span class="comment"># 走一个全连接层，然后结果扔给sublayer[2]:SublayerConnection</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">subsequent_mask</span>(<span class="params">size</span>):</span><br><span class="line">    <span class="string">&quot;Mask out subsequent positions.&quot;</span></span><br><span class="line">    <span class="comment"># e.g., size=10</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size) <span class="comment"># (1, 10, 10)</span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    <span class="comment"># triu: 负责生成一个三角矩阵，k-th对角线以下都是设置为0 </span></span><br><span class="line">    <span class="comment"># 上三角中元素为1.</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br><span class="line">    <span class="comment"># 反转上面的triu得到的上三角矩阵，修改为下三角矩阵。</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于subsequent_mask的三角矩阵</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>这里的k=1表示1号对角线，不太容易理解：举例子如下：</p>
<p><img
src="https://pic1.zhimg.com/v2-0972afdd345e18defc3bd6cc59760ad4_b.jpg" /></p>
<blockquote>
<p>Below the attention mask shows the position each tgt word (row) is
allowed to look at (column). Words are blocked for attending to future
words during training.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">example_mask</span>():</span><br><span class="line">    LS_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;Subsequent Mask&quot;</span>: subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>][x, y].flatten(),</span><br><span class="line">                    <span class="string">&quot;Window&quot;</span>: y,</span><br><span class="line">                    <span class="string">&quot;Masking&quot;</span>: x,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(LS_data)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .properties(height=<span class="number">250</span>, width=<span class="number">250</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            alt.X(<span class="string">&quot;Window:O&quot;</span>),</span><br><span class="line">            alt.Y(<span class="string">&quot;Masking:O&quot;</span>),</span><br><span class="line">            alt.Color(<span class="string">&quot;Subsequent Mask:Q&quot;</span>, scale=alt.Scale(scheme=<span class="string">&quot;viridis&quot;</span>)),</span><br><span class="line">        )</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(example_mask)</span><br></pre></td></tr></table></figure>
<h3 id="attention">Attention</h3>
<p>An attention function can be described as mapping a query and a set
of key-value pairs to an output, where the query, keys, values, and
output are all vectors. The output is computed as a weighted sum of the
values, where the weight assigned to each value is computed by a
compatibility function of the query with the corresponding key.</p>
<p>We call our particular attention "Scaled Dot-Product Attention". The
input consists of queries and keys of dimension <span
class="math inline">\(d_k\)</span>, and values of dimension <span
class="math inline">\(d_v\)</span>. We compute the dot products of the
query with all keys, divide each by <span
class="math inline">\(\sqrt{d_k}\)</span>, and apply a softmax function
to obtain the weights on the values.</p>
<p><img
src="https://github.com/harvardnlp/annotated-transformer/blob/master/images/ModalNet-19.png?raw=1" /></p>
<p>In practice, we compute the attention function on a set of queries
simultaneously, packed together into a matrix <span
class="math inline">\(Q\)</span>. The keys and values are also packed
together into matrices <span class="math inline">\(K\)</span> and <span
class="math inline">\(V\)</span>. We compute the matrix of outputs
as:</p>
<blockquote>
<p>翻译</p>
</blockquote>
<p>一个Attention function 可以被描述成一个“映射query和一系列key-value
pair 到输出”，其中 query, keys, values, output 都是一个向量。
这些权值是由 querys 和 keys 通过相关函数计算出来的。
【注：原文中的query就是没有复数形式】</p>
<p>本文中使用的attention
被称作<code>Scaled Dot-Product Attention</code>。输入包含了 <span
class="math inline">\(d_k\)</span> 维的queries 和 keys，以及values
的维数是 <span class="math inline">\(d_v\)</span>。
我们使用所有的values计算query的dot products，然后除以 <span
class="math inline">\(\sqrt{d_k}\)</span>
，再应用一个softmax函数去获得值的权重。【需要关注value在不同地方的含义。是key-value，还是计算得到的value？】</p>
<p>实际处理中，我们将同时计算一系列 query 的 attention，并将这些queries
写成矩阵Q的形式。同样，将 keys, values
同样打包成矩阵K和矩阵V。计算公式如下</p>
<p><span class="math display">\[
   \mathrm{Attention}(Q, K, V) =
\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V
\]</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>): </span><br><span class="line"><span class="comment"># query, key, value的形状类似于(30, 8, 10, 64), (30, 8, 11, 64), </span></span><br><span class="line"><span class="comment"># 30是batch.size，即当前batch中有多少一个序列；8=head.num，注意力头的个数；</span></span><br><span class="line"><span class="comment"># 10=目标序列中词的个数，11=源语言序列传过来的memory中，当前序列的词的个数，</span></span><br><span class="line"><span class="comment"># 64是每个词对应的向量表示。</span></span><br><span class="line"><span class="comment"># 类似于，这里假定query来自target language sequence；</span></span><br><span class="line"><span class="comment"># key和value都来自source language sequence.</span></span><br><span class="line">  <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span> </span><br><span class="line"></span><br><span class="line">  d_k = query.size(-<span class="number">1</span>) <span class="comment"># d_k = 64</span></span><br><span class="line">  scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k) </span><br><span class="line">    <span class="comment"># 先是(30, 8, 10, 64)和(30, 8, 64, 11)相乘，</span></span><br><span class="line">    <span class="comment">#（注意是最后两个维度相乘）得到(30,8,10,11)，</span></span><br><span class="line">    <span class="comment"># 代表10个目标语言序列中每个词和11个源语言序列的分别的“亲密度”。</span></span><br><span class="line">    <span class="comment"># 然后除以sqrt(d_k)=8，防止过大的亲密度。</span></span><br><span class="line">    <span class="comment"># 这里的scores的shape是(30, 8, 10, 11)</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">    scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>) </span><br><span class="line">    <span class="comment"># 使用mask，对已经计算好的scores，按照mask矩阵，填-1e9，</span></span><br><span class="line">    <span class="comment"># 然后在下一步计算softmax的时候，被设置成-1e9的数对应的值~=0,被忽视</span></span><br><span class="line"></span><br><span class="line">  p_attn = F.softmax(scores, dim = -<span class="number">1</span>) </span><br><span class="line">    <span class="comment"># 对scores的最后一个维度执行softmax，得到的还是一个tensor, (30, 8, 10, 11)</span></span><br><span class="line">  <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">    p_attn = dropout(p_attn) <span class="comment">#执行一次dropout</span></span><br><span class="line">  <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回的第一项，是(30,8,10,11)乘以 value=(30,8,11,64)，得到的tensor是(30,8,10,64)，</span></span><br><span class="line">    <span class="comment"># 和query的最初的形状一样。另外，返回p_attn，形状为(30,8,10,11). </span></span><br><span class="line">    <span class="comment"># 注意，这里返回p_attn主要是用来可视化显示多头注意力机制。</span></span><br></pre></td></tr></table></figure>
<p>The two most commonly used attention functions are additive attention
<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.0473">(cite)</a>, and dot-product
(multiplicative) attention. Dot-product attention is identical to our
algorithm, except for the scaling factor of <span
class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>. Additive attention
computes the compatibility function using a feed-forward network with a
single hidden layer. While the two are similar in theoretical
complexity, dot-product attention is much faster and more
space-efficient in practice, since it can be implemented using highly
optimized matrix multiplication code.</p>
<p>While for small values of <span class="math inline">\(d_k\)</span>
the two mechanisms perform similarly, additive attention outperforms dot
product attention without scaling for larger values of <span
class="math inline">\(d_k\)</span> <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.03906">(cite)</a>. We suspect that for
large values of <span class="math inline">\(d_k\)</span>, the dot
products grow large in magnitude, pushing the softmax function into
regions where it has extremely small gradients (To illustrate why the
dot products get large, assume that the components of <span
class="math inline">\(q\)</span> and <span
class="math inline">\(k\)</span> are independent random variables with
mean <span class="math inline">\(0\)</span> and variance <span
class="math inline">\(1\)</span>. Then their dot product, <span
class="math inline">\(q \cdot k = \sum_{i=1}^{d_k} q_ik_i\)</span>, has
mean <span class="math inline">\(0\)</span> and variance <span
class="math inline">\(d_k\)</span>.). To counteract this effect, we
scale the dot products by <span
class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>.</p>
<blockquote>
<p>翻译</p>
</blockquote>
<p>最常用的两个 attention 是
<code>additive attention</code>和<code>dot-product(multiplicative) attention</code>。除了使用放缩因子
<span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>
之外，本文中使用的算法与<code>Dot-product attention</code>算法完全一致。<code>Additive attention</code>
使用一个只有一层的前向反馈网络计算<code>compatibility function</code>。然而两者在理论复杂度上是相似的，实际上，<code>dot-product attention</code>更快些，且空间效率更高些，这是因为它可以使用高度优化的矩阵乘法代码来实现。</p>
<p>而对于<span
class="math inline">\(d_k\)</span>的较小值，这两种机制的表现相似，但加法注意力比（<span
class="math inline">\(d_k\)</span>值较大而没有缩放的）点积注意力更胜一筹。我们怀疑，对于大的<span
class="math inline">\(d_k\)</span>值，点积的量级会变大，导致softmax函数进入到了梯度极小的区域。</p>
<p>为了说明点积变大的原因。假设<span
class="math inline">\(q\)</span>和<span
class="math inline">\(k\)</span>的组成部分是独立的随机变量的组成部分是独立的随机变量，其均值为
<span class="math inline">\(0\)</span>，方差为 <span
class="math inline">\(1\)</span>。 那么它们的点积, <span
class="math inline">\(q \cdot k = \sum_{i=1}^{d_k}
q_ik_i\)</span>，其均值为<span
class="math inline">\(0\)</span>，方差为<span
class="math inline">\(d_k\)</span>。</p>
<p>为了抵消这种影响，我们将点积的比例定为 <span
class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>。</p>
<blockquote>
<p>关于 dot-product attention 为什么要 scale <span
class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span></p>
</blockquote>
<p>李沐老师在【Transformer论文逐段精读】中对这部分的解读摘录如下：</p>
<p>当<span
class="math inline">\(d_k\)</span>不是很大的时候，除不除都没关系。但是当<span
class="math inline">\(d_k\)</span>很大的时候，也就是向量较长，内积可能非常大。当内积值较大时，差距也会较大。</p>
<p>而又因为softmax的操作趋向于让大的更大，小的更小，也就是置信的地方更接近1，不置信的地方更接近0，由此得到收敛，因此梯度会很小甚至梯度消失，导致模型会很快“跑不动”，失去了学习的作用。</p>
<p><img
src="https://github.com/harvardnlp/annotated-transformer/blob/master/images/ModalNet-20.png?raw=1" /></p>
<p>Multi-head attention allows the model to jointly attend to
information from different representation subspaces at different
positions. With a single attention head, averaging inhibits this.</p>
<p><span class="math display">\[
\mathrm{MultiHead}(Q, K, V) =
    \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \\
    \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i,
VW^V_i)
\]</span></p>
<p>Where the projections are parameter matrices <span
class="math inline">\(W^Q_i \in \mathbb{R}^{d_{\text{model}} \times
d_k}\)</span>, <span class="math inline">\(W^K_i \in
\mathbb{R}^{d_{\text{model}} \times d_k}\)</span>, <span
class="math inline">\(W^V_i \in \mathbb{R}^{d_{\text{model}} \times
d_v}\)</span> and <span class="math inline">\(W^O \in \mathbb{R}^{hd_v
\times d_{\text{model}}}\)</span>.</p>
<p>In this work we employ <span class="math inline">\(h=8\)</span>
parallel attention layers, or heads. For each of these we use <span
class="math inline">\(d_k=d_v=d_{\text{model}}/h=64\)</span>. Due to the
reduced dimension of each head, the total computational cost is similar
to that of single-head attention with full dimensionality.</p>
<blockquote>
<p>翻译</p>
</blockquote>
<p>Multi-head attention
允许模型从不同位置的不同表示空间中联合利用信息。如果只是单头attention
，那么平均将会抑制这种状况。</p>
<p><span class="math display">\[
\mathrm{MultiHead}(Q, K, V) =
    \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \\
    \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i,
VW^V_i)
\]</span></p>
<p>在这个文本中，投影矩阵分别为 <span class="math inline">\(W^Q_i \in
\mathbb{R}^{d_{\text{model}} \times d_k}\)</span>、<span
class="math inline">\(W^K_i \in \mathbb{R}^{d_{\text{model}} \times
d_k}\)</span>、<span class="math inline">\(W^V_i \in
\mathbb{R}^{d_{\text{model}} \times d_v}\)</span> 和 <span
class="math inline">\(W^O \in \mathbb{R}^{hd_v \times
d_{\text{model}}}\)</span>，其中 <span
class="math inline">\(d_{\text{model}}\)</span> 表示模型的维度，<span
class="math inline">\(d_k\)</span> 和 <span
class="math inline">\(d_v\)</span>
分别表示注意力机制中的查询、键和值向量的维度，<span
class="math inline">\(h\)</span> 表示注意力头的数量。</p>
<p>在本论文中，我们使用<code>h = 8</code>
个并行的注意力层，或者说注意力头。其中每一个注意力头，我们使用 <span
class="math inline">\(d_k=d_v=d_{\text{model}}/h=64\)</span>。由于减少了每个头的维度，总的计算损失与单个完全维度的attention
是相似的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module): </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>): </span><br><span class="line">    <span class="comment"># h=8, d_model=512</span></span><br><span class="line">    <span class="string">&quot;Take in model size and number of heads.&quot;</span> </span><br><span class="line">    <span class="built_in">super</span>(MultiHeadedAttention, self).__init__() </span><br><span class="line">    <span class="keyword">assert</span> d_model % h == <span class="number">0</span> <span class="comment"># We assume d_v always equals d_k 512%8=0</span></span><br><span class="line">    self.d_k = d_model // h <span class="comment"># d_k=512//8=64</span></span><br><span class="line">    self.h = h <span class="comment"># h=8</span></span><br><span class="line">    self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>) </span><br><span class="line">    <span class="comment"># 定义四个Linear networks, 每个的大小是(512, 512)的，</span></span><br><span class="line">    <span class="comment"># 每个Linear network里面有两类可训练参数，Weights，</span></span><br><span class="line">    <span class="comment"># 其大小为512*512，以及biases，其大小为d_model=512。</span></span><br><span class="line"></span><br><span class="line">    self.attn = <span class="literal">None</span> </span><br><span class="line">    self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>): </span><br><span class="line">   <span class="comment"># 注意，输入query的形状类似于(30, 10, 512)，</span></span><br><span class="line">   <span class="comment"># key.size() ~ (30, 11, 512), </span></span><br><span class="line">   <span class="comment"># 以及value.size() ~ (30, 11, 512)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># Same mask applied to all h heads. </span></span><br><span class="line">      mask = mask.unsqueeze(<span class="number">1</span>) <span class="comment"># mask后续细细分解。</span></span><br><span class="line">    nbatches = query.size(<span class="number">0</span>) <span class="comment">#e.g., nbatches=30</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1) Do all the linear projections in batch from </span></span><br><span class="line">    <span class="comment"># d_model =&gt; h x d_k </span></span><br><span class="line">    query, key, value = [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k)</span><br><span class="line">      .transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="keyword">for</span> l, x <span class="keyword">in</span> </span><br><span class="line">      <span class="built_in">zip</span>(self.linears, (query, key, value))] </span><br><span class="line">      <span class="comment"># 这里是形成Q、K、V矩阵的过程，也是前三个Linear Networks的具体应用，</span></span><br><span class="line">      <span class="comment"># 例如query=(30, 10, 512) -&gt; Linear network -&gt; (30, 10, 512) </span></span><br><span class="line">      <span class="comment"># -&gt; view -&gt; (30, 10, 8, 64) -&gt; transpose(1,2) -&gt; (30, 8, 10, 64)</span></span><br><span class="line">      <span class="comment"># 对于key和value，也是类似的，从(30, 11, 512) -&gt; (30, 8, 11, 64)。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">    x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">      dropout=self.dropout) </span><br><span class="line">      <span class="comment"># 调用上面定义好的attention函数，输出的x形状为(30, 8, 10, 64)；</span></span><br><span class="line">      <span class="comment"># attn的形状为(30, 8, 10=target.seq.len, 11=src.seq.len)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">    x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, -<span class="number">1</span>, self.h * self.d_k) </span><br><span class="line">      <span class="comment"># x ~ (30, 8, 10, 64) -&gt; transpose(1,2) -&gt; </span></span><br><span class="line">      <span class="comment"># (30, 10, 8, 64) -&gt; contiguous() and view -&gt; </span></span><br><span class="line">      <span class="comment"># (30, 10, 8*64) = (30, 10, 512)</span></span><br><span class="line">    <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x) </span><br><span class="line">      <span class="comment"># 执行第四个Linear network，把(30, 10, 512)经过一次linear network，</span></span><br><span class="line">      <span class="comment"># 得到(30, 10, 512).</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">query, key, value = [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) </span><br><span class="line"><span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))] </span><br></pre></td></tr></table></figure>
<p>该代码的意思是将传入的Q、K、V三维矩阵经过一层全连接层后，重塑为四维矩阵，并且四维矩阵的第二、三维转置。<strong>torch.view函数的功能和numpy.reshape的功能差不多。</strong>下图可以很好的解释数据结构：</p>
<p><img
src="https://img-blog.csdnimg.cn/img_convert/6caa97d4f13601dc7a8f2f6ccb96181c.png" /></p>
<p>如图所示，最后得到的结果是个四维矩阵，维度依次是B、h、F、d_k。</p>
<ul>
<li>B是Batch-size</li>
<li>h是多头自注意力机制中的头数</li>
<li>F是一个样本中的字符数，可以理解为一个句子中的单词个数</li>
<li>d_k是每个字符对应的embedding长度。</li>
</ul>
<p>理解了这个类，Transformer的精髓也就理解得差不多了</p>
<h3 id="applications-of-attention-in-our-model">Applications of
Attention in our Model</h3>
<p>The Transformer uses multi-head attention in three different ways: 1)
In "encoder-decoder attention" layers, the queries come from the
previous decoder layer, and the memory keys and values come from the
output of the encoder. This allows every position in the decoder to
attend over all positions in the input sequence. This mimics the typical
encoder-decoder attention mechanisms in sequence-to-sequence models such
as <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1609.08144">(cite)</a>.</p>
<ol start="2" type="1">
<li><p>The encoder contains self-attention layers. In a self-attention
layer all of the keys, values and queries come from the same place, in
this case, the output of the previous layer in the encoder. Each
position in the encoder can attend to all positions in the previous
layer of the encoder.</p></li>
<li><p>Similarly, self-attention layers in the decoder allow each
position in the decoder to attend to all positions in the decoder up to
and including that position. We need to prevent leftward information
flow in the decoder to preserve the auto-regressive property. We
implement this inside of scaled dot-product attention by masking out
(setting to <span class="math inline">\(-\infty\)</span>) all values in
the input of the softmax which correspond to illegal
connections.</p></li>
</ol>
<h2 id="position-wise-feed-forward-networks">Position-wise Feed-Forward
Networks</h2>
<p>In addition to attention sub-layers, each of the layers in our
encoder and decoder contains a fully connected feed-forward network,
which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in
between.</p>
<p><span class="math display">\[\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2
+ b_2\]</span></p>
<p>While the linear transformations are the same across different
positions, they use different parameters from layer to layer. Another
way of describing this is as two convolutions with kernel size 1. The
dimensionality of input and output is <span
class="math inline">\(d_{\text{model}}=512\)</span>, and the inner-layer
has dimensionality <span class="math inline">\(d_{ff}=2048\)</span>.</p>
<blockquote>
<p>翻译</p>
</blockquote>
<p>除了 <code>attention sub-layers</code> 【这里的attention
sub-layers应该是一个名词】之外，encoder 和 decoder
的每层包含了一个全连接前馈网络，它分别相同地应用到每一个位置。这包含一个用<code>ReLu</code>做连接的两个线性转换操作。</p>
<p><span class="math display">\[\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2
+ b_2\]</span></p>
<p>尽管不同的位置有着相同的线性转换，但是它们使用不同的参数从一层到另一层。另一种描述这个的方法是：可以将这个看成是两个卷积核大小为1的卷积。输入和输出的维度都是
<span
class="math inline">\(d_{\text{model}}=512\)</span>，同时，内部层维度是
<span class="math inline">\(d_{ff}=2048\)</span></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implements FFN equation.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="comment"># d_model = 512</span></span><br><span class="line">        <span class="comment"># d_ff = 2048 = 512*4</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        <span class="comment"># 构建第一个全连接层，(512, 2048)，其中有两种可训练参数：</span></span><br><span class="line">        <span class="comment"># weights矩阵，(512, 2048)，以及</span></span><br><span class="line">        <span class="comment"># biases偏移向量, (2048)</span></span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        <span class="comment"># 构建第二个全连接层, (2048, 512)，两种可训练参数：</span></span><br><span class="line">        <span class="comment"># weights矩阵，(2048, 512)，以及</span></span><br><span class="line">        <span class="comment"># biases偏移向量, (512)</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x shape = (batch.size, sequence.len, 512)</span></span><br><span class="line">        <span class="comment"># 例如, (30, 10, 512)</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br><span class="line">        <span class="comment"># x (30, 10, 512) -&gt; self.w_1 -&gt; (30, 10, 2048)</span></span><br><span class="line">        <span class="comment"># -&gt; relu -&gt; (30, 10, 2048) </span></span><br><span class="line">        <span class="comment"># -&gt; dropout -&gt; (30, 10, 2048)</span></span><br><span class="line">        <span class="comment"># -&gt; self.w_2 -&gt; (30, 10, 512)是输出的shape</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于FFN的理解</p>
</blockquote>
<p>实际上这就是一个MLP，但它是对每个词单独作用，并且保证对不同词作用的MLP参数相同。因此FFN实际就是一个线性层加上一个ReLU再加上一个线性层。<br />
单隐藏层的 MLP，中间扩维到4倍 2048，最后投影回到 512
维度大小，便于残差连接。</p>
<p><img src="https://img-blog.csdnimg.cn/2b45b8a6dbd44d29805b8b42268c2d93.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5omT552A54Gv56y85pG46buR,size_20,color_FFFFFF,t_70,g_se,x_16" width = "50%" /></p>
<p><img src="https://pic3.zhimg.com/80/v2-c9259cedcf5f77e244cb4ac946108cc6_1440w.webp" width = "50%" /></p>
<h2 id="embeddings-and-softmax">Embeddings and Softmax</h2>
<p>Similarly to other sequence transduction models, we use learned
embeddings to convert the input tokens and output tokens to vectors of
dimension <span class="math inline">\(d_{\text{model}}\)</span>. We also
use the usual learned linear transformation and softmax function to
convert the decoder output to predicted next-token probabilities. In our
model, we share the same weight matrix between the two embedding layers
and the pre-softmax linear transformation, similar to <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.05859">(cite)</a>. In the embedding
layers, we multiply those weights by <span
class="math inline">\(\sqrt{d_{\text{model}}}\)</span>.</p>
<blockquote>
<p>翻译</p>
</blockquote>
<p>与序列推导模型相似，我们使用embeddings 去将 input tokens和 output
tokens转换成维度是 <span class="math inline">\({d_{model}}\)</span>
的向量，我们同时使用通常学习的线性转换和softmax 函数，用于将decoder
输出转换成一个可预测的接下来单词的概率。在我们的模型中：在两个embedding
layers 和 pre-softmax 线性转换中共用相同的权重矩阵。在embedding
layers，我们将这些权重乘以<span class="math inline">\(\sqrt
{d_{model}}\)</span></p>
<blockquote>
<p>关于共享两个 Embedding 和 pre-softmax 权重矩阵</p>
</blockquote>
<p>换句话说，同一组权重被用来将源语言和目标语言的词转换为连续矢量表示，以及将连续表示转换为目标语言词的预测。这种共享权重的做法有助于模型更有效地学习，因为它可以在源语言和目标语言中使用共同的模式。</p>
<p>共享权重是否有用，取决于源语言和目标语言之间的关系。如果这些语言有类似的结构和许多共同的词根，就像你提到的欧洲语系一样（本文用到了英法、英德翻译），那么共享权重就会有好处。然而，如果这些语言非常不同，就像中文和英文，那么共享权重可能没有用，甚至可能损害性能。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,d_model,vocab</span>):</span><br><span class="line">    <span class="comment"># d_model=512, vocab=当前语言的词表大小</span></span><br><span class="line">    <span class="built_in">super</span>(Embeddings,self).__init__()</span><br><span class="line">    self.lut=nn.Embedding(vocab,d_model) </span><br><span class="line">    <span class="comment"># one-hot转词嵌入，这里有一个待训练的矩阵lut，大小是vocab*d_model</span></span><br><span class="line">    self.d_model=d_model <span class="comment"># 512</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>): </span><br><span class="line">     <span class="comment"># x ~ (batch.size, sequence.length, one-hot), </span></span><br><span class="line">     <span class="comment"># one-hot大小=vocab，当前语言的词表大小</span></span><br><span class="line"></span><br><span class="line">     <span class="keyword">return</span> self.lut(x)*math.sqrt(self.d_model) </span><br><span class="line">     <span class="comment"># 输出的tensor大小类似于(batch.size, sequence.length, 512)</span></span><br><span class="line">     <span class="comment"># 每个元素主动乘以sqrt(512)=22.6缩放比例</span></span><br></pre></td></tr></table></figure>
<p>Embeddings类的forward函数中的<code>math.sqrt(d_model)</code>的乘法是一种归一化技术，用于缩放embedding。这个缩放系数用于确保两个embedding之间的点积与模型内的其他点积具有相似的大小，这有助于稳定训练过程，防止梯度变得过大或过小。</p>
<p>在实践中，比例因子的具体数值并不关键，不同的模型和实验中也使用了不同的数值。选择<code>math.sqrt(d_model)</code>作为比例因子是基于变换器的模型领域中常见的，它被用来确保嵌入中的值与模型的其他部分处于相似的尺度。然而，如果需要的话，你可以使用一个不同的比例系数，或者甚至完全不使用比例系数，这取决于你的具体使用情况和要求。</p>
<h2 id="positional-encoding">Positional Encoding</h2>
<p>Since our model contains no recurrence and no convolution, in order
for the model to make use of the order of the sequence, we must inject
some information about the relative or absolute position of the tokens
in the sequence. To this end, we add "positional encodings" to the input
embeddings at the bottoms of the encoder and decoder stacks. The
positional encodings have the same dimension <span
class="math inline">\(d_{\text{model}}\)</span> as the embeddings, so
that the two can be summed. There are many choices of positional
encodings, learned and fixed <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.03122.pdf">(cite)</a>.</p>
<p>In this work, we use sine and cosine functions of different
frequencies:</p>
<p><span class="math display">\[PE_{(pos,2i)} = \sin(pos /
10000^{2i/d_{\text{model}}})\]</span></p>
<p><span class="math display">\[PE_{(pos,2i+1)} = \cos(pos /
10000^{2i/d_{\text{model}}})\]</span></p>
<p>where <span class="math inline">\(pos\)</span> is the position and
<span class="math inline">\(i\)</span> is the dimension. That is, each
dimension of the positional encoding corresponds to a sinusoid. The
wavelengths form a geometric progression from <span
class="math inline">\(2\pi\)</span> to <span class="math inline">\(10000
\cdot 2\pi\)</span>. We chose this function because we hypothesized it
would allow the model to easily learn to attend by relative positions,
since for any fixed offset <span class="math inline">\(k\)</span>, <span
class="math inline">\(PE_{pos+k}\)</span> can be represented as a linear
function of <span class="math inline">\(PE_{pos}\)</span>.</p>
<p>In addition, we apply dropout to the sums of the embeddings and the
positional encodings in both the encoder and decoder stacks. For the
base model, we use a rate of <span
class="math inline">\(P_{drop}=0.1\)</span>.</p>
<p>因为我们的模型没有包含RNN和CNN，为了让模型充分利用序列的顺序信息，我们必须获取一些信息关于tokens
在序列中相对或者绝对的位置。为了这个目的，我们在encoder 和 decoder
栈的底部 加上了<code>positional encodings</code>到 input
embeddings中。这个<code>positional embedding</code> 与
<code>embedding</code>有相同的维度 <span
class="math inline">\(d_{model}\)</span>。有许多关于<code>positional ecodings</code>的选择。</p>
<p>在本论文中，我们使用不同频率的sine 和 cosine 函数。公式如下：<br />
<span class="math display">\[
\begin{aligned} PE_{pos,2i} = sin( \frac{pos}{10000^{2i/d_{model}}}) \\
PE_{pos,2i+1} = cos(\frac{pos}{10000^{2i/d_{model}}}) \end{aligned}
\]</span></p>
<p>其中<code>pos</code>
是位置，<code>i</code>是维度。也就是说：每个位置编码的维度对应一个正弦曲线。波长形成了一个几何级数从2π
到 10000*2π。
选择这个函数的原因是：我们假设它让我们的模型容易学习到位置信息，因为对任何一个固定的偏移k，
<span class="math inline">\(PE_{pos+k}\)</span> 可以代表成一个 <span
class="math inline">\(PE_{pos}\)</span> 的线性函数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module): </span><br><span class="line">  <span class="string">&quot;Implement the PE function.&quot;</span> </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>): </span><br><span class="line">    <span class="comment"># d_model=512,dropout=0.1,</span></span><br><span class="line">    <span class="comment"># max_len=5000，代表事先准备好长度为5000的序列的位置编码，其实没必要，一般100或者200足够了。</span></span><br><span class="line">    <span class="built_in">super</span>(PositionalEncoding, self).__init__() </span><br><span class="line">    self.dropout = nn.Dropout(p=dropout) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the positional encodings once in log space. </span></span><br><span class="line">    pe = torch.zeros(max_len, d_model) </span><br><span class="line">    <span class="comment"># (5000,512)矩阵，保持每个位置的位置编码，一共5000个位置，</span></span><br><span class="line">    <span class="comment"># 每个位置用一个512维度向量来表示其位置编码</span></span><br><span class="line">    position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>) </span><br><span class="line">    <span class="comment"># (5000) -&gt; (5000,1)</span></span><br><span class="line">    div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * </span><br><span class="line">      -(math.log(<span class="number">10000.0</span>) / d_model)) </span><br><span class="line">    <span class="comment"># (0,2,…, 4998)一共准备2500个值，供sin, cos调用</span></span><br><span class="line">    <span class="comment"># 公式做了变形，详见下方说明</span></span><br><span class="line">    pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment"># 偶数下标的位置</span></span><br><span class="line">    pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment"># 奇数下标的位置</span></span><br><span class="line">    pe = pe.unsqueeze(<span class="number">0</span>) </span><br><span class="line">    <span class="comment"># (5000, 512) -&gt; (1, 5000, 512) 为batch.size留出位置</span></span><br><span class="line">    self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe) </span><br><span class="line">    <span class="comment"># The pe tensor is then registered as a buffer, </span></span><br><span class="line">    <span class="comment"># which means it will not be part of the model&#x27;s parameters </span></span><br><span class="line">    <span class="comment"># and will not require gradients during the backward pass.</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">    x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>) </span><br><span class="line">    <span class="comment"># 接受 Embeddings 的词嵌入结果x，然后把自己的位置编码pe缩放成x.size(1)的长度，(封装成torch的Variable)，加上去</span></span><br><span class="line">    <span class="comment"># 由于self.pe[:, :x.size(1)]不是一个可学习的参数，所以设置requires_grad=False，表示不需要梯度。</span></span><br><span class="line">    <span class="comment"># 例如，假设x是(30,10,512)的一个tensor，30是batch.size, 10是该batch的序列长度, 512是每个词的词嵌入向量；</span></span><br><span class="line">    <span class="comment"># 则该行代码的第二项是(1, min(10, 5000), 512)=(1,10,512)，</span></span><br><span class="line">    <span class="comment"># 在具体相加的时候，会扩展(1,10,512)为(30,10,512)，保证一个batch中的30个序列，都使用（叠加）一样的位置编码。</span></span><br><span class="line">    <span class="keyword">return</span> self.dropout(x) <span class="comment"># 增加一次dropout操作</span></span><br><span class="line"><span class="comment"># 注意，位置编码不会更新，是写死的，所以这个class里面没有可训练的参数。</span></span><br></pre></td></tr></table></figure>
<p>为了计算这个公式，上面的公式做了一系列变形，因此代码可能第一眼看上去和公式不像，下面以<span
class="math inline">\(2i\)</span>（偶数情况）为例子说明：</p>
<p><img
src="https://pic3.zhimg.com/v2-acd068d4806fbbe420f81210b495a432_b.jpg" /></p>
<p><img
src="https://pic3.zhimg.com/80/v2-acd068d4806fbbe420f81210b495a432_1440w.webp" /></p>
<p><strong>注意，位置编码不会更新，是写死的，所以这个class里面没有可训练的参数。</strong></p>
<p>有了 <code>Embeddings</code> 和 <code>PositionalEncoding</code></p>
<p>在具体使用的时候，是通过torch.nn.Sequential来把他们两个串起来的：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># in Full Model</span></span><br><span class="line"><span class="comment"># nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span></span><br><span class="line"><span class="comment"># nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>Below the positional encoding will add in a sine wave based on
position. The frequency and offset of the wave is different for each
dimension.</p>
</blockquote>
<blockquote>
<p>关于位置编码的可视化</p>
</blockquote>
<p>下面的位置编码将添加一个基于位置的正弦波。波的频率和偏移对于每个维度都是不同的。（最多）</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">example_positional</span>():</span><br><span class="line">    pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>) </span><br><span class="line">    <span class="comment"># d_model=20, dropout=0,max_len=5000</span></span><br><span class="line">    y = pe.forward(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))</span><br><span class="line">    <span class="comment"># x是(1, 100, 20)的一个tensor，1是batch.size, 100是该batch的序列长度, 20是每个词的词嵌入向量d_model；</span></span><br><span class="line">    <span class="comment"># y (1, 100, 20),从d_model中选取dim</span></span><br><span class="line">    data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;embedding&quot;</span>: y[<span class="number">0</span>, :, dim],</span><br><span class="line">                    <span class="string">&quot;dimension&quot;</span>: dim,</span><br><span class="line">                    <span class="string">&quot;position&quot;</span>: <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">100</span>)),</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> dim <span class="keyword">in</span> [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">800</span>)</span><br><span class="line">        .encode(x=<span class="string">&quot;position&quot;</span>, y=<span class="string">&quot;embedding&quot;</span>, color=<span class="string">&quot;dimension:N&quot;</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(example_positional)</span><br></pre></td></tr></table></figure>
<p>We also experimented with using learned positional embeddings <a
target="_blank" rel="noopener" href="https://arxiv.org/pdf/1705.03122.pdf">(cite)</a> instead, and
found that the two versions produced nearly identical results. We chose
the sinusoidal version because it may allow the model to extrapolate to
sequence lengths longer than the ones encountered during training.</p>
<blockquote>
<p>翻译</p>
</blockquote>
<p>我们还尝试使用学习到的位置嵌入（另一种方式得到的positional
embeddding）来代替，发现这两个版本产生了几乎相同的结果。我们选择正弦版本是因为它可以让模型推断出比训练期间遇到的序列长度更长的序列长度。</p>
<blockquote>
<p>关于positional embedding</p>
</blockquote>
<p>位置嵌入是一个符号在序列中的位置的连续表示。它们被添加到符号的连续表示中（即词嵌入），以告知模型每个符号在输入序列中的相对或绝对位置。</p>
<p>有两种方法可以产生位置嵌入：学习型和正弦型。学习型位置嵌入是由神经网络生成的，并与模型的其他部分一起训练。正弦位置嵌入是使用一个数学函数生成的，如正弦或余弦，它将序列中的每个位置映射到一个独特的连续表示。</p>
<p>该论文的作者发现，这两个版本的位置嵌入在他们的实验中产生了几乎相同的结果。然而，他们选择了正弦波版本，因为它可能允许模型推断出比训练期间遇到的序列长度更长的序列。这是因为正弦函数是周期性的，可以无限期地重复模式，而学习的位置嵌入是由一个固定大小的神经网络产生的，可能不能很好地推广到更长的序列。</p>
<h2 id="full-model">Full Model</h2>
<blockquote>
<p>Here we define a function from hyperparameters to a full model.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_model</span>(<span class="params">src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span><br><span class="line"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;Helper: Construct a model from hyperparameters.&quot;</span></span><br><span class="line">    <span class="comment"># src_vocab = 源语言词表大小</span></span><br><span class="line">    <span class="comment"># tgt_vocab = 目标语言词表大小</span></span><br><span class="line">    </span><br><span class="line">    c = copy.deepcopy <span class="comment"># 对象的深度copy/clone</span></span><br><span class="line">    attn = MultiHeadedAttention(h, d_model) <span class="comment"># 8, 512</span></span><br><span class="line">    <span class="comment"># 构造一个MultiHeadAttention对象</span></span><br><span class="line">    </span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    <span class="comment"># 512, 2048, 0.1</span></span><br><span class="line">    <span class="comment"># 构造一个feed forward对象</span></span><br><span class="line"></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    <span class="comment"># 位置编码</span></span><br><span class="line"></span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model <span class="comment"># EncoderDecoder 对象</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于参数初始化</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">  <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">    nn.init.xavier_uniform(p)</span><br></pre></td></tr></table></figure>
<p>这段代码的作用是初始化Transformer模型的参数。"Glorot /
fan_avg"是一种常用的初始化方法，也被称为"Xavier
initialization"。该方法的目的是使得模型的参数有合适的初始值，以避免梯度消失和爆炸的问题。</p>
<p>因此，这段代码的重要性在于它为Transformer模型的学习提供了合适的初始条件，从而提高模型的学习效率和精度。在机器学习领域，初始参数的选择对于模型的效果有很大影响，因此该代码是很重要的。</p>
<h2 id="inference">Inference:</h2>
<blockquote>
<p>Here we make a forward step to generate a prediction of the model. We
try to use our transformer to memorize the input. As you will see the
output is randomly generated due to the fact that the model is not
trained yet. In the next tutorial we will build the training function
and try to train our model to memorize the numbers from 1 to 10.</p>
</blockquote>
<blockquote>
<p>在这里，我们向前迈出一步以生成模型的预测。我们尝试使用我们的转换器来记住输入。正如您将看到的那样，由于模型尚未训练，输出是随机生成的。在下一个教程中，我们将构建训练函数并尝试训练我们的模型来记住从
1 到 10 的数字。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inference_test</span>():</span><br><span class="line">    test_model = make_model(<span class="number">11</span>, <span class="number">11</span>, <span class="number">2</span>)</span><br><span class="line">    test_model.<span class="built_in">eval</span>()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    memory = test_model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">        out = test_model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = test_model.generator(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.empty(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Example Untrained Model Prediction:&quot;</span>, ys)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_tests</span>():</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        inference_test()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(run_tests)</span><br></pre></td></tr></table></figure>
<h1 id="第二部分-模型训练">第二部分: 模型训练</h1>
<h1 id="训练">训练</h1>
<p>This section describes the training regime （训练机制） for our
models.</p>
<blockquote>
<p>We stop for a quick interlude to introduce some of the tools needed
to train a standard encoder decoder model. First we define a batch
object that holds the src and target sentences for training, as well as
constructing the masks.</p>
</blockquote>
<blockquote>
<p>我们暂时停下来介绍一些训练标准编码器解码器模型所需的工具。首先，我们定义一个批处理对象，其中包含用于训练的
src 和目标句子，以及构建掩码。</p>
</blockquote>
<h2 id="batches-and-masking">Batches and Masking</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Batch</span>:</span><br><span class="line">    <span class="string">&quot;Object for holding a batch of data with mask during training.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, src, trg=<span class="literal">None</span>, pad=<span class="number">0</span></span>):</span><br><span class="line">        <span class="comment"># src: 源语言序列，(batch.size, src.seq.len)</span></span><br><span class="line">        <span class="comment"># 二维tensor，第一维度是batch.size；第二个维度是源语言句子的长度</span></span><br><span class="line">        <span class="comment"># 例如：[ [2,1,3,4], [2,3,1,4] ]这样的二行四列的，</span></span><br><span class="line">        <span class="comment"># 1-4代表每个单词word的id</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># trg: 目标语言序列，默认为空，其shape和src类似</span></span><br><span class="line">        <span class="comment"># (batch.size, trg.seq.len)，</span></span><br><span class="line">        <span class="comment"># 二维tensor，第一维度是batch.size；第二个维度是目标语言句子的长度</span></span><br><span class="line">        <span class="comment"># 例如trg=[ [2,1,3,4], [2,3,1,4] ] for a &quot;copy network&quot;</span></span><br><span class="line">        <span class="comment"># (输出序列和输入序列完全相同）</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># pad: 源语言和目标语言统一使用的 位置填充符号，&#x27;&lt;blank&gt;&#x27;</span></span><br><span class="line">        <span class="comment"># 所对应的id，这里默认为0</span></span><br><span class="line">        <span class="comment"># 例如，如果一个source sequence，长度不到4，则在右边补0</span></span><br><span class="line">        <span class="comment"># [1,2] -&gt; [1,2,0,0]</span></span><br><span class="line">        </span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(-<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># src = (batch.size, seq.len) -&gt; != pad -&gt; </span></span><br><span class="line">        <span class="comment"># (batch.size, seq.len) -&gt; usnqueeze -&gt;</span></span><br><span class="line">        <span class="comment"># (batch.size, 1, seq.len) 相当于在倒数第二个维度扩展</span></span><br><span class="line">        <span class="comment"># e.g., src=[ [2,1,3,4], [2,3,1,4] ]对应的是</span></span><br><span class="line">        <span class="comment"># src_mask=[ [[1,1,1,1], [1,1,1,1]] ]</span></span><br><span class="line">        <span class="keyword">if</span> trg <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.trg = trg[:, :-<span class="number">1</span>] <span class="comment"># 重要</span></span><br><span class="line">            <span class="comment"># trg 相当于目标序列的前N-1个单词的序列</span></span><br><span class="line">            <span class="comment">#（去掉了最后一个词）</span></span><br><span class="line">            self.trg_y = trg[:, <span class="number">1</span>:]</span><br><span class="line">            <span class="comment"># trg_y 相当于目标序列的后N-1个单词的序列</span></span><br><span class="line">            <span class="comment"># (去掉了第一个词&lt;start&gt;）</span></span><br><span class="line">            <span class="comment"># 目的是(src + trg) 来预测出(trg_y)，</span></span><br><span class="line">            <span class="comment"># 这个在illustrated transformer中详细图示过。</span></span><br><span class="line">            self.trg_mask = \</span><br><span class="line">                self.make_std_mask(self.trg, pad)</span><br><span class="line">            self.ntokens = (self.trg_y != pad).data.<span class="built_in">sum</span>()</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_std_mask</span>(<span class="params">tgt, pad</span>):</span><br><span class="line">        <span class="string">&quot;Create a mask to hide padding and future words.&quot;</span></span><br><span class="line">        <span class="comment"># 这里的tgt类似于：</span></span><br><span class="line">        <span class="comment"># [ [2,1,3], [2,3,1] ] （最初的输入目标序列，分别去掉了最后一个词</span></span><br><span class="line">        <span class="comment"># pad=0, &#x27;&lt;blank&gt;&#x27;的id编号</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(-<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 得到的tgt_mask类似于</span></span><br><span class="line">        <span class="comment"># tgt_mask = tensor([[[1, 1, 1]],[[1, 1, 1]]], dtype=torch.uint8)</span></span><br><span class="line">        <span class="comment"># shape=(2,1,3)</span></span><br><span class="line">        tgt_mask = tgt_mask &amp; Variable(</span><br><span class="line">            subsequent_mask(tgt.size(-<span class="number">1</span>)).type_as(tgt_mask.data))</span><br><span class="line">        <span class="comment"># 先看subsequent_mask, 其输入的是tgt.size(-1)=3</span></span><br><span class="line">        <span class="comment"># 这个函数的输出为= tensor([[[1, 0, 0],</span></span><br><span class="line">        <span class="comment"># [1, 1, 0],</span></span><br><span class="line">        <span class="comment"># [1, 1, 1]]], dtype=torch.uint8)</span></span><br><span class="line">        <span class="comment"># type_as 把这个tensor转成tgt_mask.data的type(也是torch.uint8)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 这样的话，&amp;的两边的tensor.shape分别是(2,1,3), (1,3,3);</span></span><br><span class="line">        <span class="comment"># tgt_mask = tensor([[[1, 1, 1]],[[1, 1, 1]]], dtype=torch.uint8)</span></span><br><span class="line">        <span class="comment"># and</span></span><br><span class="line">        <span class="comment"># tensor([[[1, 0, 0], [1, 1, 0], [1, 1, 1]]], dtype=torch.uint8)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (2,3,3)就是得到的tensor</span></span><br><span class="line">        <span class="comment"># tgt_mask.data = tensor(</span></span><br><span class="line">        <span class="comment"># [[1, 0, 0],</span></span><br><span class="line">        <span class="comment"># [1, 1, 0],</span></span><br><span class="line">        <span class="comment"># [1, 1, 1]]], dtype=torch.uint8)</span></span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于"Batch"类</p>
</blockquote>
<p>用于存储和处理训练数据</p>
<p><code>__init__</code>方法初始化了Batch对象的各个成员变量：</p>
<ul>
<li>src：源语言序列</li>
<li>src_mask：源语言序列的掩码，用于表示序列中的某些位置是否有效</li>
<li>trg：目标语言序列（可选）</li>
<li>trg_y：目标语言序列的标签，即目标语言序列的下一个词</li>
<li>trg_mask：目标语言序列的掩码，表示该序列中哪些位置是有效的，用于注意力机制</li>
<li>ntokens：表示目标语言序列中有效词的数量</li>
</ul>
<p><code>make_std_mask</code>是一个静态方法，用于创建目标语言序列的mask。该方法的作用是防止注意力机制关注未来的预测词。它创建了一个mask，通过将目标语言序列中有效位置与subsequent_mask进行位运算来实现该目的。</p>
<blockquote>
<p>Next we create a generic training and scoring function to keep track
of loss. We pass in a generic loss compute function that also handles
parameter updates.</p>
</blockquote>
<blockquote>
<p>接下来我们创建一个通用的训练和评分函数来跟踪损失。我们传入了一个通用的损失计算函数，该函数也处理参数更新。</p>
</blockquote>
<h2 id="training-loop">Training Loop</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TrainState</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Track number of steps, examples, and tokens processed&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    step: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># Steps in the current epoch</span></span><br><span class="line">    accum_step: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># Number of gradient accumulation steps</span></span><br><span class="line">    samples: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># total # of examples used</span></span><br><span class="line">    tokens: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># total # of tokens processed</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_epoch</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data_iter,</span></span><br><span class="line"><span class="params">    model,</span></span><br><span class="line"><span class="params">    loss_compute,</span></span><br><span class="line"><span class="params">    optimizer,</span></span><br><span class="line"><span class="params">    scheduler,</span></span><br><span class="line"><span class="params">    mode=<span class="string">&quot;train&quot;</span>,</span></span><br><span class="line"><span class="params">    accum_iter=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">    train_state=TrainState(<span class="params"></span>),</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Train a single epoch&quot;&quot;&quot;</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    total_tokens = <span class="number">0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    tokens = <span class="number">0</span></span><br><span class="line">    n_accum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_iter):</span><br><span class="line">        out = model.forward(</span><br><span class="line">            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask</span><br><span class="line">        )</span><br><span class="line">        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)</span><br><span class="line">        <span class="comment"># loss_node = loss_node / accum_iter</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span> <span class="keyword">or</span> mode == <span class="string">&quot;train+log&quot;</span>:</span><br><span class="line">            loss_node.backward()</span><br><span class="line">            train_state.step += <span class="number">1</span></span><br><span class="line">            train_state.samples += batch.src.shape[<span class="number">0</span>]</span><br><span class="line">            train_state.tokens += batch.ntokens</span><br><span class="line">            <span class="keyword">if</span> i % accum_iter == <span class="number">0</span>:</span><br><span class="line">                optimizer.step()</span><br><span class="line">                optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line">                n_accum += <span class="number">1</span></span><br><span class="line">                train_state.accum_step += <span class="number">1</span></span><br><span class="line">            scheduler.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss</span><br><span class="line">        total_tokens += batch.ntokens</span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">40</span> == <span class="number">1</span> <span class="keyword">and</span> (mode == <span class="string">&quot;train&quot;</span> <span class="keyword">or</span> mode == <span class="string">&quot;train+log&quot;</span>):</span><br><span class="line">            lr = optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">            elapsed = time.time() - start</span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                (</span><br><span class="line">                    <span class="string">&quot;Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f &quot;</span></span><br><span class="line">                    + <span class="string">&quot;| Tokens / Sec: %7.1f | Learning Rate: %6.1e&quot;</span></span><br><span class="line">                )</span><br><span class="line">                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)</span><br><span class="line">            )</span><br><span class="line">            start = time.time()</span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">        <span class="keyword">del</span> loss</span><br><span class="line">        <span class="keyword">del</span> loss_node</span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens, train_state</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 run_epoch</p>
</blockquote>
<p>这段代码实现了单个epoch的训练。它遍历了data_iter中的所有batch，对于每个batch，它调用模型的forward方法，获得输出，然后计算loss并使用loss进行backward。在每个batch上，它使用优化器(本文中为Adam)更新模型的参数，并使用学习率调度器调整学习率。</p>
<p>需要注意的是，在训练模式下，如果批次数是指定的累积次数的倍数，则进行一次优化步骤，并将梯度归零。此外，如果指定了"train
+
log"模式，则每隔40个批次，将输出当前的学习率，损失，以及每秒处理的token数。</p>
<p>最后，该函数返回平均每个token的损失和训练状态。</p>
<h2 id="training-data-and-batching">Training Data and Batching</h2>
<p>We trained on the standard WMT 2014 English-German dataset consisting
of about 4.5 million sentence pairs. Sentences were encoded using
byte-pair encoding, which has a shared source-target vocabulary of about
37000 tokens. For English-French, we used the significantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens
into a 32000 word-piece vocabulary.</p>
<p>Sentence pairs were batched together by approximate sequence length.
Each training batch contained a set of sentence pairs containing
approximately 25000 source tokens and 25000 target tokens.</p>
<h2 id="hardware-and-schedule">Hardware and Schedule</h2>
<p>We trained our models on one machine with 8 NVIDIA P100 GPUs. For our
base models using the hyperparameters described throughout the paper,
each training step took about 0.4 seconds. We trained the base models
for a total of 100,000 steps or 12 hours. For our big models, step time
was 1.0 seconds. The big models were trained for 300,000 steps (3.5
days).</p>
<h2 id="optimizer">Optimizer</h2>
<p>We used the Adam optimizer <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1412.6980">(cite)</a> with <span
class="math inline">\(\beta_1=0.9\)</span>, <span
class="math inline">\(\beta_2=0.98\)</span> and <span
class="math inline">\(\epsilon=10^{-9}\)</span>. We varied the learning
rate over the course of training, according to the formula:</p>
<p><span class="math display">\[
lrate = d_{\text{model}}^{-0.5} \cdot
  \min({step\_num}^{-0.5},
    {step\_num} \cdot {warmup\_steps}^{-1.5})
\]</span></p>
<p>This corresponds to increasing the learning rate linearly for the
first <span class="math inline">\(warmup\_steps\)</span> training steps,
and decreasing it thereafter proportionally to the inverse square root
of the step number. We used <span
class="math inline">\(warmup\_steps=4000\)</span>.</p>
<blockquote>
<p>Note: This part is very important. Need to train with this setup of
the model.</p>
</blockquote>
<blockquote>
<p>Example of the curves of this model for different model sizes and for
optimization hyperparameters.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rate</span>(<span class="params">step, model_size, factor, warmup</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    we have to default the step to 1 for LambdaLR function</span></span><br><span class="line"><span class="string">    to avoid zero raising to negative power.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">        step = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> factor * (</span><br><span class="line">        model_size ** (-<span class="number">0.5</span>) * <span class="built_in">min</span>(step ** (-<span class="number">0.5</span>), step * warmup ** (-<span class="number">1.5</span>))</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 rate</p>
</blockquote>
<p>这段代码实现了一个学习率策略，具体来说是返回了一个学习率的值，该学习率策略被用于在训练中动态调整学习率以提高模型的性能。</p>
<p>具体来说，这个函数使用了两种不同的学习率策略。在开始的 warmup
过程中，学习率随着训练步数增加而逐渐增加。这在模型开始训练之前有助于防止在非常小的学习率下的不稳定性。在
warmup
过程结束后，学习率随着训练步数增加而逐渐减小。这有助于在训练中更稳定地对模型进行优化。</p>
<p>在该函数中，参数 model_size 是模型的大小，参数 factor
是用于调整学习率的系数，参数 warmup 是 warmup 过程的步数，参数 step
是当前训练步数。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">example_learning_schedule</span>():</span><br><span class="line">    opts = [</span><br><span class="line">        [<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>],  <span class="comment"># example 1</span></span><br><span class="line">        [<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>],  <span class="comment"># example 2</span></span><br><span class="line">        [<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>],  <span class="comment"># example 3</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    dummy_model = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    learning_rates = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># we have 3 examples in opts list.</span></span><br><span class="line">    <span class="keyword">for</span> idx, example <span class="keyword">in</span> <span class="built_in">enumerate</span>(opts):</span><br><span class="line">        <span class="comment"># run 20000 epoch for each example</span></span><br><span class="line">        optimizer = torch.optim.Adam(</span><br><span class="line">            dummy_model.parameters(), lr=<span class="number">1</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">        )</span><br><span class="line">        lr_scheduler = LambdaLR(</span><br><span class="line">            optimizer=optimizer, lr_lambda=<span class="keyword">lambda</span> step: rate(step, *example)</span><br><span class="line">        )</span><br><span class="line">        tmp = []</span><br><span class="line">        <span class="comment"># take 20K dummy training steps, save the learning rate at each step</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line">            tmp.append(optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>])</span><br><span class="line">            optimizer.step()</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line">        learning_rates.append(tmp)</span><br><span class="line"></span><br><span class="line">    learning_rates = torch.tensor(learning_rates)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Enable altair to handle more than 5000 rows</span></span><br><span class="line">    alt.data_transformers.disable_max_rows()</span><br><span class="line"></span><br><span class="line">    opts_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;Learning Rate&quot;</span>: learning_rates[warmup_idx, :],</span><br><span class="line">                    <span class="string">&quot;model_size:warmup&quot;</span>: [<span class="string">&quot;512:4000&quot;</span>, <span class="string">&quot;512:8000&quot;</span>, <span class="string">&quot;256:4000&quot;</span>][</span><br><span class="line">                        warmup_idx</span><br><span class="line">                    ],</span><br><span class="line">                    <span class="string">&quot;step&quot;</span>: <span class="built_in">range</span>(<span class="number">20000</span>),</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> warmup_idx <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(opts_data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">600</span>)</span><br><span class="line">        .encode(x=<span class="string">&quot;step&quot;</span>, y=<span class="string">&quot;Learning Rate&quot;</span>, color=<span class="string">&quot;model_size:warmup:N&quot;</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">example_learning_schedule()</span><br></pre></td></tr></table></figure>
<p>这个函数生成的图像说明的是不同的模型大小和不同的 warmup
设置对学习率变化的影响。代码中有三个 example，分别对应 model_size 为
512，256， warmup 为 4000，8000，4000。运行代码会返回一张图，横轴是
20,000
步训练中每一步，纵轴是当前的学习率，图上每条曲线代表不同的模型大小和
warmup 设置。颜色代表了不同的模型大小和 warmup
设置的组合。该图显示了学习率的变化趋势，从大到小，最终逐渐接近一个稳定值。</p>
<h2 id="regularization">Regularization</h2>
<h3 id="label-smoothing">Label Smoothing</h3>
<p>During training, we employed label smoothing of value <span
class="math inline">\(\epsilon_{ls}=0.1\)</span> <a
target="_blank" rel="noopener" href="https://arxiv.org/abs/1512.00567">(cite)</a>. This hurts
perplexity, as the model learns to be more unsure, but improves accuracy
and BLEU score.</p>
<blockquote>
<p>We implement label smoothing using the KL div loss. Instead of using
a one-hot target distribution, we create a distribution that has
<code>confidence</code> of the correct word and the rest of the
<code>smoothing</code> mass distributed throughout the vocabulary.</p>
</blockquote>
<blockquote>
<p>我们使用 KL div
损失实现标签平滑。我们没有使用one-hot目标分布，而是创建了一个分布，该分布对正确的词具有置信度，其余的平滑质量分布在整个词汇表中。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LabelSmoothing</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implement label smoothing.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, padding_idx, smoothing=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(reduction=<span class="string">&quot;sum&quot;</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, target</span>):</span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>))</span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, true_dist.clone().detach())</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 Label Smoothing</p>
</blockquote>
<p>Label
Smoothing是一种常用的数据增强方法，目的是通过降低标签的置信度来抵抗过拟合。</p>
<p>上面的代码实现了这一方法，在构造函数中定义了一个KLDivLoss损失函数，表示用于计算两个概率分布的KL散度的损失。然后定义了几个基本的参数：size，表示每个输入的预测分布的维数；padding_idx，表示序列的填充值在预测分布中的位置；smoothing，表示要在每个预测分布上进行的平滑的系数；confidence，表示对真实标签的置信度。</p>
<p>在前向传播中，首先确保输入的维数是正确的，然后复制预测分布并将其全部填充为平滑系数除以size-2的值。接下来，在真实标签的位置增加置信度，在填充值的位置设置为0。然后使用蒙版确保对填充值进行正确的处理。最后计算KLDivLoss损失。</p>
<blockquote>
<p>Here we can see an example of how the mass is distributed to the
words based on confidence.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example of label smoothing.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_label_smoothing</span>():</span><br><span class="line">    crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">    predict = torch.FloatTensor(</span><br><span class="line">        [</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">    crit(x=predict.log(), target=torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>]))</span><br><span class="line">    LS_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;target distribution&quot;</span>: crit.true_dist[x, y].flatten(),</span><br><span class="line">                    <span class="string">&quot;columns&quot;</span>: y,</span><br><span class="line">                    <span class="string">&quot;rows&quot;</span>: x,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(LS_data)</span><br><span class="line">        .mark_rect(color=<span class="string">&quot;Blue&quot;</span>, opacity=<span class="number">1</span>)</span><br><span class="line">        .properties(height=<span class="number">200</span>, width=<span class="number">200</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            alt.X(<span class="string">&quot;columns:O&quot;</span>, title=<span class="literal">None</span>),</span><br><span class="line">            alt.Y(<span class="string">&quot;rows:O&quot;</span>, title=<span class="literal">None</span>),</span><br><span class="line">            alt.Color(</span><br><span class="line">                <span class="string">&quot;target distribution:Q&quot;</span>, scale=alt.Scale(scheme=<span class="string">&quot;viridis&quot;</span>)</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(example_label_smoothing)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 example_label_smoothing 的图像解释</p>
</blockquote>
<p>这张图是使用Altair库生成的，展示了一个模拟的Label
Smoothing例子中的目标分布与预测分布之间的差异。</p>
<p>它使用了5个类别和一组预测分布，并计算了通过LabelSmoothing损失函数产生的目标分布。目标分布是一个5x5的矩阵，每个数字代表了一个类别的置信度。</p>
<p>图中的矩形的颜色代表了目标分布的值，从浅到深的颜色代表了从小到大的置信度。由于图中的数字是实际的数字，因此可以使用Altair生成的交互式图像来查看每个数字的实际值。</p>
<blockquote>
<p>Label smoothing actually starts to penalize the model if it gets very
confident about a given choice.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, crit</span>):</span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([[<span class="number">0</span>, x / d, <span class="number">1</span> / d, <span class="number">1</span> / d, <span class="number">1</span> / d]])</span><br><span class="line">    <span class="keyword">return</span> crit(predict.log(), torch.LongTensor([<span class="number">1</span>])).data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">penalization_visualization</span>():</span><br><span class="line">    crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line">    loss_data = pd.DataFrame(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;Loss&quot;</span>: [loss(x, crit) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">100</span>)],</span><br><span class="line">            <span class="string">&quot;Steps&quot;</span>: <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">99</span>)),</span><br><span class="line">        &#125;</span><br><span class="line">    ).astype(<span class="string">&quot;float&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(loss_data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">350</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            x=<span class="string">&quot;Steps&quot;</span>,</span><br><span class="line">            y=<span class="string">&quot;Loss&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(penalization_visualization)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 penalization_visualization 的图像解释</p>
</blockquote>
<p>这张图展示了一组使用标签平滑的损失函数的结果，它显示了标签平滑的惩罚效果。</p>
<p>x 表示对预测为正确类别的信心程度，从 1 到 100
增加，代表对预测为正确类别的信心程度不断增加。损失函数 Loss 由函数 loss
计算得出。图上显示的折线图每一步增加 x，相应地，Loss 也在不断减少。</p>
<p>这表明，随着对正确类别的信心程度的增加，标签平滑算法惩罚效果越弱。因此，如果对正确类别的预测信心很高，那么损失值就很低；如果对正确类别的预测信心很低，则损失值会很高。</p>
<p>总的来说，这张图显示了标签平滑惩罚效果的变化情况，可以帮助我们更好地理解标签平滑算法的工作原理。</p>
<h1 id="第一个例子">第一个例子</h1>
<blockquote>
<p>We can begin by trying out a simple copy-task. Given a random set of
input symbols from a small vocabulary, the goal is to generate back
those same symbols.</p>
</blockquote>
<h2 id="synthetic-data">Synthetic Data</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_gen</span>(<span class="params">V, batch_size, nbatches</span>):</span><br><span class="line">    <span class="string">&quot;Generate random data for a src-tgt copy task.&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nbatches):</span><br><span class="line">        data = torch.randint(<span class="number">1</span>, V, size=(batch_size, <span class="number">10</span>))</span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        src = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        tgt = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h2 id="loss-computation">Loss Computation</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleLossCompute</span>:</span><br><span class="line">    <span class="string">&quot;A simple loss compute and train function.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, generator, criterion</span>):</span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = criterion</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x, y, norm</span>):</span><br><span class="line">        x = self.generator(x)</span><br><span class="line">        sloss = (</span><br><span class="line">            self.criterion(</span><br><span class="line">                x.contiguous().view(-<span class="number">1</span>, x.size(-<span class="number">1</span>)), y.contiguous().view(-<span class="number">1</span>)</span><br><span class="line">            )</span><br><span class="line">            / norm</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> sloss.data * norm, sloss</span><br></pre></td></tr></table></figure>
<h2 id="greedy-decoding">Greedy Decoding</h2>
<blockquote>
<p>This code predicts a translation using greedy decoding for
simplicity.</p>
</blockquote>
<blockquote>
<p>为简单起见，此代码使用贪婪解码预测翻译。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decode</span>(<span class="params">model, src, src_mask, max_len, start_symbol</span>):</span><br><span class="line">    memory = model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_len - <span class="number">1</span>):</span><br><span class="line">        out = model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = model.generator(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> ys</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 greedy_decode</p>
</blockquote>
<p>这段代码实现了贪心算法的解码，这是机器翻译等任务中常用的算法。</p>
<p>该函数接受以下参数：</p>
<ul>
<li>model：神经网络模型，需要有encode()和decode()方法</li>
<li>src：输入的数据</li>
<li>src_mask：输入数据的掩码</li>
<li>max_len：解码的最大长度</li>
<li>start_symbol：开始符</li>
</ul>
<p>该函数通过调用模型的encode方法，将输入编码为内存。然后，在一个for循环中进行解码，每一步用model的decode方法计算输出，再用model的generator方法预测下一个词的概率分布。最后选择概率最大的词作为下一个词，继续迭代。最终将解码的结果返回。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Train the simple copy task.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_simple_model</span>():</span><br><span class="line">    V = <span class="number">11</span></span><br><span class="line">    criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line">    model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        model.parameters(), lr=<span class="number">0.5</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = LambdaLR(</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        lr_lambda=<span class="keyword">lambda</span> step: rate(</span><br><span class="line">            step, model_size=model.src_embed[<span class="number">0</span>].d_model, factor=<span class="number">1.0</span>, warmup=<span class="number">400</span></span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">80</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        model.train()</span><br><span class="line">        run_epoch(</span><br><span class="line">            data_gen(V, batch_size, <span class="number">20</span>),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(model.generator, criterion),</span><br><span class="line">            optimizer,</span><br><span class="line">            lr_scheduler,</span><br><span class="line">            mode=<span class="string">&quot;train&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        run_epoch(</span><br><span class="line">            data_gen(V, batch_size, <span class="number">5</span>),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(model.generator, criterion),</span><br><span class="line">            DummyOptimizer(),</span><br><span class="line">            DummyScheduler(),</span><br><span class="line">            mode=<span class="string">&quot;eval&quot;</span>,</span><br><span class="line">        )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">    max_len = src.shape[<span class="number">1</span>]</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, max_len)</span><br><span class="line">    <span class="built_in">print</span>(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># execute_example(example_simple_model)</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 example_simple_model</p>
</blockquote>
<p>这是一个简单的模型训练的示例，它使用了一个称为 LabelSmoothing
的损失函数，一个名为 make_model 的模型生成函数以及一个名为 data_gen
的数据生成器。</p>
<p>其中，模型会被训练 20 次，每次运行一个 epoch，并使用 run_epoch
函数进行处理。训练时使用 Adam 优化器，学习率可以使用 LambdaLR
动态调整。批次大小为 80，每个 epoch 由 20 批数据训练，以及 5
批数据评估。</p>
<p>最后，使用 greedy_decode
函数，根据训练好的模型进行解码，并将结果打印输出。</p>
<h1 id="第三部分-真实世界的例子">第三部分: 真实世界的例子</h1>
<blockquote>
<p>Now we consider a real-world example using the Multi30k
German-English Translation task. This task is much smaller than the WMT
task considered in the paper, but it illustrates the whole system. We
also show how to use multi-gpu processing to make it really fast.</p>
</blockquote>
<blockquote>
<p>现在我们考虑使用 Multi30k
德语-英语翻译任务的真实示例。该任务比论文中考虑的 WMT
任务小得多，但它说明了整个系统。我们还展示了如何使用多 GPU
处理使其真正快速。</p>
</blockquote>
<h2 id="data-loading">Data Loading</h2>
<blockquote>
<p>We will load the dataset using torchtext and spacy for
tokenization.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Load spacy tokenizer models, download them if they haven&#x27;t been</span></span><br><span class="line"><span class="comment"># downloaded already</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_tokenizers</span>():</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spacy_de = spacy.load(<span class="string">&quot;de_core_news_sm&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        os.system(<span class="string">&quot;python -m spacy download de_core_news_sm&quot;</span>)</span><br><span class="line">        spacy_de = spacy.load(<span class="string">&quot;de_core_news_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spacy_en = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        os.system(<span class="string">&quot;python -m spacy download en_core_web_sm&quot;</span>)</span><br><span class="line">        spacy_en = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> spacy_de, spacy_en</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 load_tokenizers</p>
</blockquote>
<p>这个函数的作用是加载 spacy
语言模型，首先尝试加载德语（de_core_news_sm）和英语（en_core_web_sm）的
spacy 语言模型。如果没有安装这两个语言模型，则使用 os.system 函数执行
"python -m spacy download de_core_news_sm" 和 "python -m spacy download
en_core_web_sm" 进行下载。最后返回加载好的两个 spacy 语言模型。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text, tokenizer</span>):</span><br><span class="line">    <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> tokenizer.tokenizer(text)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">yield_tokens</span>(<span class="params">data_iter, tokenizer, index</span>):</span><br><span class="line">    <span class="keyword">for</span> from_to_tuple <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">yield</span> tokenizer(from_to_tuple[index])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于分词</p>
</blockquote>
<p>这两个函数都是进行分词操作的。</p>
<p>tokenize 函数接收一个文本字符串和一个 tokenizer 对象，并使用
tokenizer 对文本进行分词，返回分词后的结果，结果是一个字符串列表。</p>
<p>yield_tokens 函数接收一个数据迭代器、一个 tokenizer
和一个索引值，对每个数据迭代器中的元组进行分词，并使用 yield
语句产生分词后的结果，该函数可以生成一个分词的生成器。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_vocabulary</span>(<span class="params">spacy_de, spacy_en</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_de</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_de)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_en</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_en)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Building German Vocabulary ...&quot;</span>)</span><br><span class="line">    train, val, test = datasets.Multi30k(language_pair=(<span class="string">&quot;de&quot;</span>, <span class="string">&quot;en&quot;</span>))</span><br><span class="line">    vocab_src = build_vocab_from_iterator(</span><br><span class="line">        yield_tokens(train + val + test, tokenize_de, index=<span class="number">0</span>),</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        specials=[<span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>, <span class="string">&quot;&lt;blank&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Building English Vocabulary ...&quot;</span>)</span><br><span class="line">    train, val, test = datasets.Multi30k(language_pair=(<span class="string">&quot;de&quot;</span>, <span class="string">&quot;en&quot;</span>))</span><br><span class="line">    vocab_tgt = build_vocab_from_iterator(</span><br><span class="line">        yield_tokens(train + val + test, tokenize_en, index=<span class="number">1</span>),</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        specials=[<span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>, <span class="string">&quot;&lt;blank&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    vocab_src.set_default_index(vocab_src[<span class="string">&quot;&lt;unk&gt;&quot;</span>])</span><br><span class="line">    vocab_tgt.set_default_index(vocab_tgt[<span class="string">&quot;&lt;unk&gt;&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab_src, vocab_tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_vocab</span>(<span class="params">spacy_de, spacy_en</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> exists(<span class="string">&quot;vocab.pt&quot;</span>):</span><br><span class="line">        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)</span><br><span class="line">        torch.save((vocab_src, vocab_tgt), <span class="string">&quot;vocab.pt&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        vocab_src, vocab_tgt = torch.load(<span class="string">&quot;vocab.pt&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Finished.\nVocabulary sizes:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab_src))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab_tgt))</span><br><span class="line">    <span class="keyword">return</span> vocab_src, vocab_tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_interactive_notebook():</span><br><span class="line">    <span class="comment"># global variables used later in the script</span></span><br><span class="line">    spacy_de, spacy_en = show_example(load_tokenizers)</span><br><span class="line">    vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于词汇表建立</p>
</blockquote>
<p>build_vocabulary 函数实现了建立英德语言对的词汇表。它使用了两个 Spacy
库加载了两个语言的语言模型：德语模型 "de_core_news_sm" 和英语模型
"en_core_web_sm"。如果语言模型没有被下载，代码会自动下载。然后，使用
tokenize 函数和 yield_tokens 函数从 Multi30k
数据集中的训练、验证和测试数据构建词汇表。最后，返回德语词汇表和英语词汇表。</p>
<p>load_vocab 函数的目的是加载词汇表。它会检查是否存在 "vocab.pt"
文件，如果不存在则通过调用 build_vocabulary 函数构建词汇表，并保存为
"vocab.pt" 文件；如果存在则从文件中加载词汇表。</p>
<p>最后，在 Jupyter Notebook 中运行的情况下，会调用 show_example
函数加载分词器，并通过调用 load_vocab 函数加载词汇表。</p>
<blockquote>
<p>Batching matters a ton for speed. We want to have very evenly divided
batches, with absolutely minimal padding. To do this we have to hack a
bit around the default torchtext batching. This code patches their
default batching to make sure we search over enough sentences to find
tight batches.</p>
</blockquote>
<h2 id="iterators">Iterators</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collate_batch</span>(<span class="params"></span></span><br><span class="line"><span class="params">    batch,</span></span><br><span class="line"><span class="params">    src_pipeline,</span></span><br><span class="line"><span class="params">    tgt_pipeline,</span></span><br><span class="line"><span class="params">    src_vocab,</span></span><br><span class="line"><span class="params">    tgt_vocab,</span></span><br><span class="line"><span class="params">    device,</span></span><br><span class="line"><span class="params">    max_padding=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">    pad_id=<span class="number">2</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    bs_id = torch.tensor([<span class="number">0</span>], device=device)  <span class="comment"># &lt;s&gt; token id</span></span><br><span class="line">    eos_id = torch.tensor([<span class="number">1</span>], device=device)  <span class="comment"># &lt;/s&gt; token id</span></span><br><span class="line">    src_list, tgt_list = [], []</span><br><span class="line">    <span class="keyword">for</span> (_src, _tgt) <span class="keyword">in</span> batch:</span><br><span class="line">        processed_src = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                bs_id,</span><br><span class="line">                torch.tensor(</span><br><span class="line">                    src_vocab(src_pipeline(_src)),</span><br><span class="line">                    dtype=torch.int64,</span><br><span class="line">                    device=device,</span><br><span class="line">                ),</span><br><span class="line">                eos_id,</span><br><span class="line">            ],</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        processed_tgt = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                bs_id,</span><br><span class="line">                torch.tensor(</span><br><span class="line">                    tgt_vocab(tgt_pipeline(_tgt)),</span><br><span class="line">                    dtype=torch.int64,</span><br><span class="line">                    device=device,</span><br><span class="line">                ),</span><br><span class="line">                eos_id,</span><br><span class="line">            ],</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        src_list.append(</span><br><span class="line">            <span class="comment"># warning - overwrites values for negative values of padding - len</span></span><br><span class="line">            pad(</span><br><span class="line">                processed_src,</span><br><span class="line">                (</span><br><span class="line">                    <span class="number">0</span>,</span><br><span class="line">                    max_padding - <span class="built_in">len</span>(processed_src),</span><br><span class="line">                ),</span><br><span class="line">                value=pad_id,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        tgt_list.append(</span><br><span class="line">            pad(</span><br><span class="line">                processed_tgt,</span><br><span class="line">                (<span class="number">0</span>, max_padding - <span class="built_in">len</span>(processed_tgt)),</span><br><span class="line">                value=pad_id,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    src = torch.stack(src_list)</span><br><span class="line">    tgt = torch.stack(tgt_list)</span><br><span class="line">    <span class="keyword">return</span> (src, tgt)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 collate_batch</p>
</blockquote>
<p>collate_batch函数的目的是将一个batch（即一个数据集的一部分）中的每一个数据对进行预处理，并返回处理后的结果。</p>
<p>该函数接收六个参数：</p>
<ul>
<li><code>batch</code>：包含数据对的batch；</li>
<li><code>src_pipeline</code>：预处理源语言的函数；</li>
<li><code>tgt_pipeline</code>：预处理目标语言的函数；</li>
<li><code>src_vocab</code>：源语言的词汇表；</li>
<li><code>tgt_vocab</code>：目标语言的词汇表；</li>
<li><code>device</code>：指定的设备，如cpu或gpu。</li>
</ul>
<p>对于每个数据对，它首先使用<code>src_pipeline</code>对源语言文本进行预处理，然后使用<code>src_vocab</code>将预处理后的源语言文本转换为数字id。同样地，它也会对目标语言文本进行相同的操作。</p>
<p>然后，它在每个处理后的语言文本开头加上一个&lt;s&gt;标记，在结尾加上一个&lt;eos&gt;标记。</p>
<p>最后，它会调用pad函数将每个处理后的语言文本补全到指定长度，并返回处理后的结果（两个语言文本组成的元组）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataloaders</span>(<span class="params"></span></span><br><span class="line"><span class="params">    device,</span></span><br><span class="line"><span class="params">    vocab_src,</span></span><br><span class="line"><span class="params">    vocab_tgt,</span></span><br><span class="line"><span class="params">    spacy_de,</span></span><br><span class="line"><span class="params">    spacy_en,</span></span><br><span class="line"><span class="params">    batch_size=<span class="number">12000</span>,</span></span><br><span class="line"><span class="params">    max_padding=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">    is_distributed=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="comment"># def create_dataloaders(batch_size=12000):</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_de</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_de)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_en</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_en)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">        <span class="keyword">return</span> collate_batch(</span><br><span class="line">            batch,</span><br><span class="line">            tokenize_de,</span><br><span class="line">            tokenize_en,</span><br><span class="line">            vocab_src,</span><br><span class="line">            vocab_tgt,</span><br><span class="line">            device,</span><br><span class="line">            max_padding=max_padding,</span><br><span class="line">            pad_id=vocab_src.get_stoi()[<span class="string">&quot;&lt;blank&gt;&quot;</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    train_iter, valid_iter, test_iter = datasets.Multi30k(</span><br><span class="line">        language_pair=(<span class="string">&quot;de&quot;</span>, <span class="string">&quot;en&quot;</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_iter_map = to_map_style_dataset(</span><br><span class="line">        train_iter</span><br><span class="line">    )  <span class="comment"># DistributedSampler needs a dataset len()</span></span><br><span class="line">    train_sampler = (</span><br><span class="line">        DistributedSampler(train_iter_map) <span class="keyword">if</span> is_distributed <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    )</span><br><span class="line">    valid_iter_map = to_map_style_dataset(valid_iter)</span><br><span class="line">    valid_sampler = (</span><br><span class="line">        DistributedSampler(valid_iter_map) <span class="keyword">if</span> is_distributed <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_dataloader = DataLoader(</span><br><span class="line">        train_iter_map,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=(train_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">        sampler=train_sampler,</span><br><span class="line">        collate_fn=collate_fn,</span><br><span class="line">    )</span><br><span class="line">    valid_dataloader = DataLoader(</span><br><span class="line">        valid_iter_map,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=(valid_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">        sampler=valid_sampler,</span><br><span class="line">        collate_fn=collate_fn,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> train_dataloader, valid_dataloader</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 create_dataloaders</p>
</blockquote>
<p>这是一个创建数据加载器的函数，其目的是为训练和验证数据生成PyTorch数据加载器。</p>
<p>该函数定义了以下步骤：</p>
<ol type="1">
<li><p>定义了tokenize_de和tokenize_en函数，用于分词。</p></li>
<li><p>定义了collate_fn函数，用于在生成数据加载器之前整理数据。</p></li>
<li><p>使用Multi30k类加载数据集，生成训练、验证和测试数据迭代器。</p></li>
<li><p>将数据迭代器转换为可以处理的数据集，并生成分布式样本器（如果需要）。</p></li>
<li><p>使用PyTorch的DataLoader类生成训练和验证数据加载器。</p></li>
</ol>
<p>最后，该函数返回训练和验证数据加载器。</p>
<h2 id="training-the-system">Training the System</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_worker</span>(<span class="params"></span></span><br><span class="line"><span class="params">    gpu,</span></span><br><span class="line"><span class="params">    ngpus_per_node,</span></span><br><span class="line"><span class="params">    vocab_src,</span></span><br><span class="line"><span class="params">    vocab_tgt,</span></span><br><span class="line"><span class="params">    spacy_de,</span></span><br><span class="line"><span class="params">    spacy_en,</span></span><br><span class="line"><span class="params">    config,</span></span><br><span class="line"><span class="params">    is_distributed=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Train worker process using GPU: <span class="subst">&#123;gpu&#125;</span> for training&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    torch.cuda.set_device(gpu)</span><br><span class="line"></span><br><span class="line">    pad_idx = vocab_tgt[<span class="string">&quot;&lt;blank&gt;&quot;</span>]</span><br><span class="line">    d_model = <span class="number">512</span></span><br><span class="line">    model = make_model(<span class="built_in">len</span>(vocab_src), <span class="built_in">len</span>(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.cuda(gpu)</span><br><span class="line">    module = model</span><br><span class="line">    is_main_process = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> is_distributed:</span><br><span class="line">        dist.init_process_group(</span><br><span class="line">            <span class="string">&quot;nccl&quot;</span>, init_method=<span class="string">&quot;env://&quot;</span>, rank=gpu, world_size=ngpus_per_node</span><br><span class="line">        )</span><br><span class="line">        model = DDP(model, device_ids=[gpu])</span><br><span class="line">        module = model.module</span><br><span class="line">        is_main_process = gpu == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    criterion = LabelSmoothing(</span><br><span class="line">        size=<span class="built_in">len</span>(vocab_tgt), padding_idx=pad_idx, smoothing=<span class="number">0.1</span></span><br><span class="line">    )</span><br><span class="line">    criterion.cuda(gpu)</span><br><span class="line"></span><br><span class="line">    train_dataloader, valid_dataloader = create_dataloaders(</span><br><span class="line">        gpu,</span><br><span class="line">        vocab_src,</span><br><span class="line">        vocab_tgt,</span><br><span class="line">        spacy_de,</span><br><span class="line">        spacy_en,</span><br><span class="line">        batch_size=config[<span class="string">&quot;batch_size&quot;</span>] // ngpus_per_node,</span><br><span class="line">        max_padding=config[<span class="string">&quot;max_padding&quot;</span>],</span><br><span class="line">        is_distributed=is_distributed,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        model.parameters(), lr=config[<span class="string">&quot;base_lr&quot;</span>], betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = LambdaLR(</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        lr_lambda=<span class="keyword">lambda</span> step: rate(</span><br><span class="line">            step, d_model, factor=<span class="number">1</span>, warmup=config[<span class="string">&quot;warmup&quot;</span>]</span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line">    train_state = TrainState()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(config[<span class="string">&quot;num_epochs&quot;</span>]):</span><br><span class="line">        <span class="keyword">if</span> is_distributed:</span><br><span class="line">            train_dataloader.sampler.set_epoch(epoch)</span><br><span class="line">            valid_dataloader.sampler.set_epoch(epoch)</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[GPU<span class="subst">&#123;gpu&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> Training ====&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        _, train_state = run_epoch(</span><br><span class="line">            (Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx) <span class="keyword">for</span> b <span class="keyword">in</span> train_dataloader),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(module.generator, criterion),</span><br><span class="line">            optimizer,</span><br><span class="line">            lr_scheduler,</span><br><span class="line">            mode=<span class="string">&quot;train+log&quot;</span>,</span><br><span class="line">            accum_iter=config[<span class="string">&quot;accum_iter&quot;</span>],</span><br><span class="line">            train_state=train_state,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        GPUtil.showUtilization()</span><br><span class="line">        <span class="keyword">if</span> is_main_process:</span><br><span class="line">            file_path = <span class="string">&quot;%s%.2d.pt&quot;</span> % (config[<span class="string">&quot;file_prefix&quot;</span>], epoch)</span><br><span class="line">            torch.save(module.state_dict(), file_path)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[GPU<span class="subst">&#123;gpu&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> Validation ====&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        sloss = run_epoch(</span><br><span class="line">            (Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx) <span class="keyword">for</span> b <span class="keyword">in</span> valid_dataloader),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(module.generator, criterion),</span><br><span class="line">            DummyOptimizer(),</span><br><span class="line">            DummyScheduler(),</span><br><span class="line">            mode=<span class="string">&quot;eval&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(sloss)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_main_process:</span><br><span class="line">        file_path = <span class="string">&quot;%sfinal.pt&quot;</span> % config[<span class="string">&quot;file_prefix&quot;</span>]</span><br><span class="line">        torch.save(module.state_dict(), file_path)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 train_worker</p>
</blockquote>
<p>这段代码实现了一个训练工作进程，它是在训练神经机器翻译（NMT）模型时使用的。</p>
<p>代码流程如下：</p>
<ol type="1">
<li>设置当前 GPU 设备。</li>
<li>根据给定的词汇，创建一个 NMT 模型，并将其移动到 GPU 设备上。</li>
<li>如果处于分布式环境，则将模型进行分布式数据并行（DDP）处理。</li>
<li>创建一个简单的损失计算函数，并移动到 GPU 设备上。</li>
<li>创建训练数据和验证数据加载器。</li>
<li>创建一个 Adam 优化器，以及一个学习率调度器。</li>
<li>循环训练 NMT 模型，共进行指定数量的训练轮数（num_epochs）。</li>
<li>在每个训练轮结束时，显示 GPU
利用率，并在主进程中将当前模型的参数存储到文件。</li>
<li>在验证集上评估模型的性能。</li>
<li>在主进程结束时，将最终的模型参数存储到文件。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_distributed_model</span>(<span class="params">vocab_src, vocab_tgt, spacy_de, spacy_en, config</span>):</span><br><span class="line">    <span class="keyword">from</span> the_annotated_transformer <span class="keyword">import</span> train_worker</span><br><span class="line"></span><br><span class="line">    ngpus = torch.cuda.device_count()</span><br><span class="line">    os.environ[<span class="string">&quot;MASTER_ADDR&quot;</span>] = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">    os.environ[<span class="string">&quot;MASTER_PORT&quot;</span>] = <span class="string">&quot;12356&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Number of GPUs detected: <span class="subst">&#123;ngpus&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Spawning training processes ...&quot;</span>)</span><br><span class="line">    mp.spawn(</span><br><span class="line">        train_worker,</span><br><span class="line">        nprocs=ngpus,</span><br><span class="line">        args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, <span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">vocab_src, vocab_tgt, spacy_de, spacy_en, config</span>):</span><br><span class="line">    <span class="keyword">if</span> config[<span class="string">&quot;distributed&quot;</span>]:</span><br><span class="line">        train_distributed_model(</span><br><span class="line">            vocab_src, vocab_tgt, spacy_de, spacy_en, config</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        train_worker(</span><br><span class="line">            <span class="number">0</span>, <span class="number">1</span>, vocab_src, vocab_tgt, spacy_de, spacy_en, config, <span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_trained_model</span>():</span><br><span class="line">    config = &#123;</span><br><span class="line">        <span class="string">&quot;batch_size&quot;</span>: <span class="number">32</span>,</span><br><span class="line">        <span class="string">&quot;distributed&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;num_epochs&quot;</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">&quot;accum_iter&quot;</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">&quot;base_lr&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">&quot;max_padding&quot;</span>: <span class="number">72</span>,</span><br><span class="line">        <span class="string">&quot;warmup&quot;</span>: <span class="number">3000</span>,</span><br><span class="line">        <span class="string">&quot;file_prefix&quot;</span>: <span class="string">&quot;multi30k_model_&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    model_path = <span class="string">&quot;multi30k_model_final.pt&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> exists(model_path):</span><br><span class="line">        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)</span><br><span class="line"></span><br><span class="line">    model = make_model(<span class="built_in">len</span>(vocab_src), <span class="built_in">len</span>(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&quot;multi30k_model_final.pt&quot;</span>))</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_interactive_notebook():</span><br><span class="line">    model = load_trained_model()</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 Real World Example 的训练</p>
</blockquote>
<p>train_distributed_model是一个以分布式方式在多个GPU上训练深度学习模型的函数。这个函数使用PyTorch的torch.nn.DataParallel类来包装模型，并在GPU上分割数据。</p>
<p>train_model是一个训练深度学习模型的函数。它检查是否应该以分布式方式进行训练（通过检查config["distributed"]的值），并调用train_distributed_model或train_worker。</p>
<p>load_trained_model是一个从磁盘加载预训练的深度学习模型的函数。它首先检查模型是否存在，如果不存在，则使用
train_model 函数训练模型。然后使用torch.load加载模型。</p>
<p>if
is_interactive_notebook()语句检查代码是否在交互式Jupyter笔记本中运行。如果是，它就通过调用load_trained_model来创建模型的实例。</p>
<blockquote>
<p>Once trained we can decode the model to produce a set of
translations. Here we simply translate the first sentence in the
validation set. This dataset is pretty small so the translations with
greedy search are reasonably accurate.</p>
</blockquote>
<blockquote>
<p>一旦训练完成，我们就可以解码模型以生成一组翻译。这里我们简单翻译验证集中的第一句话。这个数据集非常小，所以贪婪搜索的翻译相当准确。</p>
</blockquote>
<h1 id="附加组件-bpe-search-averaging">附加组件: BPE, Search,
Averaging</h1>
<blockquote>
<p>So this mostly covers the transformer model itself. There are four
aspects that we didn't cover explicitly. We also have all these
additional features implemented in <a
target="_blank" rel="noopener" href="https://github.com/opennmt/opennmt-py">OpenNMT-py</a>.</p>
</blockquote>
<blockquote>
<p>以上是Transformer的模型构建，以下是四个原文没涉及到的细节实现部分：</p>
</blockquote>
<blockquote>
<p>(1) BPE/ Word-piece: We can use a library to first preprocess the
data into subword units. See Rico Sennrich's <a
target="_blank" rel="noopener" href="https://github.com/rsennrich/subword-nmt">subword-nmt</a>
implementation. These models will transform the training data to look
like this:</p>
</blockquote>
<p>▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an
▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .</p>
<blockquote>
<p>关于 (1) BPE 数据预处理</p>
</blockquote>
<p><code>subword-nmt</code> : sentence <code>-&gt;</code> subword</p>
<blockquote>
<p>(2) Shared Embeddings: When using BPE with shared vocabulary we can
share the same weight vectors between the source / target / generator.
See the <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1608.05859">(cite)</a> for
details. To add this to the model simply do this:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="literal">False</span>:</span><br><span class="line">    model.src_embed[<span class="number">0</span>].lut.weight = model.tgt_embeddings[<span class="number">0</span>].lut.weight</span><br><span class="line">    model.generator.lut.weight = model.tgt_embed[<span class="number">0</span>].lut.weight</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 (2) 共享 Embedding 权重的实现</p>
</blockquote>
<p>这段话和代码指的是在使用BPE（Byte-Pair
Encoding）分词方法，并且使用了共享词汇表时，源语言、目标语言和生成器可以共享同一组权重矩阵。参考论文"Using
the Output Embedding to Improve Language
Models"(https://arxiv.org/abs/1608.05859) 。</p>
<p>如果想在模型中添加这一功能，可以执行代码中的操作：将源语言的词嵌入矩阵、目标语言的词嵌入矩阵和生成器的词嵌入矩阵共享赋值为目标语言的词嵌入矩阵。</p>
<blockquote>
<p>(3) Beam Search: This is a bit too complicated to cover here. See the
<a target="_blank" rel="noopener" href="https://github.com/OpenNMT/OpenNMT-py/">OpenNMT-py</a> for a
pytorch implementation.</p>
</blockquote>
<blockquote>
<p>关于 (3) 集束搜索</p>
</blockquote>
<p>在The Annotated
Transformer的背景下，波束搜索被用作解码算法，以生成机器翻译任务中的目标序列。</p>
<p>集束搜索是一种用于序列生成任务（例如机器翻译、图像字幕和语音识别）的搜索算法，用于在给定一组输入数据的情况下找到最佳输出集（即得分最高的单词序列）。它是一种启发式搜索算法，用于通过在每个时间步维护一个波束或一组
K 个候选输出来生成高质量输出，其中 K 是波束宽度。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义计算得分的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_score</span>(<span class="params">sequence, weights</span>):</span><br><span class="line">    score = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sequence)):</span><br><span class="line">        score += sequence[i] * weights[i]</span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义搜索函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">beam_search</span>(<span class="params">weights, k</span>):</span><br><span class="line">    <span class="comment"># 存储当前最优的k个序列</span></span><br><span class="line">    best_sequences = [[]]</span><br><span class="line">    <span class="comment"># 存储当前最优的k个得分</span></span><br><span class="line">    best_scores = [<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 定义循环次数，以此来限制生成的序列长度</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(weights)):</span><br><span class="line">        all_candidates = []</span><br><span class="line">        <span class="comment"># 遍历当前最优的k个序列</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(best_sequences)):</span><br><span class="line">            sequence = best_sequences[i]</span><br><span class="line">            score = best_scores[i]</span><br><span class="line">            <span class="comment"># 生成0或1的候选序列</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>]:</span><br><span class="line">                candidate = sequence + [j]</span><br><span class="line">                candidate_score = score + j * weights[t]</span><br><span class="line">                all_candidates.append([candidate, candidate_score])</span><br><span class="line">        <span class="comment"># 根据得分排序</span></span><br><span class="line">        ordered = <span class="built_in">sorted</span>(all_candidates, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 选择得分最高的k个候选序列</span></span><br><span class="line">        best_sequences = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> ordered[:k]]</span><br><span class="line">        best_scores = [x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> ordered[:k]]</span><br><span class="line">    <span class="keyword">return</span> best_sequences[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试beam search算法</span></span><br><span class="line">weights = [<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]</span><br><span class="line">result = beam_search(weights, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最优序列：&quot;</span>, result)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最优得分：&quot;</span>, calculate_score(result, weights))</span><br></pre></td></tr></table></figure>
<p>calculate_score
函数是根据输入的序列和一些预先定义好的权重，来计算该序列的得分。beam_search
函数是执行beam search算法的主体，该函数接受两个参数：k 和 max_steps。 k
是 beam宽，表示在每个时刻保留的候选序列数； max_steps
是算法最大执行步数。</p>
<p>在这个例子中，算法的停止条件是搜索的步数到达了
max_steps，或者只剩下一个候选序列，此时算法终止。</p>
<p>该函数的主要逻辑是：初始化候选序列的列表，每个候选序列的初始状态是一个长度为1的0。在每一步，扩展候选序列，并对扩展后的序列计算得分。根据得分对所有候选序列排序，保留得分前
k 名的候选序列。
如果算法达到了最大执行步数或者只剩下了一个候选序列，则终止。</p>
<p>最终，该算法返回得分最高的序列，即最终的答案。</p>
<blockquote>
<p>(4) Model Averaging: The paper averages the last k checkpoints to
create an ensembling effect. We can do this after the fact if we have a
bunch of models:</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">average</span>(<span class="params">model, models</span>):</span><br><span class="line">    <span class="string">&quot;Average models into model&quot;</span></span><br><span class="line">    <span class="keyword">for</span> ps <span class="keyword">in</span> <span class="built_in">zip</span>(*[m.params() <span class="keyword">for</span> m <span class="keyword">in</span> [model] + models]):</span><br><span class="line">        ps[<span class="number">0</span>].copy_(torch.<span class="built_in">sum</span>(*ps[<span class="number">1</span>:]) / <span class="built_in">len</span>(ps[<span class="number">1</span>:]))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 （4）模型平均</p>
</blockquote>
<p>这段代码实现了一个名为 average 的函数，用于平均多个模型的参数。</p>
<p>这个函数接收两个参数：model 和 models。model 是一个模型，models
是一个模型的列表。该函数将所有模型的参数进行求和，再除以模型数量，最后将结果复制到
model 中。</p>
<p>这段代码可能用于在The Annotated Transformer by Harvard NLP
中训练多个模型，然后将它们的参数进行平均以得到一个最终的模型。这种方法常用于集成学习，可以提高模型的稳定性和准确率。</p>
<h1 id="结果">结果</h1>
<p>On the WMT 2014 English-to-German translation task, the big
transformer model (Transformer (big) in Table 2) outperforms the best
previously reported models (including ensembles) by more than 2.0 BLEU,
establishing a new state-of-the-art BLEU score of 28.4. The
configuration of this model is listed in the bottom line of Table 3.
Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all
previously published models and ensembles, at a fraction of the training
cost of any of the competitive models.</p>
<p>On the WMT 2014 English-to-French translation task, our big model
achieves a BLEU score of 41.0, outperforming all of the previously
published single models, at less than 1/4 the training cost of the
previous state-of-the-art model. The Transformer (big) model trained for
English-to-French used dropout rate Pdrop = 0.1, instead of 0.3.</p>
<blockquote>
<p>With the addtional extensions in the last section, the OpenNMT-py
replication gets to 26.9 on EN-DE WMT. Here I have loaded in those
parameters to our reimplemenation.</p>
</blockquote>
<blockquote>
<p>通过上一节中的附加扩展，OpenNMT-py 复制在 EN-DE WMT 上达到
26.9。在这里，我已将这些参数加载到我们的重新实现中。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Load data and model for output checks</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">check_outputs</span>(<span class="params"></span></span><br><span class="line"><span class="params">    valid_dataloader,</span></span><br><span class="line"><span class="params">    model,</span></span><br><span class="line"><span class="params">    vocab_src,</span></span><br><span class="line"><span class="params">    vocab_tgt,</span></span><br><span class="line"><span class="params">    n_examples=<span class="number">15</span>,</span></span><br><span class="line"><span class="params">    pad_idx=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">    eos_string=<span class="string">&quot;&lt;/s&gt;&quot;</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    results = [()] * n_examples</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(n_examples):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\nExample %d ========\n&quot;</span> % idx)</span><br><span class="line">        b = <span class="built_in">next</span>(<span class="built_in">iter</span>(valid_dataloader))</span><br><span class="line">        rb = Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx)</span><br><span class="line">        greedy_decode(model, rb.src, rb.src_mask, <span class="number">64</span>, <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        src_tokens = [</span><br><span class="line">            vocab_src.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> rb.src[<span class="number">0</span>] <span class="keyword">if</span> x != pad_idx</span><br><span class="line">        ]</span><br><span class="line">        tgt_tokens = [</span><br><span class="line">            vocab_tgt.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> rb.tgt[<span class="number">0</span>] <span class="keyword">if</span> x != pad_idx</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Source Text (Input)        : &quot;</span></span><br><span class="line">            + <span class="string">&quot; &quot;</span>.join(src_tokens).replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Target Text (Ground Truth) : &quot;</span></span><br><span class="line">            + <span class="string">&quot; &quot;</span>.join(tgt_tokens).replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        )</span><br><span class="line">        model_out = greedy_decode(model, rb.src, rb.src_mask, <span class="number">72</span>, <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">        model_txt = (</span><br><span class="line">            <span class="string">&quot; &quot;</span>.join(</span><br><span class="line">                [vocab_tgt.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> model_out <span class="keyword">if</span> x != pad_idx]</span><br><span class="line">            ).split(eos_string, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            + eos_string</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Model Output               : &quot;</span> + model_txt.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_model_example</span>(<span class="params">n_examples=<span class="number">5</span></span>):</span><br><span class="line">    <span class="keyword">global</span> vocab_src, vocab_tgt, spacy_de, spacy_en</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Preparing Data ...&quot;</span>)</span><br><span class="line">    _, valid_dataloader = create_dataloaders(</span><br><span class="line">        torch.device(<span class="string">&quot;cpu&quot;</span>),</span><br><span class="line">        vocab_src,</span><br><span class="line">        vocab_tgt,</span><br><span class="line">        spacy_de,</span><br><span class="line">        spacy_en,</span><br><span class="line">        batch_size=<span class="number">1</span>,</span><br><span class="line">        is_distributed=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Loading Trained Model ...&quot;</span>)</span><br><span class="line"></span><br><span class="line">    model = make_model(<span class="built_in">len</span>(vocab_src), <span class="built_in">len</span>(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.load_state_dict(</span><br><span class="line">        torch.load(<span class="string">&quot;multi30k_model_final.pt&quot;</span>, map_location=torch.device(<span class="string">&quot;cpu&quot;</span>))</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Checking Model Outputs:&quot;</span>)</span><br><span class="line">    example_data = check_outputs(</span><br><span class="line">        valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model, example_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># execute_example(run_model_example)</span></span><br></pre></td></tr></table></figure>
<h2 id="attention-visualization">Attention Visualization</h2>
<blockquote>
<p>Even with a greedy decoder the translation looks pretty good. We can
further visualize it to see what is happening at each layer of the
attention</p>
</blockquote>
<blockquote>
<p>即使使用贪心解码器，翻译看起来也很不错。我们可以进一步可视化它，看看每一层注意力都发生了什么</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mtx2df</span>(<span class="params">m, max_row, max_col, row_tokens, col_tokens</span>):</span><br><span class="line">    <span class="string">&quot;convert a dense matrix to a data frame with row and column indices&quot;</span></span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        [</span><br><span class="line">            (</span><br><span class="line">                r,</span><br><span class="line">                c,</span><br><span class="line">                <span class="built_in">float</span>(m[r, c]),</span><br><span class="line">                <span class="string">&quot;%.3d %s&quot;</span></span><br><span class="line">                % (r, row_tokens[r] <span class="keyword">if</span> <span class="built_in">len</span>(row_tokens) &gt; r <span class="keyword">else</span> <span class="string">&quot;&lt;blank&gt;&quot;</span>),</span><br><span class="line">                <span class="string">&quot;%.3d %s&quot;</span></span><br><span class="line">                % (c, col_tokens[c] <span class="keyword">if</span> <span class="built_in">len</span>(col_tokens) &gt; c <span class="keyword">else</span> <span class="string">&quot;&lt;blank&gt;&quot;</span>),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> r &lt; max_row <span class="keyword">and</span> c &lt; max_col</span><br><span class="line">        ],</span><br><span class="line">        <span class="comment"># if float(m[r,c]) != 0 and r &lt; max_row and c &lt; max_col],</span></span><br><span class="line">        columns=[<span class="string">&quot;row&quot;</span>, <span class="string">&quot;column&quot;</span>, <span class="string">&quot;value&quot;</span>, <span class="string">&quot;row_token&quot;</span>, <span class="string">&quot;col_token&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attn_map</span>(<span class="params">attn, layer, head, row_tokens, col_tokens, max_dim=<span class="number">30</span></span>):</span><br><span class="line">    df = mtx2df(</span><br><span class="line">        attn[<span class="number">0</span>, head].data,</span><br><span class="line">        max_dim,</span><br><span class="line">        max_dim,</span><br><span class="line">        row_tokens,</span><br><span class="line">        col_tokens,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data=df)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .encode(</span><br><span class="line">            x=alt.X(<span class="string">&quot;col_token&quot;</span>, axis=alt.Axis(title=<span class="string">&quot;&quot;</span>)),</span><br><span class="line">            y=alt.Y(<span class="string">&quot;row_token&quot;</span>, axis=alt.Axis(title=<span class="string">&quot;&quot;</span>)),</span><br><span class="line">            color=<span class="string">&quot;value&quot;</span>,</span><br><span class="line">            tooltip=[<span class="string">&quot;row&quot;</span>, <span class="string">&quot;column&quot;</span>, <span class="string">&quot;value&quot;</span>, <span class="string">&quot;row_token&quot;</span>, <span class="string">&quot;col_token&quot;</span>],</span><br><span class="line">        )</span><br><span class="line">        .properties(height=<span class="number">400</span>, width=<span class="number">400</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_encoder</span>(<span class="params">model, layer</span>):</span><br><span class="line">    <span class="keyword">return</span> model.encoder.layers[layer].self_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_decoder_self</span>(<span class="params">model, layer</span>):</span><br><span class="line">    <span class="keyword">return</span> model.decoder.layers[layer].self_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_decoder_src</span>(<span class="params">model, layer</span>):</span><br><span class="line">    <span class="keyword">return</span> model.decoder.layers[layer].src_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_layer</span>(<span class="params">model, layer, getter_fn, ntokens, row_tokens, col_tokens</span>):</span><br><span class="line">    <span class="comment"># ntokens = last_example[0].ntokens</span></span><br><span class="line">    attn = getter_fn(model, layer)</span><br><span class="line">    n_heads = attn.shape[<span class="number">1</span>]</span><br><span class="line">    charts = [</span><br><span class="line">        attn_map(</span><br><span class="line">            attn,</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">            h,</span><br><span class="line">            row_tokens=row_tokens,</span><br><span class="line">            col_tokens=col_tokens,</span><br><span class="line">            max_dim=ntokens,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_heads)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">assert</span> n_heads == <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> alt.vconcat(</span><br><span class="line">        charts[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># | charts[1]</span></span><br><span class="line">        | charts[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># | charts[3]</span></span><br><span class="line">        | charts[<span class="number">4</span>]</span><br><span class="line">        <span class="comment"># | charts[5]</span></span><br><span class="line">        | charts[<span class="number">6</span>]</span><br><span class="line">        <span class="comment"># | charts[7]</span></span><br><span class="line">        <span class="comment"># layer + 1 due to 0-indexing</span></span><br><span class="line">    ).properties(title=<span class="string">&quot;Layer %d&quot;</span> % (layer + <span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<h2 id="encoder-self-attention">Encoder Self Attention</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">viz_encoder_self</span>():</span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[</span><br><span class="line">        <span class="built_in">len</span>(example_data) - <span class="number">1</span></span><br><span class="line">    ]  <span class="comment"># batch object for the final example</span></span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model, layer, get_encoder, <span class="built_in">len</span>(example[<span class="number">1</span>]), example[<span class="number">1</span>], example[<span class="number">1</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># &amp; layer_viz[1]</span></span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># &amp; layer_viz[3]</span></span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        <span class="comment"># &amp; layer_viz[5]</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_encoder_self)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>关于 Encoder Self Attention</p>
</blockquote>
<p>生成的可视化显示了Transformer模型中编码器部分的每个层的激活情况。可视化的层是通过visualize_layer函数的list
layer_viz指定的。这个函数生成了一个激活的热图，颜色越亮，激活值越高。</p>
<p>在这个具体的实现中，可视化只限于输入数据的最后一个例子（example_data）。viz_encoder_self函数返回0、2、4层的激活值的连接。最后的结果用show_example函数显示。</p>
<p>注意：可视化可以让我们了解编码器是如何处理输入序列的，以及激活是如何在Transformer的每一层上变化的。</p>
<h2 id="decoder-self-attention">Decoder Self Attention</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">viz_decoder_self</span>():</span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[<span class="built_in">len</span>(example_data) - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model,</span><br><span class="line">            layer,</span><br><span class="line">            get_decoder_self,</span><br><span class="line">            <span class="built_in">len</span>(example[<span class="number">1</span>]),</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">1</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">3</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">5</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_decoder_self)</span><br></pre></td></tr></table></figure>
<h2 id="decoder-src-attention">Decoder Src Attention</h2>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">viz_decoder_src</span>():</span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[<span class="built_in">len</span>(example_data) - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model,</span><br><span class="line">            layer,</span><br><span class="line">            get_decoder_src,</span><br><span class="line">            <span class="built_in">max</span>(<span class="built_in">len</span>(example[<span class="number">1</span>]), <span class="built_in">len</span>(example[<span class="number">2</span>])),</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">            example[<span class="number">2</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">1</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">3</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">5</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_decoder_src)</span><br></pre></td></tr></table></figure>
<h1 id="参考链接">参考链接</h1>
<h2 id="完整代码实现">完整代码实现</h2>
<ul>
<li><p>哈佛大学NLP组的notebook，很详细文字和代码描述，用pytorch实现</p>
<p><a
target="_blank" rel="noopener" href="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p></li>
</ul>
<h2 id="注释说明参考网页">注释说明参考网页</h2>
<ol type="1">
<li><p>哈佛大学NLP组的colab notebook：The Annotated "Attention is All
You Need".ipynb</p>
<p><a
target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1xQXSv6mtAOLXxEMi8RvaW8TW-7bvYBDF">https://colab.research.google.com/drive/1xQXSv6mtAOLXxEMi8RvaW8TW-7bvYBDF</a></p></li>
<li><p>The Annotated Transformer的中文注释版:</p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/107889011">https://zhuanlan.zhihu.com/p/107889011</a></p></li>
<li><p>论文阅读笔记（结合李沐视频）--- Attention is all you
need（Transformer）逐段精读</p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/qq_39594939/article/details/122823436">https://blog.csdn.net/qq_39594939/article/details/122823436</a></p></li>
<li><p>庖丁解牛式读《Attention is all your need》</p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/liu16659/article/details/108141534">https://blog.csdn.net/liu16659/article/details/108141534</a></p></li>
<li><p>Transformer代码阅读</p>
<p><a
target="_blank" rel="noopener" href="http://fancyerii.github.io/2019/03/09/transformer-codes/">http://fancyerii.github.io/2019/03/09/transformer-codes/</a></p></li>
</ol>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E7%A0%94/">科研</a><a class="post-meta__tags" href="/tags/transformer/">transformer</a></div><div class="post_share"><div class="social-share" data-image="https://github.com/serika-onoe/web-img/raw/main/Transformer/harvard.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/10/11/%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86/"><img class="prev-cover" src="https://github.com/Zeying-Gong/web-img/raw/main/background/coder_openai.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">同步电脑端和手机端日历教程（基于Outlook邮箱）</div></div></a></div><div class="next-post pull-right"><a href="/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/"><img class="next-cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/simpletransformers-logo.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/" title="Transformer 零基础解析教程，从Encoder-Decoder架构说起（1&#x2F;4）"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-14</div><div class="title">Transformer 零基础解析教程，从Encoder-Decoder架构说起（1&#x2F;4）</div></div></a></div><div><a href="/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/" title="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2&#x2F;4）"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-11</div><div class="title">Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2&#x2F;4）</div></div></a></div><div><a href="/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/" title="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3&#x2F;4）"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/simpletransformers-logo.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-01-31</div><div class="title">Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3&#x2F;4）</div></div></a></div><div><a href="/2022/12/09/%E6%88%91%E7%9A%84%E7%A7%91%E7%A0%94%E7%BB%8F%E5%8E%86/" title="我的项目经历"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Experience/uav3(1).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-09</div><div class="title">我的项目经历</div></div></a></div><div><a href="/2024/01/08/%20%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/" title="“Embodied AI”前言"><img class="cover" src="https://github.com/Zeying-Gong/web-img/raw/main/background/coder_openai.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-01-08</div><div class="title">“Embodied AI”前言</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/coder.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">空之影</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">11</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">5</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/zeying-gong" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="https://www.linkedin.com/in/zeying-gong" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a><a class="social-icon" href="mailto:zgong313@connect.hkust-gz.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://twitter.com/KungRikhard" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">个人学术主页已更新，可点击顶部导航栏的“关于我”选项查看，欢迎合作交流~</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AF%BC%E8%88%AA"><span class="toc-number">1.</span> <span class="toc-text">导航</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">2.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%83%8C%E6%99%AF"><span class="toc-number">3.</span> <span class="toc-text">背景</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">4.</span> <span class="toc-text">第一部分: 模型架构</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#encoder-and-decoder-stacks"><span class="toc-number">4.1.</span> <span class="toc-text">Encoder and Decoder Stacks</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder"><span class="toc-number">4.1.1.</span> <span class="toc-text">Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decoder"><span class="toc-number">4.1.2.</span> <span class="toc-text">Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#attention"><span class="toc-number">4.1.3.</span> <span class="toc-text">Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#applications-of-attention-in-our-model"><span class="toc-number">4.1.4.</span> <span class="toc-text">Applications of
Attention in our Model</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#position-wise-feed-forward-networks"><span class="toc-number">4.2.</span> <span class="toc-text">Position-wise Feed-Forward
Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#embeddings-and-softmax"><span class="toc-number">4.3.</span> <span class="toc-text">Embeddings and Softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#positional-encoding"><span class="toc-number">4.4.</span> <span class="toc-text">Positional Encoding</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#full-model"><span class="toc-number">4.5.</span> <span class="toc-text">Full Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#inference"><span class="toc-number">4.6.</span> <span class="toc-text">Inference:</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="toc-number">5.</span> <span class="toc-text">第二部分: 模型训练</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83"><span class="toc-number">6.</span> <span class="toc-text">训练</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#batches-and-masking"><span class="toc-number">6.1.</span> <span class="toc-text">Batches and Masking</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#training-loop"><span class="toc-number">6.2.</span> <span class="toc-text">Training Loop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#training-data-and-batching"><span class="toc-number">6.3.</span> <span class="toc-text">Training Data and Batching</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hardware-and-schedule"><span class="toc-number">6.4.</span> <span class="toc-text">Hardware and Schedule</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#optimizer"><span class="toc-number">6.5.</span> <span class="toc-text">Optimizer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#regularization"><span class="toc-number">6.6.</span> <span class="toc-text">Regularization</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#label-smoothing"><span class="toc-number">6.6.1.</span> <span class="toc-text">Label Smoothing</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E4%B8%AA%E4%BE%8B%E5%AD%90"><span class="toc-number">7.</span> <span class="toc-text">第一个例子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#synthetic-data"><span class="toc-number">7.1.</span> <span class="toc-text">Synthetic Data</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#loss-computation"><span class="toc-number">7.2.</span> <span class="toc-text">Loss Computation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#greedy-decoding"><span class="toc-number">7.3.</span> <span class="toc-text">Greedy Decoding</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E9%83%A8%E5%88%86-%E7%9C%9F%E5%AE%9E%E4%B8%96%E7%95%8C%E7%9A%84%E4%BE%8B%E5%AD%90"><span class="toc-number">8.</span> <span class="toc-text">第三部分: 真实世界的例子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#data-loading"><span class="toc-number">8.1.</span> <span class="toc-text">Data Loading</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#iterators"><span class="toc-number">8.2.</span> <span class="toc-text">Iterators</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#training-the-system"><span class="toc-number">8.3.</span> <span class="toc-text">Training the System</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%99%84%E5%8A%A0%E7%BB%84%E4%BB%B6-bpe-search-averaging"><span class="toc-number">9.</span> <span class="toc-text">附加组件: BPE, Search,
Averaging</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%93%E6%9E%9C"><span class="toc-number">10.</span> <span class="toc-text">结果</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#attention-visualization"><span class="toc-number">10.1.</span> <span class="toc-text">Attention Visualization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#encoder-self-attention"><span class="toc-number">10.2.</span> <span class="toc-text">Encoder Self Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#decoder-self-attention"><span class="toc-number">10.3.</span> <span class="toc-text">Decoder Self Attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#decoder-src-attention"><span class="toc-number">10.4.</span> <span class="toc-text">Decoder Src Attention</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="toc-number">11.</span> <span class="toc-text">参考链接</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-number">11.1.</span> <span class="toc-text">完整代码实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E9%87%8A%E8%AF%B4%E6%98%8E%E5%8F%82%E8%80%83%E7%BD%91%E9%A1%B5"><span class="toc-number">11.2.</span> <span class="toc-text">注释说明参考网页</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2024/01/08/%20%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/" title="“Embodied AI”前言"><img src="https://github.com/Zeying-Gong/web-img/raw/main/background/coder_openai.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="“Embodied AI”前言"/></a><div class="content"><a class="title" href="/2024/01/08/%20%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/" title="“Embodied AI”前言">“Embodied AI”前言</a><time datetime="2024-01-08T15:38:00.000Z" title="发表于 2024-01-08 23:38:00">2024-01-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/10/11/%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86/" title="同步电脑端和手机端日历教程（基于Outlook邮箱）"><img src="https://github.com/Zeying-Gong/web-img/raw/main/background/coder_openai.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="同步电脑端和手机端日历教程（基于Outlook邮箱）"/></a><div class="content"><a class="title" href="/2023/10/11/%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86/" title="同步电脑端和手机端日历教程（基于Outlook邮箱）">同步电脑端和手机端日历教程（基于Outlook邮箱）</a><time datetime="2023-10-11T14:28:00.000Z" title="发表于 2023-10-11 22:28:00">2023-10-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/" title="Transformer 零基础解析教程，完整版代码最终挑战（4/4）"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/harvard.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 零基础解析教程，完整版代码最终挑战（4/4）"/></a><div class="content"><a class="title" href="/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/" title="Transformer 零基础解析教程，完整版代码最终挑战（4/4）">Transformer 零基础解析教程，完整版代码最终挑战（4/4）</a><time datetime="2023-02-05T03:53:10.000Z" title="发表于 2023-02-05 11:53:10">2023-02-05</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/" title="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/simpletransformers-logo.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）"/></a><div class="content"><a class="title" href="/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/" title="Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）">Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）</a><time datetime="2023-01-31T09:00:10.000Z" title="发表于 2023-01-31 17:00:10">2023-01-31</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/" title="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）"/></a><div class="content"><a class="title" href="/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/" title="Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）">Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）</a><time datetime="2023-01-11T09:00:10.000Z" title="发表于 2023-01-11 17:00:10">2023-01-11</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By 空之影</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script defer src="https://cdn.jsdelivr.net/gh/CodeByZach/pace/pace.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>