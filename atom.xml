<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张乐盈的技术博客</title>
  
  <subtitle>欢迎来到我的博客!</subtitle>
  <link href="https://le-ying.github.io/atom.xml" rel="self"/>
  
  <link href="https://le-ying.github.io/"/>
  <updated>2025-05-31T14:55:51.154Z</updated>
  <id>https://le-ying.github.io/</id>
  
  <author>
    <name>张乐盈</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>我的硬件项目</title>
    <link href="https://le-ying.github.io/2025/05/31/%E6%88%91%E7%9A%84%E7%A1%AC%E4%BB%B6%E9%A1%B9%E7%9B%AE/"/>
    <id>https://le-ying.github.io/2025/05/31/%E6%88%91%E7%9A%84%E7%A1%AC%E4%BB%B6%E9%A1%B9%E7%9B%AE/</id>
    <published>2025-05-31T13:21:53.000Z</published>
    <updated>2025-05-31T14:55:51.154Z</updated>
    
    <content type="html"><![CDATA[<h1 id="我的项目经历--目录">我的项目经历--目录</h1><h2 id="硬件控制项目">硬件控制项目</h2><ol type="1"><li><a href="#dog">玩具狗的多种开关方式</a></li><li><a href="#GPS">GPS欺骗</a></li><li><a href="#Arduino">基于Arduino的音乐播放器</a></li></ol><h2 id="gps欺骗"><a name='GPS'></a>GPS欺骗</h2><p>2019.9 - 2019.11</p><hr /><h3 id="描述"><a name='-1'></a>描述：</h3><p>在Linux环境下，应用GPS卫星定位的手机,通过HackRFOne发射欺骗信号，实现点到点欺骗或轨迹欺骗，可在1、2分钟内成功欺骗到指定位置在指定轨迹内依据给定的加速度、速度进行不间断运动。</p><p>用到的编程语言和软件工具有：</p><p>（1）硬件：HackRF One -- 带 TCXO 时钟模块和天线，用于发射GPS信号 (2)软件： | 软件 | 作用 | | ---- | ---- | | Google Earth |选中欺骗地点，勾画目标轨迹 | | SatGen | 目标轨迹并存储为运动路径 | |gps-sdr-sim | 采样数据文件，生成GPS数据源 | | Gnuradio |流程图式运行GPS欺骗的程序 | | hackrf-tools |通过hackrf_transfer函数，在命令行运行GPS欺骗 |</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/gps%20mind.png" /></p><h3 id="成绩"><a name='-1'></a>成绩：</h3><p>实际手机位于广州大学城生活区某一定点静止不动，将定位欺骗至1千公里外的上海交大的操场跑道上变速跑步,全程精确度5m以内。</p><p><iframe height=498 width=510 src='https://player.youku.com/embed/XNDg0MTAwNDUyOA==' frameborder=0 allowfullscreen></iframe></p><h2id="玩具狗的多种开关方式"><a name='dog'></a>玩具狗的多种开关方式</h2><p>2020.11 - 2021.1</p><hr /><h3 id="描述-1"><a name='-1'></a>描述：</h3><p>根据玩具电子狗，通过其电路图进行相应修改，可得到不同开关相应方式，除了下面视频外也已经实现磁控、小程序控制、蓝牙控制等方式</p><p>用到的硬件模块有：</p><ol type="1"><li>玩具电子狗 -- 具备基本行走，吠叫功能</li><li>电路板 -- 实现不同方式开关并焊接电路 （3) 蓝牙开关模块 --具有微信小程序控制系统</li></ol><h3 id="成绩-1"><a name='-1'></a>成绩：</h3><p><strong>键控开关方式</strong></p><iframe height="498" width="510" src="https://player.youku.com/embed/XNDg5NzI5MDU0MA==" frameborder="0" allowfullscreen></iframe><p><strong>温控开关方式</strong></p><iframe height="498" width="510" src="https://player.youku.com/embed/XNDg5NzI5MDcyMA==" frameborder="0" allowfullscreen></iframe><h2id="基于arduino的音乐播放器"><a name='Arduino'></a>基于Arduino的音乐播放器</h2><p>2020.4 - 2020.6</p><hr /><h3 id="描述-2"><a name='-1'></a>描述：</h3><p>通过手机(串口)或电脑输入控制，实现了MP3的基本功能（曲目切换，多种播放模式，音量调节）。</p><p>用到的硬件模块有：</p><ol type="1"><li>Arduino -- 中央处理器</li><li>tf卡 -- 存储曲目 （3) 扬声器 -- 播放声音</li><li>LCD屏幕 -- 显示播放模式、曲目</li></ol><h3 id="成绩-2"><a name='-1'></a>成绩：</h3><iframe height="498" width="510" src="https://player.youku.com/embed/XNDg0MTAzMDQwOA==" frameborder="0" allowfullscreen></iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;我的项目经历--目录&quot;&gt;我的项目经历--目录&lt;/h1&gt;
&lt;h2 id=&quot;硬件控制项目&quot;&gt;硬件控制项目&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;a href=&quot;#dog&quot;&gt;玩具狗的多种开关方式&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;#GPS&quot;&gt;</summary>
      
    
    
    
    <category term="总结" scheme="https://le-ying.github.io/categories/%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="科研" scheme="https://le-ying.github.io/tags/%E7%A7%91%E7%A0%94/"/>
    
    <category term="项目" scheme="https://le-ying.github.io/tags/%E9%A1%B9%E7%9B%AE/"/>
    
  </entry>
  
  <entry>
    <title>我的软件项目</title>
    <link href="https://le-ying.github.io/2025/05/31/%E6%88%91%E7%9A%84%E8%BD%AF%E4%BB%B6%E9%A1%B9%E7%9B%AE/"/>
    <id>https://le-ying.github.io/2025/05/31/%E6%88%91%E7%9A%84%E8%BD%AF%E4%BB%B6%E9%A1%B9%E7%9B%AE/</id>
    <published>2025-05-31T03:10:23.000Z</published>
    <updated>2025-05-31T14:55:40.820Z</updated>
    
    <content type="html"><![CDATA[<h1 id="我的项目经历--目录">我的项目经历--目录</h1><h2 id="软件编程项目">软件编程项目</h2><ol type="1"><li><a href="#ChatGPT">可上传私人文档的ChatGPT</a></li><li><a href="#Python">Python爬取国家统计数据</a></li><li><a href="#app">app制作与安全性分析</a></li><li><a href="#safety">绕过认证系统实验</a></li><li><a href="#GUI">手写数字识别GUI</a></li><li><a href="#pointcloud">弱监督条件下的点云语义理解</a></li><li><a href="#map">疫情地图小程序</a></li></ol><h2id="可上传私人文档的chatgpt"><a name='ChatGPT'></a>可上传私人文档的ChatGPT</h2><p>2023.6</p><hr /><h3 id="描述"><a name='-1'></a>描述：</h3><p>搭建一个支持上传自定义知识库的ChatGPT（包含文档如 docx, txt, pdf,jpg等等，甚至可以直接上传文件夹），支持中英双语。采用的解决方案是Longchain+ChatGLM</p><p>用到的编程语言和软件工具有：</p><p>（1）阿里云机器学习PAI平台</p><p>ChatGLM的参数量（80亿）比正版GPT-3（1750亿）要少，所以可以下载到本地运行，支持中英双语，并且可以通过源代码更换各部分API接口，硬件配置最低要求为6 GB显存的GPU。</p><table><thead><tr class="header"><th>量化等级</th><th>最低 GPU 显存（推理）</th><th>最低 GPU 显存（高效参数微调）</th></tr></thead><tbody><tr class="odd"><td>FP16（无量化）</td><td>13 GB</td><td>14 GB</td></tr><tr class="even"><td>INT8</td><td>8 GB</td><td>9 GB</td></tr><tr class="odd"><td>INT4</td><td>6 GB</td><td>7 GB</td></tr></tbody></table><p>如果手头的硬件条件暂时不满足，而且想要免费使用，可以考虑白嫖阿里云的机器学习PAI 平台，使用 A10 显卡，免费试用活动页 <ahref="https://free.aliyun.com/">https://free.aliyun.com/</a>，申请教程可以参考<ahref="https://mp.weixin.qq.com/s/ymAsufnnk5tbUNobmgNtBw">https://mp.weixin.qq.com/s/ymAsufnnk5tbUNobmgNtBw</a></p><p>（2）Python -- 实现算法逻辑</p><p>项目实现原理如下图所示，过程包括加载文件 -&gt; 读取文本 -&gt;文本分割 -&gt; 文本向量化 -&gt; 问句向量化 -&gt;在文本向量中匹配出与问句向量最相似的top k个文本 -&gt;匹配出的文本作为上下文和问题一起添加到 prompt 中 -&gt; 提交给 LLM生成回答。</p><p><imgsrc="https://github.com/imClumsyPanda/langchain-ChatGLM/raw/master/img/langchain+chatglm.png" /></p><p>从文档处理角度来看，实现流程如下：</p><p><imgsrc="https://github.com/imClumsyPanda/langchain-ChatGLM/raw/master/img/langchain+chatglm2.png" /></p><h3 id="成绩"><a name='-1'></a>成绩：</h3><p>做了一个演示视频</p><p><iframe height=498 width=510 src="//player.bilibili.com/player.html?aid=529428129&bvid=BV1uu411W779&cid=1153998729&page=1" frameborder=0 allowfullscreen></iframe></p><h2id="python爬取国家统计数据"><a name='Python'></a>Python爬取国家统计数据</h2><p>2021.1</p><hr /><h3 id="描述-1"><a name='-1'></a>描述：</h3><p>独立完成，爬取“国家统计局”八个省份、六个季度的城乡居民收支基本情况</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/crawl%20mind.jpg" /></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/crawl2.jpg" /></p><p>用到的编程语言和软件工具有：</p><p>（1）Python -- 通过panda库实现爬虫功能，通过xlwings库实现表格处理</p><h3 id="成绩-1"><a name='-1'></a>成绩：</h3><p>爬取国家统计局八个省份、六个季度的表格数据到excel表格中，同时代码可筛去无效数据，自动整理excel表格，通过xlwings库实现数据居中、自适应列宽等功能。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/crawl3.jpg" /></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/crawl4.jpg" /></p><h2 id="app制作与安全性分析"><a name='app'></a>app制作与安全性分析</h2><p>2019.10 - 2020.1</p><hr /><h3 id="描述-2"><a name='-1'></a>描述：</h3><p>app实现要求:该app具有用户/口令登录功能，并可供使用者注册。注册时口令只作长度限制（如8位长度），但强度暂不作要求。用户名/口令保存在手机上，口令保存时作加密处理（自行选择加密算法）。</p><p>功能比较简单，弹出一浮窗，显示app需要获取存储空间、设备信息、地理位置权限的提示，可选择授权或拒绝。通过在手机上运行此app，注册若干个账号，口令设置时有强口令，也有弱口令，然后分析其安全性，加以改进。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/app1.png" /></p><p>客户端登录功能的相关代码（Kotlin）：</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/app2.png" /></p><p>获取存储空间、设备信息、地理位置权限这些权限的相关语句：</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/app3.png" /></p><p>Androbugs分析截图：</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/app5.png" /></p><p>分析后修改了原有app的注册/登录认证方式，采用OAuth2规范中的授权码模式：</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/app6.png" /></p><p>将外部存储改为内部存储：</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/app7.png" /></p><p>用到的编程语言和软件工具有：</p><p>（1）Kotlin -- 通过Android Studio实现app功能 （2）Androbugs --分析app安全性</p><h3 id="成绩-2"><a name='-1'></a>成绩：</h3><p>完整的运行视频如下：</p><p><iframe height=498 width=510 src='https://player.youku.com/embed/XNDg0MTQwNjUwOA==' frameborder=0 allowfullscreen></iframe></p><h2 id="绕过认证系统实验"><a name='safety'></a>绕过认证系统实验</h2><p>2019.9</p><hr /><h3 id="描述-3"><a name='-1'></a>描述：</h3><p>很多商场、饭店的商业WIFI采用了WEBPortal认证方式，但有些认证系统存在漏洞，可以利用 DNS TUNNEL绕过网关计费系统。存在这种漏洞的商业WIFI环境，并且可验证能够利用 DNSTUNNEL 穿越网关计费系统。</p><p>DNSTunnel真正用来“免密上网”，其实不太实际。尽管我们组已经“砍掉了”云服务器的开支（把代理服务器搬到本地来进行了），结果整个实验还是花掉了6块钱来购买域名。</p><p>用到的编程语言和软件工具有：</p><p>（1）树莓派 -- 搭建本地代理服务器 （2）Portal --拓扑结构分析和DNS仿真配置</p><h3 id="成绩-3"><a name='-1'></a>成绩：</h3><p>整个实验其实是告诉我们：黑客会“见缝插针”，DNS这样专门用于域名查询的协议，也可以被拿来传输数据。若将来需要做网络应用层的协议设计、维护工作，一定要加倍小心，在网络安全方面要非常谨慎。另外对于个人来说，如果连接到公共网络，一定要提高警惕，谨防“高科技偷窃”，因为我们难以知道黑客下一个目标是哪里。</p><iframe height="498" width="510" src="https://player.youku.com/embed/XNDg5NzI1OTY0OA==" frameborder="0" allowfullscreen></iframe><h2 id="手写数字识别gui"><a name='GUI'></a>手写数字识别GUI</h2><p>2020.11 - 2021.1</p><hr /><h3 id="描述-4"><a name='-1'></a>描述：</h3><p>不使用框架，实现手写数字识别GUI开发</p><p>用到的编程语言和软件工具有：</p><p>（1）Python --开发GUI界面（基于Qt5），涉及基本bp算法实现和正则化（BN，L2正则化，RMSProp）等优化算法，并实现pyqt界面及三个功能:mnist中抽取识别,上传图片识别,画板手写识别</p><h3 id="成绩-4"><a name='-1'></a>成绩：</h3><p><iframe height=498 width=510 src='https://player.youku.com/embed/XNDg0MTAwMDg3Mg==' frameborder=0 allowfullscreen></iframe></p><h2id="弱监督条件下的点云语义理解"><a name='pointcloud'></a>弱监督条件下的点云语义理解</h2><p>2020.10 - 2021.1</p><hr /><h3 id="描述-5"><a name='-1'></a>描述：</h3><p>为解决三维点云语义分割中数据标注昂贵的问题，尝试使用弱监督学习的方法进行研究。进行了论文综述，同时复现了“PointNet++”代码。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/pointcloud%20mind.png" /></p><p>用到的编程语言和软件工具有：</p><p>（1）Python -- 通过Jupyter Notebook复现代码</p><h3 id="成绩-5"><a name='-1'></a>成绩：</h3><p>基于百度AI平台的PaddlePaddle框架，对十组家具图片生成的无序点云进行分类处理，复现了“PointNet++”论文中91.9%的准确率。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/point%20cloud2.jpg" /></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/point%20cloud3.jpg" /></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/point%20cloud4.jpg" /></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/point%20cloud5.jpg" /></p><h2 id="疫情地图小程序"><a name='map'></a>疫情地图小程序</h2><p>2020.6</p><hr /><h3 id="描述-6"><a name='-1'></a>描述：</h3><p>疫情期间做的一个疫情地图，分为国内、国外两个板块，每个板块分为当日累计疫情、当日新增疫情两个子板块，引用了开课吧的数据源，颜色越深说明感染人数越多。</p><p>用到的编程语言和软件工具有：</p><p>（1）html -- 引用开课吧数据源，尝试进行数字可视化</p><h3 id="成绩-6"><a name='-1'></a>成绩：</h3><iframe height="498" width="510" src="https://player.youku.com/embed/XNDg5NzI4NDU0MA==" frameborder="0" allowfullscreen></iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;我的项目经历--目录&quot;&gt;我的项目经历--目录&lt;/h1&gt;
&lt;h2 id=&quot;软件编程项目&quot;&gt;软件编程项目&lt;/h2&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;a href=&quot;#ChatGPT&quot;&gt;可上传私人文档的ChatGPT&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a hre</summary>
      
    
    
    
    <category term="总结" scheme="https://le-ying.github.io/categories/%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="科研" scheme="https://le-ying.github.io/tags/%E7%A7%91%E7%A0%94/"/>
    
    <category term="项目" scheme="https://le-ying.github.io/tags/%E9%A1%B9%E7%9B%AE/"/>
    
  </entry>
  
  <entry>
    <title>Habitat 3.0 环境配置及报错完美解决（含Linux、MacOS系统）</title>
    <link href="https://le-ying.github.io/2024/01/09/%20Habitat%E9%85%8D%E7%BD%AE/"/>
    <id>https://le-ying.github.io/2024/01/09/%20Habitat%E9%85%8D%E7%BD%AE/</id>
    <published>2024-01-09T02:57:00.000Z</published>
    <updated>2024-02-27T05:39:36.357Z</updated>
    
    <content type="html"><![CDATA[<p>最近在入门 <code>Embodied AI</code> ，在实验室中了解到<strong>Meta</strong>的Habitat模拟器。这是一个用于agent仿真的模拟平台，并且刚出了<strong>Habitat3.0</strong>版本。无奈新版本嘛，难免有各种各样的bug，刚好网上也找不到太多资料，于是我总结了habitat-lab开发库配置过程中遇到的一部分bug。一方面给自己总结记录，另一方面帮助大家避开一些坑。</p><h1 id="linux环境配置ubuntu-22.04-gpu-3090">Linux环境（配置Ubuntu 22.04+ GPU 3090）</h1><p>笔者一开始是打算用自己的windows机器跑下实例代码，发现居然没法<code>conda install habitat-sim</code>，看样子不支持windows环境。于是用linux服务器进行接下来的实验，参考<ahref="https://github.com/facebookresearch/habitat-lab/tree/main">habitat-lab的官方github主页</a>进行安装。</p><ol type="1"><li>创建conda环境</li></ol><p>我们采用默认的conda安装方式，先执行</p><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">conda create -n habitat python=3.9 cmake=3.14.0</span><br><span class="line">conda activate habitat</span><br></pre></td></tr></table></figure><ol start="2" type="1"><li>配置底层模拟器</li></ol><p><strong>habitat-sim</strong>，也就是habitat-lab的核心模拟器。<ahref="https://github.com/facebookresearch/habitat-sim">habitat-sim官方</a>给出了几种安装方式（如果没有显示器也不一定要下载headless版本，下默认版本然后通过NoMachine远程连接查看桌面亲测可行）。那么我们这里就跟着habitat-lab的官方步骤往下走：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">conda install habitat-sim withbullet -c conda-forge -c aihabitat</span><br></pre></td></tr></table></figure><ol start="3" type="1"><li>安装开发依赖</li></ol><p>这一步官方考虑的是让用户gitclone一个稳定版本的分支，但是事实上并没有这个分支，这个问题会让后续的训练运行出错，这里暂且埋一手伏笔<em>[1]</em>。</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone --branch stable https://github.com/facebookresearch/habitat-lab.git</span><br><span class="line">cd habitat-lab</span><br><span class="line">pip install -e habitat-lab  # install habitat_lab</span><br><span class="line">pip install -e habitat-baselines  # install habitat_baselines</span><br></pre></td></tr></table></figure><ol start="4" type="1"><li>安装测试数据</li></ol><p>第一，需要先配置hugging face的ssh密钥，步骤见<ahref="https://huggingface.co/blog/zh/password-git-deprecation">huggingface官方说明</a>。事实上这里引用了habitat-sim库中的<ahref="https://github.com/facebookresearch/habitat-sim/blob/main/src_python/habitat_sim/utils/datasets_download.py">datasets_downloads</a>文件，并且根据这个文件，我们可以看到里面定义了每个数据对应的下载网址，因此如果无法科学上网也可以自行下载后放在指定位置（按以上的默认命令会下到habitat-lab/data/version_data，而不是habitat-lab/data/）。</p><p>随后运行该指令，如果数据集不完整，也不会继续安装，需要把原路径的文件删掉从头安装。如果数据集安装完整了，就会生成symlink，如下图。<strong>注意提示</strong><code>Replace versioned data?</code><strong>的时候务必选n，否则会再下载一次。</strong></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/habitat_configuration/figure1.png" /></p><p>第二，安装（点导航）测试数据集的时候，通过搭梯子也可能没法下载，需要直接在<ahref="https://github.com/facebookresearch/habitat-sim/blob/main/src_python/habitat_sim/utils/datasets_download.py">datasets_downloads</a>找到对应网址，http://dl.fbaipublicfiles.com/habitat/habitat-test-pointnav-dataset_v1.0.zip，这个链接复制到浏览器就可以直接下载，同样放到路径habitat-lab/data/version_data</p><ol start="5" type="1"><li>测试配置环境</li></ol><p>有两种方式的测试方法，分为非交互式和交互式（根据用户输入进行活动交互）</p><ul><li>非交互式：运行<code>python examples/example.py</code>，如果没有在前一步安装数据，这一步就会自动安装，最后提示<code>Episode finished after 1 steps.</code>就算成功。</li><li>交互式：需要先安装依赖<code>pip install pygame==2.0.1 pybullet==3.0.4</code>，随后运行<code>python examples/example.py --never-end</code>。按理说成功的话会显示一个交互式的pygame页面，可操纵fetchrobot在场景中移动。但是Ubuntu 20.04及以上的机子会显示以下报错。</li></ul><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">X Error of failed request:  BadAccess (attempt to access private resource denied)</span><br><span class="line">  Major opcode of failed request:  152 (GLX)</span><br><span class="line">  Minor opcode of failed request:  5 (X<span class="emphasis">_GLXMakeCurrent)</span></span><br><span class="line"><span class="emphasis">  Serial number of failed request:  98</span></span><br><span class="line"><span class="emphasis">  Current serial number in output stream:  98</span></span><br></pre></td></tr></table></figure><p>官网中也提到了这个问题还没解决，并且根据<ahref="https://github.com/facebookresearch/habitat-lab/issues?q=is%3Aissue+is%3Aopen+badaccess">github官网相关的Issues</a>，我们可以推断在短期内不太可能解决这个问题，因此我换了MacOS系统来解决交互测试的问题。</p><h1 id="macos环境配置mac-m1-pro">MacOS环境（配置Mac M1 Pro）</h1><ol type="1"><li>创建conda环境</li></ol><p>因为我用了苹果自带芯片M1（arm64架构），不同于intelx86_64，所以conda中有些包下不到对应版本很可能是这个问题，对于这种情况需要创建基于x86架构平台的conda环境。</p><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">CONDA<span class="emphasis">_SUBDIR=osx-64 conda create -n habitat python=3.9 cmake=3.14.0</span></span><br><span class="line"><span class="emphasis">conda activate habitat</span></span><br></pre></td></tr></table></figure><ol start="2" type="1"><li>配置底层模拟器（同上述Linux环境操作）</li></ol><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install habitat-sim withbullet -c conda-forge -c aihabitat</span><br></pre></td></tr></table></figure><ol start="3" type="1"><li>安装开发依赖（同上述Linux环境操作）</li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone --branch stable https://github.com/facebookresearch/habitat-lab.git</span><br><span class="line">cd habitat-lab</span><br><span class="line">pip install -e habitat-lab</span><br><span class="line">pip install -e habitat-baselines</span><br><span class="line">conda update --all</span><br></pre></td></tr></table></figure><ol start="4" type="1"><li>安装测试数据（同上述Linux环境操作）</li></ol><ul><li><p>安装3D测试场景:</p><p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python -m habitat_sim.utils.datasets_download --uids habitat_test_scenes --data-path data/</span><br></pre></td></tr></table></figure></p></li><li><p>安装（点导航）测试数据集:</p><p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python -m habitat_sim.utils.datasets_download --uids habitat_test_pointnav_dataset --data-path data/</span><br></pre></td></tr></table></figure></p></li></ul><ol start="5" type="1"><li>测试配置环境</li></ol><ul><li>非交互式：运行<code>python examples/example.py</code>，同样是提示<code>Episode finished after 1 steps.</code>就算成功。</li><li>交互式：需要先安装依赖<code>pip install pygame==2.0.1 pybullet==3.0.4</code>，随后运行<code>python examples/example.py --never-end</code>。按理说成功的话会显示一个交互式的pygame页面，可操纵fetchrobot在场景中移动。</li></ul><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/habitat_configuration/figure2.png" /></p><p>如果你的代码能正常执行，那么恭喜你不用看后文了。</p><p>如果你的代码在测试配置环境中报错，很可能属于以下两种，（Github官网和互联网的相关资料很少），最后总结出完美的解决方法如下：</p><h3 id="macos环境特殊报错">MacOS环境特殊报错</h3><ol type="1"><li><strong>测试代码报错如下：</strong></li></ol><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">OMP: Error #15: Initializing libiomp5.dylib, but found libomp.dylib already initialized.</span><br><span class="line">OMP: Hint This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can degrade performance or cause incorrect results. The best thing to do is to ensure that only a single OpenMP runtime is linked into the process, e.g. by avoiding static linking of the OpenMP runtime in any library. As an unsafe, unsupported, undocumented workaround you can set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the program to continue to execute, but that may cause crashes or silently produce incorrect results. For more information, please see http://www.intel.com/software/products/support/.</span><br></pre></td></tr></table></figure><p>为了防止该报错，Stackoverflow有多种方法，</p><p>方法一：在终端（Terminal）中关闭警告（临时的解决方法）</p><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">export KMP<span class="emphasis">_DUPLICATE_</span>LIB<span class="emphasis">_OK=True </span></span><br></pre></td></tr></table></figure><p>方法二：针对intel mac，参考<ahref="https://stackoverflow.com/questions/55714135/how-can-i-fix-an-omp-error-15-initializing-libiomp5-dylib-but-found-libomp">这个链接</a>，键入以下命令解决</p><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">conda uninstall intel-openmp</span><br><span class="line">conda install -c intel openmp</span><br><span class="line">conda install nomkl</span><br></pre></td></tr></table></figure><p>方法三：针对m芯片 mac</p><p>对于苹果芯片，以上两种方法基本都没用。这是因为苹果自带芯片比较特殊，如果下载的包（主要是numpy,spicy, matplotlib）含有不同的C++编译方法，并行编程会报错。我们参考<ahref="https://stackoverflow.com/questions/53014306/error-15-initializing-libiomp5-dylib-but-found-libiomp5-dylib-already-initial">Stackoverflow的问答</a>找到可能的原因，numpy,scipy, pandas,tensorflow等几个包可能使用了不同的并行编程方式，优先解决它们这几个包的问题。对我来说，我运行了以下代码修复：</p><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">conda update --all</span><br><span class="line">pip uninstalll numba</span><br><span class="line">conda install numba</span><br></pre></td></tr></table></figure><p>最终归因于numba使用了pip下载，和conda下载的numpy不兼容导致这个问题，该报错完美解决。</p><ol start="2" type="1"><li><strong>测试代码报错</strong></li></ol><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">AttributeError: &#x27;RearrangeSim&#x27; object has no attribute &#x27;<span class="emphasis">_initialized’</span></span><br></pre></td></tr></table></figure><p>我的代码一开始没有这个报错，后来似乎是因为habitat官方把stable版本的代码更新不全导致，因为我直接在原来的环境使用了以下命令解决：</p><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">conda install habitat-sim withbullet -c conda-forge -c aihabitat-nightly</span><br></pre></td></tr></table></figure><p>区别在于原来最后的部分是aihabitat，多了nightly部分，代表官方每天发布的最新版本。</p><ol start="3" type="1"><li><strong>安装pybullet报错</strong></li></ol><p>交互测试前需要通过<code>pip install pygame==2.0.1 pybullet==3.0.4</code>下载依赖，单独下载前一个是ok的，后一个的下载却报错如下（只截取最后一句）：</p><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">ERROR: Could not build wheels for pybullet, which is required to install pyproject.toml-based projects</span><br></pre></td></tr></table></figure><p>直接换下载源（pip换成conda）解决，默认下最新版本就可以了</p><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">conda install pybullet</span><br></pre></td></tr></table></figure><p>其他杂七杂八的bug网上应该查得到，这里不一一赘述了，habitat的讨论帖实在太少，欢迎留言评论交流~</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;最近在入门 &lt;code&gt;Embodied AI&lt;/code&gt; ，在实验室中了解到
&lt;strong&gt;Meta&lt;/strong&gt;
的Habitat模拟器。这是一个用于agent仿真的模拟平台，并且刚出了&lt;strong&gt;Habitat
3.0&lt;/strong&gt;版本。无奈新版</summary>
      
    
    
    
    <category term="笔记" scheme="https://le-ying.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="科研" scheme="https://le-ying.github.io/tags/%E7%A7%91%E7%A0%94/"/>
    
    <category term="Embodied AI" scheme="https://le-ying.github.io/tags/Embodied-AI/"/>
    
  </entry>
  
  <entry>
    <title>“Embodied AI”前言 -- 婴儿给AI时代的六个启示</title>
    <link href="https://le-ying.github.io/2024/01/08/%20%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/"/>
    <id>https://le-ying.github.io/2024/01/08/%20%E5%85%B7%E8%BA%AB%E6%99%BA%E8%83%BD/</id>
    <published>2024-01-08T15:38:00.000Z</published>
    <updated>2024-02-27T05:39:29.181Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章深度解析了美国自然科学领域的一篇论文，<ahref="https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf">《<strong>Thedevelopment of embodied cognition: six lessons frombabies</strong>》</a> ，其中的定义被 <strong>Meta</strong>认为是目前火热的 "Embodied AI" (具身智能)的官方定义。有趣的是，最近我还看到另一篇文章描述了人类的经验并不一定适用于机器，也就是<strong>OpenAI</strong> 推崇的 <ahref="https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf">《<strong>TheBitter Lesson</strong>》</a> 。这两种观点都很值得参考借鉴。</p><h3 id="embodied-ai的定义">“Embodied AI”的定义：</h3><p>一种先进的人工智能类型，它通过与周围环境的互动来实现学习。这种AI不依赖于静态数据集（如ImageNet、COCO、VQA等），而是涉及具有物理或虚拟实体的智能系统（例如robotsor trained assistants）的研究。</p><h3 id="embodied-ai和internet-ai的区别">“Embodied AI”和“InternetAI”的区别：</h3><ol type="1"><li><strong>Embodied AI（实体化人工智能）</strong>:<ul><li>实体化AI指的是那些具有物理实体的AI系统，比如机器人或者具有传感器和执行器的智能设备。</li><li>这类AI通过与现实世界直接互动来学习和适应。它们依赖于对物理环境的感知（通过摄像头、触觉传感器等）和对环境的物理作用（通过机械手臂、移动机构等）。</li><li>实体化AI的一个核心概念是智能不仅来源于信息处理，还来自于身体与环境的动态互动。</li><li>应用示例包括自主驾驶汽车、工业机器人、服务机器人等。</li></ul></li><li><strong>Internet AI（互联网人工智能）</strong>:<ul><li>互联网AI主要指的是在互联网环境中运行的AI系统，如搜索引擎、推荐系统、语音助手等。</li><li>这类AI通常处理大量的数字数据（如文本、图像、用户行为数据），并在云计算环境中执行复杂的数据分析和模式识别任务。</li><li>互联网AI更多地关注信息处理、数据挖掘、用户行为分析等，而不涉及物理世界的直接交互。</li><li>应用示例包括搜索引擎优化、个性化内容推荐、社交媒体分析等。</li></ul></li></ol><h3 id="embodied-ai的基础-具身假设embodiment-hypothesis">EmbodiedAI的基础-具身假设（Embodiment Hypothesis）</h3><p>The embodiment hypothesis is the idea that intelligence emerges inthe interaction of an agent with an environment and as a result ofsensorimotor activity.</p><p>Habitat的Tutorial提到了具身假设的定义：智能是在主体与环境的相互作用中产生的，并且是感觉运动活动的结果。</p><p>这个定义来自2005年Indiana University的一篇论文，<ahref="https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf">《<strong>Thedevelopment of embodied cognition: six lessons frombabies</strong>》</a>，作者出自心理学系和计算机系，感觉二十年前的发现现在看也毫不过时，并且很有启发性，因此记录如下。文中的lesson我这里翻译为启发，【】中的内容是我自己的理解和笔记。</p><h2 id="overview这六个启发是什么">Overview—这六个启发是什么？</h2><ol type="1"><li><strong>多模态体验（Bemulti-modal）</strong>：婴儿通过感知和行动来学习，他们的感官系统协同工作。【多模态对任务表现有促进作用】</li><li><strong>渐进式发展（Beincremental）</strong>：婴儿逐渐发展，从不断变化的环境中学习。【模态的训练顺序很重要，应该由易到难学习】</li><li><strong>与物理世界的互动（Bephysical）</strong>：与物理世界的互动帮助婴儿组织感知、行动和思考。【可以通过我们与环境的位置、朝向等交互信息，省去大量推理过程】</li><li><strong>探索（Explore）</strong>：通过非目标导向的探索，婴儿发现新的问题和解决方案。【探索试错的过程有助于催发新的解决方案】</li><li><strong>社会学习（Besocial）</strong>：在成熟伙伴的指导和支持下进行的社会环境中学习。【与能够引导和帮助它们的成熟代理进行有效社会互动】</li><li><strong>语言学习（Learn alanguage）</strong>：语言作为一种符号系统，改变了儿童的思维方式和与世界的互动。【可通过某种语言系统，补充给机器人体验之外的知识，对场景、对象的语义进行监督学习，赋予象征性推理的思维能力】</li></ol><p>以下是论文全文解读，为了方便理解，笔者改成了由浅入深的Q&amp;A逻辑，并且略去表述模糊的图6。</p><h2 id="一多模态体验">一、多模态体验</h2><h3 id="原文描述">原文描述</h3><p>Babies’ experience of the world is profoundly multimodal. We proposethat multiple overlapping and time-locked sensory systems enable thedeveloping system to educate itself—without defined external tasks orteachers—just by perceiving and acting in the world.</p><p>婴儿体验世界的方式深刻地多模态。作者提出，多种重叠且时间锁定的感觉系统，使得发展中的系统能够在没有明确外部任务或教师的情况下，通过感知和行动在世界中自我教育。这意味着婴儿能够通过同时使用多种感觉通道来理解和学习环境，而这种能力对他们的认知发展至关重要。</p><h3id="multiple-overlapping-and-time-locked-sensory-systems具体是什么意思">Multipleoverlapping and time-locked sensory systems具体是什么意思？</h3><p>指的是婴儿的不同感觉系统（如视觉、听觉、触觉）在他们处理信息时是相互重叠和时间上同步的。例如，婴儿看到一个物体的同时，可能会触摸它，从而同时获取视觉和触觉信息。这种多感官的同步加强了他们对环境的理解。</p><h3id="人们为什么需要这么多感觉系统视觉听觉触觉嗅觉本体感觉平衡">人们为什么需要这么多感觉系统—视觉、听觉、触觉、嗅觉、本体感觉、平衡？</h3><p>原因在于神经结构中的"简并性"。</p><ul><li>简并性意味着单一功能可能由多种神经信号配置执行，并且不同神经簇参与多种功能。这导致系统即使失去某个组件也能正常运行。例如，失明的人通过其他感官仍能形成空间概念。</li><li>此外，简并性还使得感觉系统能够相互教育，无需外部老师。这一点在婴儿如何观察和学习自己行为的描述中得到体现。这些多模态体验是时间限制的且相互关联。</li></ul><p>第二个特点，被Edelman称为“再入”。这是一个涉及跨模态的多个同时表现形式的明确的相互关系。例如，当一个人体验一个苹果时，这种体验是视觉的，但也会唤起苹果的气味、味道、触感、重量，以及与苹果相关的一系列动作的感觉和运动。重要的是，这些多模态体验是有时间限制且彼此相关的。当手移动苹果时感觉到的变化与看到苹果移动时的变化是同步的。这种时间上锁定的相关性创造了一个强大的学习机制。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/babies/figure1.png" /></p><p>图1展示了四种相关的映射。其中一个是苹果的物理属性与视觉系统中的神经活动之间的映射。另一个映射是苹果的物理属性与触觉系统中的神经活动之间的映射。第三个和第四个映射是Edelman所说的再入映射：视觉系统的活动映射到触觉系统，触觉系统的活动映射到视觉系统。因此，刺激的两个独立映射——视觉和触觉——提供了对世界的不同解释，并且通过实时相关联，它们相互教育。同时，视觉系统通过苹果上的阴影和纹理的变化以及点的共线运动被激活，而触觉系统则通过压力和纹理的同步变化被激活。在实时的每一步中，这些异质过程中的活动都相互映射，使系统能够在其自身活动中发现超越特定模态的高阶规律。</p><h3 id="那么婴儿给了我们什么启发">那么婴儿给了我们什么启发？</h3><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/babies/figure2.png" /></p><p>这个实验由Diamond进行，研究了婴儿如何从不同类型的盒子中取出隐藏的玩具。在实验中，玩具被藏在有一个开口的盒子下，这些盒子要么是不透明的（遮盖玩具），要么是透明的（婴儿可以看到盒子下的玩具）。关键的发现是，9个月大的婴儿更容易从不透明容器中取出玩具，而不是从透明容器中取出。透明容器的问题在于，婴儿会直接尝试通过透明表面去够玩具，而不是寻找并找到开口。</p><p>然而，如果婴儿有使用透明容器的经验，他们能够很快解决这个问题。Titzer、Thelen和Smith的研究提供了8个月大的婴儿一些不透明或透明的桶在家中玩耍。家长的唯一指示是把这些容器放在玩具箱中，让婴儿在玩耍时可以使用。这些婴儿在9个月大时接受了Diamond的任务测试。那些玩过不透明容器的婴儿无法从透明容器中取出物体，就像在原始的Diamond研究中一样。然而，那些玩过透明容器的婴儿则迅速找到了开口，并从透明盒子中取出了物体。</p><h3 id="对embodied-ai的启示">对embodied ai的启示？</h3><p>人类认知的许多概念、过程本质上是多模态的。即使在被明确设计为单模态的任务中，多种感觉模态也会对表现有所贡献。例如，视觉对象识别似乎会自动激活与该对象相关的动作。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/babies/figure3.png" /></p><p>在以上实验中，向成年人展示了一张水壶的图片，任务很简单：通过按按钮来指示所展示的对象是否为水壶（“是或否”）。Metrics是反应时间。尽管这是一项纯视觉对象识别任务。然而，如果按下按钮表示“是”的反应与投手的把手在同一侧，参与者识别物体的速度要快得多，就好像看到把手启动了（并准备好了）到达那一侧的运动反应一样。在使用几种不同方法的各种对象和任务中也报告了类似的结果。</p><p>一般来说，当做出的反应与物体上的真实动作相兼容时，人们在视觉识别任务中会更快。这些结果告诉我们，视觉识别与动作是一体的，用同样的内部语言。在可重入映射（reentrantmappings）的思想下，视觉识别就是这样建立起来的，并通过其与对象上的动作的时间锁定连接来进行教育。</p><h2 id="二渐进式发展">二、渐进式发展</h2><h3 id="原文描述-1">原文描述</h3><p>Babies develop incrementally, and they are not smart at the start. Wepropose that their initial prematurity and the particular path they taketo development are crucial to their eventual outpacing of the world'ssmartest AI programs.</p><p>婴儿起初并不聪明，他们是逐步发展的。作者认为，婴儿最初的不成熟和他们特定的发展路径对他们最终超越世界上最聪明的人工智能程序至关重要。</p><h3id="这段话或者这个观点想表达什么">这段话或者这个观点想表达什么？</h3><p>这意味着婴儿的早期经历，尽管看似不完善，但实际上是他们认知发展的基础，使他们能够通过逐步学习和适应来理解复杂的概念和任务。这种渐进式的发展方式为他们提供了稳定的基础，最终导致了认知能力的显著增强。</p><p><strong>原文中说到：</strong></p><p>传统上，机器学习和人类学习都集中在非渐进式学习任务上，也就是<strong>整个训练集在学习开始时就已固定，然后要么整体呈现，要么随机抽样</strong>。然而，这并不是儿童接触世界的方式。3个月大的婴儿的经历与1岁儿童的经历非常不同（且受限得多），而后者的经历又与2岁儿童的不同。所有迹象都表明，这些系统性的输入变化，即经历的范围和种类的变化是重要的，实际上它们决定了发展的结果。</p><h3 id="那么婴儿给了我们什么启发-1">那么婴儿给了我们什么启发？</h3><p>婴儿早期的经历受到感官系统和运动系统发展的强烈影响。出生时，听觉和视觉已经开始运作，但视觉受到婴儿焦距能力的限制。尽管如此，出生后不久，婴儿就会朝向声音的方向看。随后几个月，更微妙的声音属性开始控制视觉注意力，使婴儿看向与他们听到的声音匹配的视觉事件。例如，给4个月大的婴儿看两个弹跳球的视觉展示，他们会看与弹跳球声音时间同步的展示。听觉和视觉的耦合组织了婴儿的注意力，从而决定了他们学到的内容。实际上，没有听觉的儿童，聋儿童，展示出更容易改变和更无组织的视觉注意力。</p><p>婴儿在看和听的协调是第一课中强调的再入映射和多模态学习的一种形式。但对于第二课重要的是，这些相关性在发展过程中不是固定不变的。</p><ul><li>在观察和聆听世界3到4个月后，婴儿开始尝试抓取物体，多模态的相关性发生变化。一旦婴儿能够伸手抓取，他们就能够自己提供涉及视觉、触觉探索、自我运动的本体感知输入，以及触碰物体时产生的听觉体验。</li><li>经过几周到几个月在这种新的多模态环境中生活，包括坐着、看着、听着、伸手和操作物体，婴儿的经历——以及他们可用的相关性——再次发生了根本变化。</li><li>随着他们开始爬行，然后站立和走路。自我运动更加剧烈地改变了视觉和听觉输入的性质，并且证据表明它也深刻地改变了婴儿的认知发展。</li></ul><p><strong>具体例子涉及婴儿认知中一个被广泛研究的任务，即所谓的对象概念或A不是B任务（object-conceptor A-not-B task）。</strong></p><p>婴儿自身行为变化导致输入的转变，同样会导致认知发展的转变。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/babies/figure4.png" /></p><p>这是一种评估婴儿对对象时间和空间持续性理解的测试。在这个任务中，实验者在婴儿看得到的位置A藏一个玩具，然后在让婴儿搜索之前暂停3-5秒。婴儿通常会正确地伸手到位置A并找到玩具。重复几次后，实验者将玩具藏到新的位置B，再次暂停后让婴儿伸手。8-10个月大的婴儿会犯一个有趣的“错误”——他们会伸手到之前找到玩具的位置A，而不是看到玩具消失的地方。12个月以上的婴儿在关键的B位置搜索中则表现正确。这种行为的转变与自我移动(self-locomotion/ self-movement)的开始紧密相关。</p><h3id="为什么不同阶段的婴儿和自我移动相关">为什么不同阶段的婴儿和自我移动相关？</h3><p>研究显示，自我移动的出现与8到12个月大婴儿的搜索行为变化紧密相关。婴儿开始自我移动时，他们停止犯A-not-Btask的错误。更重要的是，实验者将那些还没有自我移动能力并且犯了错误的婴儿放在学步车里，发现他们比正常情况下早3 到 4 个月进行自我移动，这加速了婴儿在A不是B任务中成功搜索的发展。</p><p>这是因为自我移动——在物体上方、旁边、里面和周围移动——提供了新的体验和新的时空关系模式，改变了婴儿对物体、空间和自我的表征。总的来说，婴儿的经历——学习系统遇到的规律性——随着发展本身系统性地改变。婴儿在发展上的每一个成就——手眼协调、坐、爬、走——都为婴儿打开了全新的多模态规律性集合。</p><h3 id="对embodied-ai的启示-1">对embodied ai的启示？</h3><p><strong>有一个至关重要的问题：</strong>感官体验的顺序在最终结果中是否重要？是否可以从一开始就让一个婴儿同时听、看、伸手、走路，就像我们直接构建一个拥有两岁大的孩子智力的人工智能一样？</p><p>研究表明，感觉系统的发展顺序对动物的智能发展非常重要。不同动物的感觉系统发展顺序不同，这与它们适应环境的方式有关。例如，提前开启小猫的眼睛会影响它们嗅觉的发展和视觉与嗅觉的协调，而在猫头鹰中打乱听觉和视觉的发展顺序会影响它们的空间定位能力。构建生物智能的要素之⼀是以正确的方式安排训练体验。</p><p>在人类建模学习的尝试中，这个原则同样适用。当整个数据集一次性呈现时，神经网络有时无法学习任务，但当数据以由易到难的方式增量呈现时，神经网络会成功。这种做法被一些人批评为作弊。但对于我们这些研究真实婴儿智力如何形成的人来说，他们的想法似乎是正确的。</p><p><strong>【后续原文版本添加了这句补充】</strong>当然，在真正的发展中，这种训练经验的顺序影响并不是由外界强加给学习者的，而是作为发展本身的结果出现的。</p><h2 id="三与物理世界的互动">三、与物理世界的互动</h2><h3 id="原文描述-2">原文描述</h3><p>Babies live in a physical world, full of rich regularities thatorganize perception,action, and ultimately thought. The intelligence ofbabies resides not just insidethemselves but is distributed across theirinteractions and experiences in thephysical world. The physical worldserves to bootstrap higher mental functions.</p><p>婴儿生活在一个物理世界中，这个世界充满了丰富的规律，这些规律组织了感知、行动，最终是思想。婴儿的智力不仅存在于他们自身内部，还分布在他们与物理世界的互动和经验中。物理世界有助于引导更高级的心智功能的发展。</p><h3id="这和第一点多模态感知不是重复了吗都是说对外界的交互自我教育">这和第一点多模态感知不是重复了吗？都是说对外界的交互自我教育</h3><p>它们侧重的方面略有不同。第一点强调的是多感官体验和感官系统之间的交互，而第三点更多地关注于婴儿与物理世界的互动对他们认知发展的影响。</p><p>这一部分的观点是：不是所有知识都需要存在大脑中，有些知识通过身体和与世界的互动来学习。</p><ul><li>人类可以不自觉地活动双腿来走路，而不是靠大脑中的知识控制。<em>（让我想到最近斯坦福的模仿学习机器人MobileALOHA，可能使用模仿学习在达成这一点上有所帮助）</em></li><li>变化盲视（changeblindness）现象。人们不会记住眼前事物的细节，因为他们不需要记住他们仅仅是看着和看到的东西。</li><li>还有一项实验"Where is the dax?"，实验包括一个实验者在小孩面前展示两个物体，然后将物体放入容器中并将容器放在桌子上。实验者查看其中一个容器并说“我在这里看到一个dax”，但不向孩子展示容器中的物体。接着，从容器中取出物体，并问孩子哪个是“dax”。孩子需要根据听到的名字将其与先前见过的物体联系起来。(dax不是一个真实的名词)</li></ul><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/babies/figure5.png" /></p><h3 id="那么婴儿给了我们什么启发-2">那么婴儿给了我们什么启发？</h3><p>⼈们可以尝试多种解决方案，包括推理和记住哪些物体从哪些容器中出来，以及说话者提供名称时可能的意图。然而，证据表明，幼儿以更简单的方式解决这个问题</p><p>在“where is thedax”实验中，幼儿采用的策略相对简单，他们利用了对象和位置之间的联系。在这个任务中，幼儿利用了世界中一个深刻且基础重要的规律：一个真实的物体根据其独特的位置与其他物体区分开来；它必须与任何其他物体处于不同的位置。在Baldwin的实验中，关键因素在于实验程序的第一部分，一个物体被放在右边，另一个放在左边。容器也分别放在左右两边，而名字则是在实验者看向一个容器（例如右边的）时给出的。孩子通过将名字与该位置关联的物体联系起来来解决这个任务。</p><h3id="将物体与位置联系起来的方法有什么应用">将物体与位置联系起来的方法，有什么应用？</h3><p>人们在讲述故事中的一个主角时常常不自觉地用一只手做手势，而在讲另一个主角时用另一只手。通过手势和注意力的方向，他们将故事中的不同事件与同一个人物联系起来。</p><p>人们还将空间作为一种记忆法，通过看向过去事件的方向来帮助记住该事件。理查森和斯派维的好莱坞方块实验就展示了这一点。参与者在不同时间被展示四个不同的视频，每个视频来自不同的空间位置。后来，在没有视频的情况下，受试者被问及这些视频的内容。眼动追踪相机记录了人们回答这些问题时的注视位置，结果显示他们系统地看向之前呈现相关信息的方向。</p><p>这与所谓的指示性指针的概念有关，是感觉运动行为——人们看向哪里、看到什么、在哪里行动——在我们的认知系统中创造一致性的一个强有力的例子，它将相关的认知内容绑定在一起，将其与其他不同的内容区分开来。</p><h3 id="对embodied-ai的启示-2">对embodied ai的启示？</h3><p>这段话讲述的是所谓的指示性指针(deicticpointers)概念，它是感觉运动行为（比如我们看向哪里、看到什么、在哪里行动）如何在我们的认知系统中创造一致性、将相关的认知内容联系在一起并将它们与其他不同的内容分开的一个强有力的例子。总的来说，要将一个想法与另一个想法联系起来，不一定需要大量与内容相关的知识或推理系统。相反，有一种更简单的方式：就是利用我们所处的世界及我们身体对这个世界的指向。</p><p>【笔者注：简而言之，这意味着我们可以通过我们与环境的直接互动和身体动作来理解和组织我们的思维，而不必完全依赖于复杂的思维过程或显式的知识系统。】</p><h2 id="四探索">四、探索</h2><h3 id="原文描述-3">原文描述</h3><p>Babies explore – they move and act in highly variable and playfulways that are 3not goal-oriented and are seemingly random. In doing so,they discover newproblems and new solutions. Exploration makesintelligence open-ended andinventive.</p><p>婴儿进行探索——他们以高度变化和富有趣味性的方式移动和行动，这些行为并非目标导向，看似随机。通过这样做，他们发现了新的问题和新的解决方案。探索使得智力开放和创新。</p><h3id="一个没有目标的学生怎么学习具体例子">一个没有目标的学生怎么学习，具体例子？</h3><p>具体来说，代理或其设计者是否必须知道，需要学习什么才能进行学习？</p><p>来自人类发展的证据帮助我们解决了这个困境，证明婴儿可以通过探索或非目标导向的行动发现学习任务和这些任务的解决方案。在婴儿中，自发的运动创造了学习的任务和机会。一个关于抓取研究的优雅展示追踪了四个婴儿在3个月的时间里从不会抓取到会抓取的发展。观察到四种非常不同的发展模式。一些婴儿在不会抓取的时期几乎不抬手臂，但安静地观察世界。其他婴儿更为紧张和活跃，挥舞和拍打，总是在移动。这些不同的婴儿必须学会解决不同问题才能学会伸手去抓取物体。活跃的婴儿需要学会变得不那么活跃，降低他的手，把它们带到中线。安静的婴儿则需要学会变得更活跃，举起她的手，把它们从通常在身边的位置抬起。于是每个婴儿都找到了一个以探索运动空间开始的解决方案。</p><p>每个婴儿学习的基本发展模式是这样的：有吸引力的玩具的出现是令人兴奋的，激发了各种不同婴儿的不同行动。这些行动首先在形式或方向上没有明确的一致性。每个婴儿以其独特的方式接触到玩具——撞到、刷过或扫过它。这些接触时刻出现后，模式逐渐频繁重复。这个周期会重复几周：看到某个玩具而激动，行动，偶尔接触。经过几个周期，越来越稳定、有效的抓取形式出现。值得注意的是，每个孩子都找到了一个解决方案——并且最终收敛到非常相似的解决方案——通过遵循各自不同的发展路径。</p><p><strong>这给我们一个构建智能代理的启示：从时间锁定相关性构建可重入映射的多模态系统，只需要移动，广泛移动，甚至随机移动，就可以学习，并通过这种探索来发现任务和解决方案。</strong></p><p>（A multimodal system that builds reentrant maps from time-lockedcorrelations only needs to be set in motion, to move about broadly, evenrandomly, to learn and through such exploration to discover both tasksand solutions.）</p><h3 id="那么婴儿给了我们什么启发-3">那么婴儿给了我们什么启发？</h3><p>论文介绍了一个实验，婴儿共轭增强（infant conjugatereinforcement）。婴儿的脚踝通过一条丝带与头顶悬挂的移动物体相连。当婴儿开始自发地踢脚时，他们激活了移动物体。在几分钟内，他们学会了脚踢和移动物体的摇晃之间的关联，这提供了有趣的视觉和听觉刺激。移动物体对婴儿的动作做出共轭反应：婴儿踢得越多、动作越有力，移动物体产生的动作和声音就越多。在这种情况下，婴儿会增加他们的踢脚动作，超过基线水平（当婴儿仅仅看着一个不动的移动物体时的水平）。婴儿在发现他们的控制能力时，他们最初探索各种动作，并选择最优模式来使有趣的事件——移动物体的运动——发生。</p><p>尽管这是一个实验任务，而不是日常现实世界中的任务，但它是一个非常适合现实世界学习的模型。移动物体为婴儿提供了许多时间锁定的相关模式。更重要的是，婴儿通过自己的探索性运动模式发现了这些关系。婴儿自己的动作与移动物体有连续的关系；他们踢得越快越用力，移动物体就摇摆得越剧烈。对婴儿来说，这是一个非常吸引人的任务；他们会微笑和大笑，当关联被移除时，他们常常会生气。因此，这个实验程序，就像世界一样，提供了复杂多样、且永远不会完全重复的事件，但都与婴儿自己的动作完美时间锁定。正是探索、自发的非任务相关的运动，开始了这个过程。没有自发的运动，没有探索，婴儿就没有从移动物体中学到东西。</p><h3 id="对embodied-ai的启示-3">对embodied ai的启示？</h3><p>年轻的哺乳动物，包括儿童，花费大量时间在没有明显目标的行为上。他们移动、晃动、到处跑、弹跳物体、扔东西，通常以成熟的思维来看，这些行为似乎没有什么好处。<strong>然而，这种通常被称为玩耍的行为对于构建开放于新解决方案的创新智能形式至关重要。</strong></p><p>（However, this behavior, commonly called play, is essential tobuilding inventive forms of intelligence that are open to newsolutions.）</p><h2 id="五社会学习">五、社会学习</h2><h3 id="原文描述-4">原文描述</h3><p>Babies act and learn in a social world in which more mature partnersguide learning and add supporting structures to that learning.</p><p>婴儿在一个社交世界中行动和学习，在这个世界中，更成熟的伙伴引导学习并为这种学习提供支持结构。</p><h3 id="这里的social指的是什么">这里的social指的是什么？</h3><p>许多研究人员观察了母亲和婴儿的面对面互动，并报告了一种看起来非常像上文共轭增强的活动和学习模式，但增加了一个回传。母亲的面部表情和她们发出的声音与婴儿的行为紧密相关。当婴儿看着母亲的眼睛时，母亲会回望并微笑，并伴随着上升音调的声音。当婴儿微笑时，母亲也微笑。当婴儿发出咕哝声时，母亲也发出咕哝声。婴儿的面部动作从母亲那里创造出有趣的视觉和声音效果，就像他们的踢腿动作从连接的移动物体中创造出有趣的视觉和声音效果一样，这些连续性创造了一个激发和探索的环境。</p><h3 id="social-partner的交互和物理世界的交互有何不同">Socialpartner的交互和物理世界的交互有何不同?</h3><p>在最初的互动中，这种探索为婴儿学习母亲（或其他成熟的社交伙伴）的面部动作和发声之间的时间锁定对应关系创造了机会，使得婴儿的行为被其他人的模式所转变。关键的是，婴儿的母亲不仅仅对婴儿的行为做出共轭反应；她们还在其基础上将其转变为常规模式。</p><p>例如，非常早期的婴儿行为显示出自然行为节奏（兴奋和平静）为照顾者提供了一个框架，照顾者可以在这个框架内与婴儿进行互动，仿佛在进行对话。随着时间的推移，婴儿不仅被动地接受照顾者的互动模式，还主动地参与到这个过程中，通过他们对照顾者的反应来形成一种双向的交流。这种互动模式帮助婴儿学习并响应外界的刺激，逐步形成更复杂的社交和认知能力。</p><h3 id="那么婴儿给了我们什么启发-4">那么婴儿给了我们什么启发？</h3><p>一方面婴儿学会模仿父母的声音，另一方面父母也通过模仿婴儿来为这种学习提供结构！也就是说，父母不仅仅是对婴儿的微笑和声音作出反应；他们还模仿婴儿。这建立了一个循环模式：婴儿发声—→父母模仿→婴儿重复发声→父母的模仿，依此类推。这为婴儿学习（studyandfinetune）类似大人的的动作和声音创造了机会。简而言之，这个循环有助于加强突出某些模式，因为父母自然会选择那些他们认为有意义的模式。</p><p>除了模仿交互，照顾者还提供多模态支持来帮助巩固早期语言学习。当父母向幼儿介绍一个物体并命名时，他们会动用一系列感觉运动支持来吸引孩子的注意力，并将那个物体与单词联系起来。父母在命名时会看着物体，他们会挥动它，让孩子看它，并将他们在指向或挥动物体时的动作中所使用的语调模式与呈现名称的方式相匹配。比如父母在和年幼的孩子说话时经常将动作和声音结合起来。例如，一位父母向孩子展示一个玩具卷尺，拉出卷尺时说：“看，你拉——”（pull），将拉字拉长，以匹配拉动动作的开始和停止。同一位父母在缩回卷尺时说：“转圈圈和圈圈和圈圈”，每个“圈圈”都与转动的新周期开始一致。通过将动作和声音结合在一起，父母将语言植根于支撑所有认知的相同多模态学习过程中，从而吸引孩子的注意力，以节奏性地将其拉向相关的语言和感知事件，并紧密地将这些事件绑定在一起。</p><h3 id="对embodied-ai的启示-4">对embodied ai的启示？</h3><p>为了使智能代理发展和学习，应该让它们参与到社交互动中，与能够引导和帮助它们的代理（如人或其他智能实体）进行有效互动。</p><p>（Raise them in a social world, coupling their behavior and learningto agents who add structure and support to those coupledinteractions）</p><p>【笔者注：这种互动不仅包括简单的行为模仿，还应涵盖对复杂社交行为的理解和学习，从而提高智能代理的认知能力和社交适应性。】</p><h2 id="六语言学习">六、语言学习</h2><h3 id="原文描述-5">原文描述</h3><p>Babies learn a language, a shared communicative system that issymbolic. And this changes everything, enabling children to form evenhigher-level and more abstract distinctions.</p><p>婴儿学习一种语言，这是一种共享的、象征性的交流系统。这改变了一切，使儿童能够形成更高层次和更抽象的区分。</p><h3 id="语言和社交之间的关系">语言和社交之间的关系？</h3><p>语言的起源似乎深深植根于我们对周围世界的直接感知，以及我们通过身体运动和社交互动获得的经验，这些经验并非仅限于语言学习，而是一种更广泛的、能够适应和解决各种问题的学习方式。</p><h3 id="什么是语言">什么是语言？</h3><p>首先，语言是一种在世界中的规律，是一个共享的交流系统。它的共享特性意味着它非常稳定，持续受到构成它的许多本地交流行为的约束。在人类的生活中，语言在智能中的作用就像重力一样普遍、无处不在。</p><p>其次，语言之所以特别，是因为它是一种符号系统。在个别词（实际上是词素）的层面上，世界中的事件与指代它们的语言形式之间的关系主要是任意的。也就是说，大多数单词的声音与它们的指代之间没有内在的相似性：单词dog 的形式并没有提示我们它指的是哪种事物。而且 dig 和 dog形式的相似性并没有传达意义上的相似性。</p><p>【笔者注—<strong>为什么语言是一个任意的符号系统？</strong></p><p>语言的任意性指的是在形式（如发音、书写）和意义之间没有直接的、固有的联系。这种任意性使得语言具有高度的灵活性和表达能力。例如，单词“dog”没有任何直观上的联系来说明它代表什么。这种任意性允许语言创建和使用广泛的词汇和概念，即使这些词汇和概念在物理形态或经验上没有直接相似性。】</p><h3id="为什么单词的形状声音与它们的指代之间可以没有内在相似性">为什么单词的形状、声音与它们的指代之间可以没有内在相似性？</h3><p>尽管从理论上讲，我们可能会期望一个基于多感官体验和身体动作（多模态和感觉运动）的学习方式会促成一种更直观、更类似于肢体模仿（哑剧）的语言形式，其中符号（如单词）直接与它们所指的对象（如实物或动作）相似，但实际上语言并不是这样。语言中的符号（例如单词）通常与它们所指的对象在外形或声音上并没有直接相似性。此外，研究表明，虽然儿童可以很容易地学习在多种感官模态上具有一定相似性的符号与其所代表的事物之间的联系，但如果这种符号与其所指的事物之间的相似性过于明显（即图标性过强），反而会阻碍他们的学习。简而言之，这意味着语言的学习和使用需要在符号和所指事物之间保持一定程度的区分和抽象性。</p><p>DeLoache的研究提供了一个有趣的例证，这项研究不是针对语言学习，而是针对儿童使用等比例模型的能力。DeLoache的实验任务是一个藏东西的游戏，参与者是2岁的儿童。在每次试验中，一个玩具会被藏在一个真实大小的房间里，比如在沙发下。孩子的任务是找到这个玩具，实验者在每次试验中都会用某种模型告诉孩子玩具的确切位置。这个模型可能是一张蓝图、一张房间的画、一张照片、一个简单的比例模型、一个详细准确的比例模型，或者一个真实大小的模型。</p><p><strong>这里有一个非常robust但违反直觉的结果：</strong>每当模型与真实房间过于相似时，年幼的孩子就无法完成这项任务。例如，当图片中的解决方案比等比例模型中的解决方案更有可能成功，当比例模型是真实房间的简化版本而不是房间的准确表示，则更有可能成功。</p><h3 id="那么婴儿给了我们什么启发-5">那么婴儿给了我们什么启发？</h3><p>一种可能性是孩子们必须学会什么是符号，而要学会什么是符号，就必须有一些共同的属性，例如区分图片与真实物体或区分口头词与其他声音的属性。</p><p>所有世界语言都是符号系统这一事实，以及符号与所指之间过度相似会扰乱它们之间映射的学习这一事实，表明任意符号赋予了一些独特且有价值的计算能力。这种能力可能在正交性的属性中。大多数情况下，个别词汇指出或指向独特的类别。至少在2到3岁儿童的词汇中是这样的，这种现象有时被称为相互排他性约束（mutualexclusivity constraint）。</p><p>更具体地说，孩子们表现得好像世界上的每个物体只接受一个名字。例如，给孩子展示两个新的物体，并告诉他们其中一个的名字（例如，“这是一个dax”），孩子们会假设任何新名字（例如“wug”）指的是第二个之前未命名的物体。语言标签的任意性和相互排他性可能在计算上很强大，因为它们分离了创造感知类别的重叠规律，有证据支持这种正交性在计算上是强大的，使儿童能够形成二阶、类似规则的概括。</p><h3id="婴儿如何解决中牙牙学语中的排他性约束">婴儿如何解决中牙牙学语中的排他性约束？</h3><p>为了解释语言学习中这一发展上强大的方面，我们首先需要提供一些关于儿童词汇学习的背景。</p><p>孩子在大约10个月大时理解他们的第一个单词，在大约12个月大时产生他们的第一个单词。他们在语言学习中的初期进展肯定是基于婴儿在世界中互动中出现的（多模态的）事物类别。然而，最初的进展是犹豫的、缓慢的、脆弱的。在第一个单词之后的6个月或更长时间里，孩子们非常缓慢地习得后续单词，而且似乎经常会忘记之前习得的单词。此外，他们似乎需要在许多不同的情境中听到每个单独的单词，才能理解它的范围。然后，在18到20个月之间，大多数孩子成为非常快速的词汇学习者，每天将4到9个新词汇添加到他们的词汇表中。在这段时间里，他们似乎只需要听到一个单词用来标记一个物体，就能知道这个单词所指的整个事物类别。</p><p>这种从单个实例到整个类别的学习尤其引人注目，因为不同种类的类别以不同的方式组织。例如，有生命的类别是通过多种模态中的许多不同种类的相似性来组织的；人工制品类别是按形状组织的，物质类别是按材料组织的。来自实验研究和计算模型的证据表明，孩子们在慢慢学习他们的第一个单词时学习这些规律，而这种学习随后创造了他们一次性学习单词的能力。</p><p>这种学习的性质可以被描述为四个步骤，如图所示。该图仅展示了孩子们学习的规律之一：人工制品类别是按形状组织的。学习过程的第一步是将名字映射到物体——例如，将“球”这个名字映射到特定的球，将“杯子”这个名字映射到特定的杯子。每个名字在孩子遇到多个例子时都要重复多次。而且重要的是，在早期词汇中，形状固定、硬度高的物体通常按形状的相似性很好地组织在类别中。这种个别名字的学习建立了第二步——关于个别类别结构的一阶概括，即知道球是圆的，杯子是杯子形的。一阶概括应该使学习者能够识别新的球和杯子。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/babies/figure6.png" /></p><p>因为大多数孩子学到的关于固体和硬质物体的名称都是根据它们的形状命名的，孩子们可能还会学习到第二阶概括，即人工制品（固体、硬质物体）的名称通常涵盖了形状相似物体的类别。</p><p>如图中第3步所示，这种概括需要超越特定的名称和类别结构。但是这种高阶概括能使孩子将任何人工制品的名称，即使是第一次遇到的，通过形状扩展到新的实例。此时，孩子的行为就像他们有一个抽象和可变化的规则：对于任何人工制品，无论其个别属性或形状如何，都通过形状形成一个类别。</p><p>第4步展示了这种高阶概括的潜在发展后果——注意正确的属性，形状——对于学习人工制品的新名称。这种解释的合理性已在实验研究中得到证明，这些研究通过教给孩子们相关的相关性有效地加速了词汇习得功能，并在神经网络模拟研究中得到证实。</p><p>语言在促进形成第二阶概括方面的作用可能非常特殊。最近的模拟研究表明，语言标签的任意性和正交性可能是关键。如果标签（词汇）不是正交的，那么容易形成第二阶概括并产生加速词汇习得率的神经网络就不会做到这一点。</p><p>【笔者注：这里提到了很多语言学的概念术语，解释如下：</p><p><strong>“正交性”（orthogonality）</strong>是指在语言或其他符号系统中，不同的符号或概念在表示上是相互独立的，互不重叠。这种正交性使得每个符号或概念在思维和处理中更清晰、更易于区分。例如，字母“A”和“B”在视觉和发音上都是独立的。字母表正交性使得每个字母都可以清晰地被识别和区分，而不会与其他字母混淆。</p><p><strong>“第二阶”（second-order）</strong>通常指的是更为复杂或抽象的概念或规则。在儿童的语言发展中，第二阶概括可能指的是儿童不仅学习了具体的词汇和概念，还能理解并应用更抽象的规则或模式来组织这些词汇和概念。例如，他们不仅学会“这是一个球”这样的具体事实，还能理解更一般的规则，比如“物体通常按照形状分类”。</p><p><strong>“类似规则的”（rule-like）</strong>是指这种概括或概念的形式类似于一个规则或原则，它指导或影响儿童如何处理和理解信息。在语言学习的背景下，这可能涉及到如何根据语言中的模式或规则来形成单词、句子或语义的理解。例如，当一个儿童学习到某个物体的名称（例如，“球”）后，他们往往会应用一种“规则”，即假设每个独特的物体都有一个独特的名称。因此，当他们遇到一个新的物体（比如一个他们之前没见过的玩具车）时，他们会期望这个新物体有一个全新的名称，而不是已经学过的任何一个名称。】</p><h3id="语言采取这种形式解决了哪些计算问题">语言采取这种形式解决了哪些计算问题？</h3><p>任意符号所带来的优势远远超出了这里暗示的内容。更为人所熟知的是符号系统的属性，即由符号组合的可能性产生的能力。对于自然语言，这是语法的领域。所有已知的自然语言都具有符号系统的两个基本属性。</p><p>首先，它们至少是近似组合的。也就是说，在语法领域，与个别词素领域不同，语言绝非任意。组合性允许听者理解他们以前未曾听到的词素组合，也使说话者能够产生他们以前从未产生或听过的词素组合。一个知道什么是dax的英语说话者自动知道daxes指的是不止一个dax。【笔者注—能够理解或创造新的表达，简言之就是望文生义】</p><p>其次，作为符号的单词允许构建结构化的表达，特别是那些具有嵌入性的表达。嵌入性之所以可能，是因为代表符号间关系的符号本身可以扮演代表物体的符号的角色。所以我们可以说像“John认为Mary怀疑他喜欢她”和“教我喜欢的课的那个女人”这样的句子。【笔者注—能够确定层层嵌套的关系背后实际指代的对象】</p><p>语言表达的正交性可能也是源于词素层面形式-意义关系的任意性，它可能是语言这些属性背后的原因。如果一个句子中的单词表达显著重叠，就无法在组合这些单词的意义时将它们保持独立。正交表达允许同时在短期记忆中维持几个独立的项目，而不会产生显著的干扰。这并不否认这些单词背后概念的丰富、分布式表达；它只是突出了正交指针对这些表达的价值。这些指针可以被操作（组合、在结构中关联），而不必直接参照它们的意义或发音。</p><p>这种能力不仅局限于自然语言的语法。这种组合和构建结构化表达的潜力适用于其他符号处理领域，似乎也是人类活动如明确规划和数学的特征。长期以来，人们提出通过语言进入符号处理的途径，尽管这个想法仍有争议，但我们认为它值得认真对待。首先，因为句子结构映射到事件结构，语言可能会教导孩子们如何关注事件结构，就像它显然教会他们关注物体的特定维度一样。其次，一旦学会了允许语言具有组合性和结构化的正交符号，就可以为其他符号系统提供基础，比如代数背后的系统。【笔者注——不仅限于语言本身，还可能影响其他符号系统的发展，例如数学和逻辑。语言通过它的结构和功能可能帮助人们学习如何处理和组织其他类型的符号系统。】</p><h3 id="对embodied-ai的启示-5">对embodied ai的启示？</h3><p>在语言世界中成长使孩子们以至少三种方式变得更聪明。</p><p>首先，最明显的是，通过学习语言，孩子们获得了更直接地获取他人知识的途径。孩子们可以接受指导，当他们对某事不确定时，他们可以提问，并最终以书面形式搜索信息。虽然这种显性口头形式的知识可能没有直接经验结果的知识丰富，但<strong>它可以补充基于经验的知识，特别是在孩子们无法直接体验的领域。</strong></p><p>其次，在学习语言时，孩子们被呈现了一个关于世界中的物体、属性和关系的显性分类。自然语言中的每个词素代表了一系列感觉、运动和认知经验的概括，通过标记这个范围，词素充当了一种受监督的类别学习形式，这对其他生物来说是不可用的。因此，学习语言的一个结果是形成了一种本体论。这不仅使孩子们能够注意到他们可能错过的规律性（例如，形状对人工制品或运动对有生命体的重要性），而且因为本体论是语言社群成员共享的，它保证了社群成员对世界反应的一定程度的共同性。【笔者注——语言相当于给儿童的监督式的类别学习，学习的结果是对世界的结构化理解，这种理解和其他人有一定的共通点，类似于民族文化】</p><p>第三，正如我们在这里所暗示的，学习语言可能是成为象征性思维的关键，其本质可能会改变学习者的计算能力。每个单词将一个分布式的语音模式和一个分布式的概念模式以一种看似局部的，或至少是正交的方式关联起来。这种关联的主要任意性质可能有助于学习局部词汇表达；因为单词形式的相似性并不意味着对应意义的相似性，反之亦然，不重叠的中介表达是最有效的选择。无论原因如何，关于语言产出中词汇访问的研究指出，首先，这表明了独特词汇层面表达的心理实际性，其次，指出了这些表达的基本正交和竞争性质。这些局部表达的优势在于可以直接对它们进行复杂推理：它们可以相互关联，甚至可以在层次结构中排列，象征性地表示那些无法通过组成概念的分布式重叠表达实现的内容。因此，<strong>象征性推理的力量——规划、逻辑和数学——可能最终源自单词在指向概念方面的功能。</strong>【笔者注——语言中概念→单词的推理，引发了规划、逻辑、数学等具有象征性思维的学科发展】</p><h2 id="结论">结论</h2><p>人工生命试图通过复杂的算法来模拟生物系统。我们在这篇文章中提出，发展心理学为创造真实世界中生活、与之相连、并了解那个世界的智能提供了可用的启发。婴儿开始时拥有一个健全的身体，配备了多种感觉和行动系统。而即使是拥有了一些预编程和硬连线的机器人，也无法达到即使是3岁儿童的标准，除非它被调整以适应那个世界的详细统计数据。我们认为，embodiedintelligence是发展的。在一个（embodied）人类儿童中，智能随着孩子探索世界而出现，利用其复杂的统计学习能力来捕捉周围的微妙规律。因为孩子从小开始逐渐进步，其中的成长带给孩子这世界中不同的规律，这些规律包括孩子和socialpartners之间的耦合，以及一个符号系统（自然语言）。儿童实现了超越任何其他动物，甚至任何当前人工设备的智能。从婴儿那里得到的启发是：智能不仅仅是具体化的；它变得具体化。<strong>（Thelesson from babies is: intelligence isn’t just embodied; it becomesembodied）</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;这篇文章深度解析了美国自然科学领域的一篇论文，&lt;a
href=&quot;https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf&quot;&gt;《&lt;strong&gt;The
development of embodied cognition: s</summary>
      
    
    
    
    <category term="笔记" scheme="https://le-ying.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="科研" scheme="https://le-ying.github.io/tags/%E7%A7%91%E7%A0%94/"/>
    
    <category term="Embodied AI" scheme="https://le-ying.github.io/tags/Embodied-AI/"/>
    
  </entry>
  
  <entry>
    <title>同步电脑端和手机端日历教程（基于Outlook邮箱）</title>
    <link href="https://le-ying.github.io/2023/10/11/%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86/"/>
    <id>https://le-ying.github.io/2023/10/11/%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86/</id>
    <published>2023-10-11T14:28:00.000Z</published>
    <updated>2023-12-03T14:06:43.499Z</updated>
    
    <content type="html"><![CDATA[<h1id="同步电脑端和手机端日历教程基于outlook邮箱">同步电脑端和手机端日历教程（基于Outlook邮箱）</h1><h2 id="电脑端配置">电脑端配置</h2><h3id="步骤1应用栏搜索日历软件并打开">步骤1：应用栏搜索“日历”软件并打开</h3><p><imgsrc="https://github.com/Zeying-Gong/web-img/raw/main/TimeManagement/1.png" /></p><h3id="步骤2登录outlook邮箱切记一定选择office-365">步骤2：登录Outlook邮箱（切记一定选择Office365）</h3><p><imgsrc="https://github.com/Zeying-Gong/web-img/raw/main/TimeManagement/2.png" /></p><h2 id="手机端配置">手机端配置</h2><h3 id="步骤1打开电子邮件app">步骤1：打开电子邮件app</h3><p><img src="https://github.com/Zeying-Gong/web-img/raw/main/TimeManagement/3.jpeg" width="50%" height="50%"></p><h3id="步骤2登录outlook邮箱切记一定选择exchange">步骤2：登录Outlook邮箱（切记一定选择Exchange）</h3><p><img src="https://github.com/Zeying-Gong/web-img/raw/main/TimeManagement/4.jpeg" width="50%" height="50%"></p><h3id="一定要勾选同步日历选项默认是勾选了的登陆成功就ok啦">一定要勾选同步日历选项（默认是勾选了的），登陆成功就OK啦</h3><h2 id="手动同步">手动同步</h2><p>一般半分钟内会自动同步，如果同步延迟，可以进行手动同步。</p><h3 id="电脑端">电脑端</h3><p><imgsrc="https://github.com/Zeying-Gong/web-img/raw/main/TimeManagement/5.png" /></p><h3 id="手机端">手机端</h3><p><imgsrc="https://github.com/Zeying-Gong/web-img/raw/main/TimeManagement/6.png" /></p><h2 id="附录日程管理qa">附录：日程管理Q&amp;A</h2><p>这篇博客灵感来自于我和GPT-4的对话，相信各位读者也可以借助GPT-4实现日程管理的私人定制。</p><h3 id="起因发现自己生活一团糟">起因：发现自己生活一团糟</h3><ul><li>Q：我早上很有精神，但是下午和晚上时常犯困。并且我经常感到失控或者做事匆忙。你有什么建议吗？</li><li>A：时间管理是一个常见的挑战，但有方法可以帮助您克服它。首先，让我们来看看您的日常时间安排，以找出改进的空间。</li></ul><h3 id="现状已有的生活方式">现状：已有的生活方式</h3><ul><li><p>Q：我目前的时间安排如下：</p><table><thead><tr class="header"><th>时间段</th><th>活动</th></tr></thead><tbody><tr class="odd"><td>7:30 AM</td><td>起床</td></tr><tr class="even"><td>7:45 AM</td><td>早餐</td></tr><tr class="odd"><td>8:00 AM - 11:00 AM</td><td>学习</td></tr><tr class="even"><td>11:00 AM</td><td>午餐</td></tr><tr class="odd"><td>11:30 AM - 1:00 PM</td><td>休息</td></tr><tr class="even"><td>1:00 PM - 5:00 PM</td><td>学习</td></tr><tr class="odd"><td>5:00 PM</td><td>晚餐</td></tr><tr class="even"><td>5:30 PM - 8:00 PM</td><td>闲暇或娱乐活动</td></tr><tr class="odd"><td>8:00 PM</td><td>洗澡</td></tr><tr class="even"><td>8:30 PM - 9:30 PM</td><td>学习</td></tr><tr class="odd"><td>9:30 PM</td><td>放松</td></tr><tr class="even"><td>10:00 PM - 11:30 PM</td><td>闲暇活动（游戏、日记等）</td></tr><tr class="odd"><td>11:30 PM</td><td>睡觉</td></tr></tbody></table></li><li><p>A：现在，让我们讨论如何优化您的日程安排以提高生产力和组织能力：</p><ul><li>计划：每天开始时，设定明确的学习目标和任务。这将帮助您保持组织性和积极性。</li><li>聚焦早晨：您似乎在早晨最有活力。当您的注意力达到巅峰时，请利用这段时间完成最重要和最具挑战性的学习任务。</li><li>休息时间：在学习期间考虑缩短休息时间。例如，每小时休息5-10分钟来充电。</li><li>午间休息：午餐后，您可以利用这段时间进行短暂休息或快速散步以恢复头脑，而不是长时间休息。</li><li>晚间学习：如果您发现晚上很难集中注意力，请考虑将难度更大的任务转移到早上，并利用晚上进行复习或阅读等轻松的活动。</li><li>睡眠：确保您有足够的睡眠，目标是每晚至少 7-8 小时。晚上良好的休息将提高您白天的精力和注意力。</li><li>休闲时间：限制晚上玩游戏或休闲活动的时间，确保您有足够的时间学习、放松和睡前放松。</li></ul></li></ul><h3 id="改进日程管理私人定制">改进：日程管理私人定制</h3><ul><li>Q：我希望你结合我的原始时间安排，给我一个改进版本</li><li>A：以下是个性化的时间安排：</li></ul><table><thead><tr class="header"><th>时间段</th><th>活动</th></tr></thead><tbody><tr class="odd"><td>7:30 AM</td><td>起床</td></tr><tr class="even"><td>7:45 AM</td><td>早餐</td></tr><tr class="odd"><td>8:00 AM - 11:00 AM</td><td>学习</td></tr><tr class="even"><td>11:00 AM</td><td>午餐</td></tr><tr class="odd"><td>11:30 AM - 11:45 AM</td><td>短暂散步</td></tr><tr class="even"><td>11:45 AM - 12:30 AM</td><td>短暂休息</td></tr><tr class="odd"><td>12:30 AM - 1:00 PM</td><td>听有声书</td></tr><tr class="even"><td>1:00 PM - 4:00 PM</td><td>学习</td></tr><tr class="odd"><td>4:00 PM - 5:00 PM</td><td>锻炼</td></tr><tr class="even"><td>5:00 PM</td><td>晚餐</td></tr><tr class="odd"><td>5:30 PM</td><td>洗澡</td></tr><tr class="even"><td>5:30 PM - 9:30 PM</td><td>晚上学习</td></tr><tr class="odd"><td>9:30 PM - 10:30 PM</td><td>写博客</td></tr><tr class="even"><td>10:30 PM - 11:00 PM</td><td>制定明天的计划</td></tr><tr class="odd"><td>11:00 PM - 12:00 PM</td><td>阅读书籍、与室友交流或玩Switch游戏</td></tr><tr class="even"><td>12:00 PM</td><td>在午夜前入睡</td></tr></tbody></table><p>温馨提示：</p><ol type="1"><li>可以尝试番茄钟工作法，工作25分钟，休息5分钟。4个番茄循环后长休息15-30分钟</li><li>休息期间不看手机，做一些拉伸或者散步</li><li>午间休息期间建议听educational podcasts</li><li>建议晚饭前段乱，</li><li>夜间放松注意达成目标（比如学乐器和写博客），而不是沉浸游戏</li></ol><p>经过这次对话，我确实反省了之前的生活方式，并且调整了作息安排。GPT-4这个私人助理确实既专业又贴心，相信每个人都可以找到适合自己的、健康规律的生活方式。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1
id=&quot;同步电脑端和手机端日历教程基于outlook邮箱&quot;&gt;同步电脑端和手机端日历教程（基于Outlook邮箱）&lt;/h1&gt;
&lt;h2 id=&quot;电脑端配置&quot;&gt;电脑端配置&lt;/h2&gt;
&lt;h3
id=&quot;步骤1应用栏搜索日历软件并打开&quot;&gt;步骤1：应用栏搜索“日历”软件并打开&lt;</summary>
      
    
    
    
    <category term="总结" scheme="https://le-ying.github.io/categories/%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="介绍" scheme="https://le-ying.github.io/tags/%E4%BB%8B%E7%BB%8D/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 零基础解析教程，完整版代码最终挑战（4/4）</title>
    <link href="https://le-ying.github.io/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/"/>
    <id>https://le-ying.github.io/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/</id>
    <published>2023-02-05T03:53:10.000Z</published>
    <updated>2023-02-26T07:15:44.987Z</updated>
    
    <content type="html"><![CDATA[<h1 id="导航">导航</h1><p><strong>这篇文章是 Transformer完整版复现代码的深度解析。</strong></p><blockquote><p>本博客的 Transformer 系列文章共计四篇，导航如下：</p></blockquote><ul><li><p><ahref="https://serika-onoe.github.io/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/">Transformer零基础解析教程，从Encoder-Decoder架构说起（1/4）</a></p></li><li><p><ahref="https://serika-onoe.github.io/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/">Transformer零基础解析教程，剥洋葱般层层剖析内在原理（2/4）</a></p></li><li><p><ahref="https://serika-onoe.github.io/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/">Transformer零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）</a></p></li><li><p><ahref="https://serika-onoe.github.io/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/">Transformer零基础解析教程，完整版代码最终挑战（4/4）<strong>本篇</strong></a></p></li></ul><h1 id="前言">前言</h1><p>由哈佛的NLP组撰写的 <ahref="https://nlp.seas.harvard.edu/2018/04/03/attention.html">TheAnnotated Transformer</a>，用代码对应论文<ahref="https://arxiv.org/abs/1706.03762">《Attention is all youneed》</a>的各个部分基本复现和还原了论文模型中初始版本的Transformer，并给出了两个机器翻译的例子。而本文内容是在 The AnnotatedTransformer 的 Colab 版基础上，进一步精读 《Attention is all youneed》中的 Transformer结构和源码。作者之所以选择它进行解读，是因为它是一个单独的ipynb文件，如果我们要实际使用只需要全部执行单元格就行了。而一些比较大的库（比如Tensor2Tensor和OpenNMT等等）尽管也包含了Transformer的实现，但是对于理解论文和跑代码来说不够方便，为什么不选择更简单实用的现成品呢？</p><p>本文完整版及执行结果可见我在Google Colab的注释版副本 <ahref="https://colab.research.google.com/drive/1LrYYeXvIS_LJURYZFp_MDdJvRJmDACwL#scrollTo=uNDgJW-gFn_g">TheAnnotated Transformer by Harvard NLP</a>.同时告诉大家一个小trick：不妨试着用Chatgpt进行代码讲解，本系列文章有相当一部分代码讲解都是经过Chatgpt辅助我理解消化的。读者可以尝试一下<ahref="https://chat.openai.com/">ChatGPT</a>，我一般尊称它为<strong>“Chat老师”</strong>，它是一个非常耐心的老师，（最重要的，它还是免费哒！）这让我想起了柏拉图或者孔夫子的教学方式——通过青年问禅师的对话体，来回答读者的困惑，并启发更深层次的哲学思考。</p><h1 id="背景">背景</h1><p>The goal of reducing sequential computation also forms the foundationof the Extended Neural GPU, ByteNet and ConvS2S, all of which useconvolutional neural networks as basic building block, computing hiddenrepresentations in parallel for all input and output positions. In thesemodels, the number of operations required to relate signals from twoarbitrary input or output positions grows in the distance betweenpositions, linearly for ConvS2S and logarithmically for ByteNet. Thismakes it more difficult to learn dependencies between distant positions.In the Transformer this is reduced to a constant number of operations,albeit at the cost of reduced effective resolution due to averagingattention-weighted positions, an effect we counteract with Multi-HeadAttention.</p><p>Self-attention, sometimes called intra-attention is an attentionmechanism relating different positions of a single sequence in order tocompute a representation of the sequence. Self-attention has been usedsuccessfully in a variety of tasks including reading comprehension,abstractive summarization, textual entailment and learningtask-independent sentence representations. End-to-end memory networksare based on a recurrent attention mechanism instead of sequencealignedrecurrence and have been shown to perform well on simple-languagequestion answering and language modeling tasks.</p><p>To the best of our knowledge, however, the Transformer is the firsttransduction model relying entirely on self-attention to computerepresentations of its input and output without using sequence alignedRNNs or convolution.</p><blockquote><p>翻译</p></blockquote><p>减少序列计算（<code>sequential computation</code>）的目标也是<code>Extended Neural GPU、ByteNet</code>、以及<code>ConvS2S</code>等模型的基础，所有这些都是使用CNN作为基础块，对于所有的输入输出位置并行计算隐层表示。在这些模型当中，模型<code>ConvS2S</code>将任意输入输出信号联系起来要求操作的次数与这两者位置间的距离呈线性关系，而模型<code>ByteNet</code>则呈对数关系。这使得学习远距离的依赖变得更加困难。然而在Transformer中，这个复杂度被减至一个常数操作，尽管由于平均<code>attention-weighted</code>位置在以减少有效地解析为代价，但我们提出一种<code>Multi-head Attention</code> 用于抵消这种影响。</p><p>自我注意（<code>self-attention</code>），有时也称为内部注意，是一个注意力机制，这种注意力机制：将单个句子不同的位置联系起来，用于计算一个序列的表示。自我注意已经被成功应用在个各种各样的任务中，诸如阅读理解，抽象总结，文本蕴含，以及学习独立任务的句子表示中。端到端的记忆网络是基于递归注意力机制，而不是对齐序列递归，同时在单一语言问答系统以及语言建模任务中，端到端的网络已经被证明效果很好。</p><p>然而，据我们所知，Transformer是第一个完全依赖于自我注意的推导模型。在接下来的部分中，我们将描述Transformer，自驱动的自我注意力机制，并且讨论它们的优缺点。</p><h1 id="第一部分-模型架构">第一部分: 模型架构</h1><p>Most competitive neural sequence transduction models have anencoder-decoder structure <ahref="https://arxiv.org/abs/1409.0473">(cite)</a>. Here, the encodermaps an input sequence of symbol representations <spanclass="math inline">\((x_1, ..., x_n)\)</span> to a sequence ofcontinuous representations <span class="math inline">\(\mathbf{z} =(z_1, ..., z_n)\)</span>. Given <spanclass="math inline">\(\mathbf{z}\)</span>, the decoder then generates anoutput sequence <span class="math inline">\((y_1,...,y_m)\)</span> ofsymbols one element at a time. At each step the model is auto-regressive<a href="https://arxiv.org/abs/1308.0850">(cite)</a>, consuming thepreviously generated symbols as additional input when generating thenext.</p><blockquote><p>翻译</p></blockquote><p>大多数的神经序列推导模型有一个 <code>encoder-decoder</code>结构。这里，<code>encoder</code> 将一个表示输入的字符序列 (<spanclass="math inline">\(x_1\)</span>,<spanclass="math inline">\(x_2\)</span>,...<spanclass="math inline">\(x_n\)</span>) 映射成另一种连续表征序列 Z=(<spanclass="math inline">\(z_1\)</span>,<spanclass="math inline">\(z_2\)</span>,...<spanclass="math inline">\(z_m\)</span>)。 基于Z，<code>decoder</code>每次产生一个元素 <span class="math inline">\(y_i\)</span>最终输出一个序列 Y=(<span class="math inline">\(y_1\)</span>,...<spanclass="math inline">\(y_m\)</span>)。<code>Decoder</code>中的每个步骤都是自回归的 ——消耗之前产生的表征作为下次的输入。</p><p>在这里，"消耗"意味着模型使用或吸收上一步生成的符号作为输入。换句话说，上一步生成的输出成为当前步骤的输入的一部分。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A standard Encoder-Decoder architecture. </span></span><br><span class="line"><span class="string">    Base for this and many other models.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, </span></span><br><span class="line"><span class="params">      src_embed, tgt_embed, generator</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        <span class="comment"># Encoder对象</span></span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        <span class="comment"># Decoder对象</span></span><br><span class="line">        self.src_embed = src_embed</span><br><span class="line">        <span class="comment"># 源语言序列的编码，包括词嵌入和位置编码</span></span><br><span class="line">        self.tgt_embed = tgt_embed</span><br><span class="line">        <span class="comment"># 目标语言序列的编码，包括词嵌入和位置编码</span></span><br><span class="line">        self.generator = generator</span><br><span class="line">        <span class="comment"># 生成器（Decoder后面 Linear + Softmax 部分）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Take in and process masked src and target sequences.&quot;</span></span><br><span class="line">        <span class="keyword">return</span> self.decode(self.encode(src, src_mask), src_mask,</span><br><span class="line">                            tgt, tgt_mask)</span><br><span class="line">        <span class="comment"># 先对源语言序列进行编码，</span></span><br><span class="line">        <span class="comment"># 结果作为memory传递给目标语言的编码器</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        <span class="comment"># src = (batch.size, seq.length)</span></span><br><span class="line">        <span class="comment"># src_mask 负责对src加掩码</span></span><br><span class="line">        <span class="keyword">return</span> self.encoder(self.src_embed(src), src_mask)</span><br><span class="line">        <span class="comment"># 对源语言序列进行编码，得到的结果为</span></span><br><span class="line">        <span class="comment"># (batch.size, seq.length, 512)的tensor</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">return</span> self.decoder(self.tgt_embed(tgt), </span><br><span class="line">          memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="comment"># 对目标语言序列进行编码，得到的结果为</span></span><br><span class="line">        <span class="comment"># (batch.size, seq.length, 512)的tensor</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Define standard linear + softmax generation step.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="comment"># d_model=512， vocab = 目标语言词表大小</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, self).__init__()</span><br><span class="line">        self.proj = nn.Linear(d_model, vocab)</span><br><span class="line">        <span class="comment"># 定义一个全连接层，可训练参数个数是(512 * trg_vocab_size) + trg_vocab_size </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(self.proj(x), dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># x 类似于 (batch.size, sequence.length, 512)</span></span><br><span class="line">        <span class="comment"># -&gt; proj 全连接层 (30, 10, trg_vocab_size) = logits</span></span><br><span class="line">        <span class="comment"># 对最后一个维度执行log_soft_max 得到(30, 10, trg_vocab_size)</span></span><br></pre></td></tr></table></figure><p>The Transformer follows this overall architecture using stackedself-attention and point-wise, fully connected layers for both theencoder and decoder, shown in the left and right halves of Figure 1,respectively.</p><blockquote><p>翻译</p></blockquote><p><code>Transformer</code>遵循了这种总体的架构：在encoder和decoder中都使用数层<code>self-attention</code>和 <code>point-wise</code>，全连接层。相对的各个部分正如Figure1中左右两部分描述的一样。</p><p><imgsrc="https://github.com/harvardnlp/annotated-transformer/blob/master/images/ModalNet-21.png?raw=1" /></p><h2 id="encoder-and-decoder-stacks">Encoder and Decoder Stacks</h2><h3 id="encoder">Encoder</h3><p>The encoder is composed of a stack of <spanclass="math inline">\(N=6\)</span> identical layers.</p><blockquote><p>翻译</p></blockquote><p>Encoder 由6个相同的层叠加而成。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">module, N</span>):</span><br><span class="line">    <span class="string">&quot;Produce N identical layers.&quot;</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(module) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br></pre></td></tr></table></figure><blockquote><p>关于 clones 函数</p></blockquote><p>实现一个网络的深copy，也就是说copy一个新的对象，和原来的对象，完全分离，不分享任何存储空间，从而保证可训练参数，都有自己的取值或梯度。</p><p>比如可以用在copy N 个 EncoderLayer 组成 Encoder</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Core encoder is a stack of N layers&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="comment"># layer = one EncoderLayer object, N=6</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N) </span><br><span class="line">        <span class="comment"># 深copy，N=6，</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        <span class="comment"># 定义一个LayerNorm，layer.size=d_model=512</span></span><br><span class="line">        <span class="comment"># 其中有两个可训练参数a_2和b_2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Pass the input (and mask) through each layer in turn.&quot;</span></span><br><span class="line">        <span class="comment"># x is alike (30, 10, 512)</span></span><br><span class="line">        <span class="comment"># (batch.size, sequence.len, d_model)</span></span><br><span class="line">        <span class="comment"># mask是类似于(batch.size, 10, 10)的矩阵</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">            <span class="comment"># 进行6次EncoderLayer操作</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line">        <span class="comment"># 最后做一次LayerNorm，最后的输出也是(30, 10, 512)</span></span><br></pre></td></tr></table></figure><p>Each layer has two sub-layers. The first is a multi-headself-attention mechanism, and the second is a simple, position-wisefully connected feed-forward network.</p><p>We employ a residual connection <ahref="https://arxiv.org/abs/1512.03385">(cite)</a> around each of thetwo sub-layers, followed by layer normalization <ahref="https://arxiv.org/abs/1607.06450">(cite)</a>.</p><p>That is, the output of each sub-layer is <spanclass="math inline">\(\mathrm{LayerNorm}(x +\mathrm{Sublayer}(x))\)</span>, where <spanclass="math inline">\(\mathrm{Sublayer}(x)\)</span> is the functionimplemented by the sub-layer itself. We apply dropout <ahref="http://jmlr.org/papers/v15/srivastava14a.html">(cite)</a> to theoutput of each sub-layer, before it is added to the sub-layer input andnormalized.</p><p>To facilitate these residual connections, all sub-layers in themodel, as well as the embedding layers, produce outputs of dimension<span class="math inline">\(d_{\text{model}}=512\)</span>.</p><blockquote><p>翻译</p></blockquote><p>Encoder由6个相同的层叠加而成。每层又分成两个子层。第一层是<code>multi-head self-attention</code>机制，第二层则是简单的<code>position-wise fully connected feed-forward network</code>。</p><p>在每个子层中，我们都使用残差网络，然后紧接着一个<code>layer normalization</code>。</p><p>也就是说：其实每个子层的实际输出是<code>LayerNorm(x+Sublayer(x))</code>，其中<code>Sublayer(x)</code>是由sub-layer层实现的。</p><p>为了简化这个残差连接，模型中的所有子层都与 embedding层相同，输出的结果维度都是 <spanclass="math inline">\(d_{model}\)</span> = 512。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Construct a layernorm module (See citation for details).&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="comment"># features=d_model=512, eps=epsilon 用于分母的非0化平滑</span></span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, self).__init__()</span><br><span class="line">        self.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        <span class="comment"># a_2 是一个可训练参数向量，(512)</span></span><br><span class="line">        self.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line">        <span class="comment"># b_2 也是一个可训练参数向量, (512)</span></span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x 的形状为(batch.size, sequence.len, 512)</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">        <span class="comment"># 对x的最后一个维度，取平均值，得到tensor (batch.size, seq.len)</span></span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 对x的最后一个维度，取标准差，得(batch.size, seq.len)</span></span><br><span class="line">        <span class="keyword">return</span> self.a_2 * (x - mean) / (std + self.eps) + self.b_2</span><br><span class="line">        <span class="comment"># 本质上类似于（x-mean)/std，不过这里加入了两个可训练向量a_2 and b_2</span></span><br><span class="line">        <span class="comment"># 以及分母上增加一个极小值epsilon，用来防止std为0的时候，除法溢出</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    A residual connection followed by a layer norm.</span></span><br><span class="line"><span class="string">    Note for code simplicity the norm is first as opposed to last.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout</span>):</span><br><span class="line">        <span class="comment"># size=d_model=512; dropout=0.1</span></span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, self).__init__()</span><br><span class="line">        self.norm = LayerNorm(size) <span class="comment"># (512)，用来定义a_2和b_2</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;Apply residual connection to any sublayer with the &quot;</span></span><br><span class="line">        <span class="string">&quot;same size.&quot;</span></span><br><span class="line">        <span class="comment"># x is alike (batch.size, sequence.len, 512)</span></span><br><span class="line">        <span class="comment"># sublayer是一个具体的MultiHeadAttention或者PositionwiseFeedForward对象</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x + self.dropout(sublayer(self.norm(x)))</span><br><span class="line">        <span class="comment"># x (30, 10, 512) -&gt; norm (LayerNorm) -&gt; (30, 10, 512)</span></span><br><span class="line">        <span class="comment"># -&gt; sublayer (MultiHeadAttention or PositionwiseFeedForward)</span></span><br><span class="line">        <span class="comment"># -&gt; (30, 10, 512) -&gt; dropout -&gt; (30, 10, 512)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 然后输入的x（没有走sublayer) + 和走sublayer后的结果，即实现了残差相加的功能</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Encoder is made up of self-attn and &quot;</span></span><br><span class="line">    <span class="string">&quot;feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="comment"># size = d_model = 512</span></span><br><span class="line">        <span class="comment"># self_attn = MultiHeadAttention对象, first sublayer</span></span><br><span class="line">        <span class="comment"># feed_forward = PositionwiseFeedForward对象，second sublayer</span></span><br><span class="line">        <span class="comment"># dropout = 0.1 (e.g.)</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 使用深度克隆方法，完整地复制出来两个SublayerConnection</span></span><br><span class="line">        self.size = size </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (left) for connections.&quot;</span></span><br><span class="line">        <span class="comment"># x shape = (30, 10, 512)</span></span><br><span class="line">        <span class="comment"># mask 是(batch.size, 10,10)的矩阵，类似于当前一个词w，有哪些词是w可见的</span></span><br><span class="line">        <span class="comment"># 源语言的序列的话，所有其他词都可见，除了&quot;&lt;blank&gt;&quot;这样的填充；</span></span><br><span class="line">        <span class="comment"># 目标语言的序列的话，所有w的左边的词，都可见。</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, </span><br><span class="line">          <span class="keyword">lambda</span> x: self.self_attn(x, x, x, mask))</span><br><span class="line">        <span class="comment"># lambda 匿名函数，说明见下文</span></span><br><span class="line">        <span class="comment"># x (30, 10, 512) -&gt; self_attn (MultiHeadAttention) </span></span><br><span class="line">        <span class="comment"># shape is same (30, 10, 512) -&gt; SublayerConnection -&gt; (30, 10, 512)</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br><span class="line">        <span class="comment"># x 和feed_forward对象一起，给第二个SublayerConnection</span></span><br></pre></td></tr></table></figure><blockquote><p>关于 SublayerConnection 实现和论文间的差异</p></blockquote><p>论文中的执行顺序是 LayerNorm(x +Sublayer(x))，即把LayerNorm放残差和的外边了，对应的代码应该是returnself.norm(x + self.dropout(sublayer(x)))。</p><p>这里的实现是先把x进行layernorm，然后扔给sublayer(例如multi-headself-attention, position-wisefeed-forward)，也有一定的道理（效果上并不差），类似于在复杂操作前，先layernorm。</p><blockquote><p>关于 EncoderLayer 中，匿名函数 lambda 的说明</p></blockquote><p>稍微难理解的是 EncoderLayer 的 forward方法使用了lambda来定义一个匿名函数。这是因为之前我们定义的self_attn函数需要4个参数(Query的输入,Key的输入,Value的输入和Mask)。</p><p>因此这里我们使用lambda的技巧把它变成一个参数x的函数，mask可以看成已知的数。（<code>lambda x</code>表示 lambda 的形参也叫x），如果看到这里觉得难以理解的话，我们不妨把改写的函数抽离出来，记作<code>self_attn_lambda</code>：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">self_attn_lambda</span>(<span class="params">x, mask</span>):</span><br><span class="line">    <span class="keyword">return</span> self.self_attn(x, x, x, mask)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">    x = self.sublayer[<span class="number">0</span>](x, self_attn_lambda(x, mask))</span><br><span class="line">    <span class="keyword">return</span> self.sublayer[<span class="number">1</span>](x, self.feed_forward)</span><br></pre></td></tr></table></figure><h3 id="decoder">Decoder</h3><p>The decoder is also composed of a stack of <spanclass="math inline">\(N=6\)</span> identical layers.</p><p>In addition to the two sub-layers in each encoder layer, the decoderinserts a third sub-layer, which performs multi-head attention over theoutput of the encoder stack. Similar to the encoder, we employ residualconnections around each of the sub-layers, followed by layernormalization.</p><p>We also modify the self-attention sub-layer in the decoder stack toprevent positions from attending to subsequent positions. This masking,combined with fact that the output embeddings are offset by oneposition, ensures that the predictions for position <spanclass="math inline">\(i\)</span> can depend only on the known outputs atpositions less than <span class="math inline">\(i\)</span>.</p><blockquote><p>翻译</p></blockquote><p>decoder 同样是由 N=6 个相同layer组成的栈。</p><p>除了encoder layer中的两个子层之外，decoder还插入了第三个子层，这层的功能就是：利用 encoder的输出，执行一个<code>multi-head attention</code>。与encoder相似，在每个子层中，我们都使用一个残差连接，接着在其后跟一个layernormalization。</p><p>为了防止当前位置看到后序的位置，我们同样修改了decoder栈中的self-attention子层。这个masking，是基于这样一种事实：输出embedding偏移了一个位置，确保对位置i的预测仅仅依赖于位置小于i的、已知的输出。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Generic N layer decoder with masking.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="comment"># layer = DecoderLayer object</span></span><br><span class="line">        <span class="comment"># N = 6</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.layers = clones(layer, N)</span><br><span class="line">        <span class="comment"># 深度copy 6次DecoderLayer</span></span><br><span class="line">        self.norm = LayerNorm(layer.size)</span><br><span class="line">        <span class="comment"># 初始化1个LayerNorm</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">            <span class="comment"># 执行6次DecoderLayer</span></span><br><span class="line">        <span class="keyword">return</span> self.norm(x)</span><br><span class="line">        <span class="comment"># 执行1次LayerNorm</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Decoder is made of self-attn, src-attn, &quot;</span></span><br><span class="line">    <span class="string">&quot;and feed forward (defined below)&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, src_attn, </span></span><br><span class="line"><span class="params">      feed_forward, dropout</span>):</span><br><span class="line">      <span class="comment"># size = d_model=512,</span></span><br><span class="line">      <span class="comment"># self_attn = one MultiHeadAttention object，目标语言序列的注意力层</span></span><br><span class="line">      <span class="comment"># src_attn = second MultiHeadAttention object, 目标语言序列和源语言序列之间的注意力层</span></span><br><span class="line">      <span class="comment"># feed_forward 全连接层</span></span><br><span class="line">      <span class="comment"># dropout = 0.1</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.size = size <span class="comment"># 512</span></span><br><span class="line">        self.self_attn = self_attn</span><br><span class="line">        self.src_attn = src_attn</span><br><span class="line">        self.feed_forward = feed_forward</span><br><span class="line">        self.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line">        <span class="comment"># 需要三个SublayerConnection, 分别在</span></span><br><span class="line">        <span class="comment"># self.self_attn, self.src_attn 和 self.feed_forward 的后边</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;Follow Figure 1 (right) for connections.&quot;</span></span><br><span class="line">        m = memory <span class="comment"># (batch.size, sequence.len, 512) </span></span><br><span class="line">        <span class="comment"># 来自源语言序列的Encoder之后的输出</span></span><br><span class="line">        <span class="comment"># 作为memory供目标语言的序列检索匹配</span></span><br><span class="line">        x = self.sublayer[<span class="number">0</span>](x, </span><br><span class="line">          <span class="keyword">lambda</span> x: self.self_attn(x, x, x, tgt_mask))</span><br><span class="line">        <span class="comment"># 通过一个匿名函数，来实现目标序列的自注意力编码</span></span><br><span class="line">        <span class="comment"># 结果扔给sublayer[0]:SublayerConnection</span></span><br><span class="line">        x = self.sublayer[<span class="number">1</span>](x, </span><br><span class="line">          <span class="keyword">lambda</span> x: self.src_attn(x, m, m, src_mask))</span><br><span class="line">        <span class="comment"># 通过第二个匿名函数，来实现目标序列和源序列的注意力计算</span></span><br><span class="line">        <span class="comment"># 结果扔给sublayer[1]:SublayerConnection</span></span><br><span class="line">        <span class="keyword">return</span> self.sublayer[<span class="number">2</span>](x, self.feed_forward)</span><br><span class="line">        <span class="comment"># 走一个全连接层，然后结果扔给sublayer[2]:SublayerConnection</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">subsequent_mask</span>(<span class="params">size</span>):</span><br><span class="line">    <span class="string">&quot;Mask out subsequent positions.&quot;</span></span><br><span class="line">    <span class="comment"># e.g., size=10</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size) <span class="comment"># (1, 10, 10)</span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line">    <span class="comment"># triu: 负责生成一个三角矩阵，k-th对角线以下都是设置为0 </span></span><br><span class="line">    <span class="comment"># 上三角中元素为1.</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(subsequent_mask) == <span class="number">0</span></span><br><span class="line">    <span class="comment"># 反转上面的triu得到的上三角矩阵，修改为下三角矩阵。</span></span><br></pre></td></tr></table></figure><blockquote><p>关于subsequent_mask的三角矩阵</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br></pre></td></tr></table></figure><p>这里的k=1表示1号对角线，不太容易理解：举例子如下：</p><p><imgsrc="https://pic1.zhimg.com/v2-0972afdd345e18defc3bd6cc59760ad4_b.jpg" /></p><blockquote><p>Below the attention mask shows the position each tgt word (row) isallowed to look at (column). Words are blocked for attending to futurewords during training.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">example_mask</span>():</span><br><span class="line">    LS_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;Subsequent Mask&quot;</span>: subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>][x, y].flatten(),</span><br><span class="line">                    <span class="string">&quot;Window&quot;</span>: y,</span><br><span class="line">                    <span class="string">&quot;Masking&quot;</span>: x,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(LS_data)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .properties(height=<span class="number">250</span>, width=<span class="number">250</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            alt.X(<span class="string">&quot;Window:O&quot;</span>),</span><br><span class="line">            alt.Y(<span class="string">&quot;Masking:O&quot;</span>),</span><br><span class="line">            alt.Color(<span class="string">&quot;Subsequent Mask:Q&quot;</span>, scale=alt.Scale(scheme=<span class="string">&quot;viridis&quot;</span>)),</span><br><span class="line">        )</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(example_mask)</span><br></pre></td></tr></table></figure><h3 id="attention">Attention</h3><p>An attention function can be described as mapping a query and a setof key-value pairs to an output, where the query, keys, values, andoutput are all vectors. The output is computed as a weighted sum of thevalues, where the weight assigned to each value is computed by acompatibility function of the query with the corresponding key.</p><p>We call our particular attention "Scaled Dot-Product Attention". Theinput consists of queries and keys of dimension <spanclass="math inline">\(d_k\)</span>, and values of dimension <spanclass="math inline">\(d_v\)</span>. We compute the dot products of thequery with all keys, divide each by <spanclass="math inline">\(\sqrt{d_k}\)</span>, and apply a softmax functionto obtain the weights on the values.</p><p><imgsrc="https://github.com/harvardnlp/annotated-transformer/blob/master/images/ModalNet-19.png?raw=1" /></p><p>In practice, we compute the attention function on a set of queriessimultaneously, packed together into a matrix <spanclass="math inline">\(Q\)</span>. The keys and values are also packedtogether into matrices <span class="math inline">\(K\)</span> and <spanclass="math inline">\(V\)</span>. We compute the matrix of outputsas:</p><blockquote><p>翻译</p></blockquote><p>一个Attention function 可以被描述成一个“映射query和一系列key-valuepair 到输出”，其中 query, keys, values, output 都是一个向量。这些权值是由 querys 和 keys 通过相关函数计算出来的。【注：原文中的query就是没有复数形式】</p><p>本文中使用的attention被称作<code>Scaled Dot-Product Attention</code>。输入包含了 <spanclass="math inline">\(d_k\)</span> 维的queries 和 keys，以及values的维数是 <span class="math inline">\(d_v\)</span>。我们使用所有的values计算query的dot products，然后除以 <spanclass="math inline">\(\sqrt{d_k}\)</span>，再应用一个softmax函数去获得值的权重。【需要关注value在不同地方的含义。是key-value，还是计算得到的value？】</p><p>实际处理中，我们将同时计算一系列 query 的 attention，并将这些queries写成矩阵Q的形式。同样，将 keys, values同样打包成矩阵K和矩阵V。计算公式如下</p><p><span class="math display">\[   \mathrm{Attention}(Q, K, V) =\mathrm{softmax}(\frac{QK^T}{\sqrt{d_k}})V\]</span></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>): </span><br><span class="line"><span class="comment"># query, key, value的形状类似于(30, 8, 10, 64), (30, 8, 11, 64), </span></span><br><span class="line"><span class="comment"># 30是batch.size，即当前batch中有多少一个序列；8=head.num，注意力头的个数；</span></span><br><span class="line"><span class="comment"># 10=目标序列中词的个数，11=源语言序列传过来的memory中，当前序列的词的个数，</span></span><br><span class="line"><span class="comment"># 64是每个词对应的向量表示。</span></span><br><span class="line"><span class="comment"># 类似于，这里假定query来自target language sequence；</span></span><br><span class="line"><span class="comment"># key和value都来自source language sequence.</span></span><br><span class="line">  <span class="string">&quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot;</span> </span><br><span class="line"></span><br><span class="line">  d_k = query.size(-<span class="number">1</span>) <span class="comment"># d_k = 64</span></span><br><span class="line">  scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k) </span><br><span class="line">    <span class="comment"># 先是(30, 8, 10, 64)和(30, 8, 64, 11)相乘，</span></span><br><span class="line">    <span class="comment">#（注意是最后两个维度相乘）得到(30,8,10,11)，</span></span><br><span class="line">    <span class="comment"># 代表10个目标语言序列中每个词和11个源语言序列的分别的“亲密度”。</span></span><br><span class="line">    <span class="comment"># 然后除以sqrt(d_k)=8，防止过大的亲密度。</span></span><br><span class="line">    <span class="comment"># 这里的scores的shape是(30, 8, 10, 11)</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">    scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>) </span><br><span class="line">    <span class="comment"># 使用mask，对已经计算好的scores，按照mask矩阵，填-1e9，</span></span><br><span class="line">    <span class="comment"># 然后在下一步计算softmax的时候，被设置成-1e9的数对应的值~=0,被忽视</span></span><br><span class="line"></span><br><span class="line">  p_attn = F.softmax(scores, dim = -<span class="number">1</span>) </span><br><span class="line">    <span class="comment"># 对scores的最后一个维度执行softmax，得到的还是一个tensor, (30, 8, 10, 11)</span></span><br><span class="line">  <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: </span><br><span class="line">    p_attn = dropout(p_attn) <span class="comment">#执行一次dropout</span></span><br><span class="line">  <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 返回的第一项，是(30,8,10,11)乘以 value=(30,8,11,64)，得到的tensor是(30,8,10,64)，</span></span><br><span class="line">    <span class="comment"># 和query的最初的形状一样。另外，返回p_attn，形状为(30,8,10,11). </span></span><br><span class="line">    <span class="comment"># 注意，这里返回p_attn主要是用来可视化显示多头注意力机制。</span></span><br></pre></td></tr></table></figure><p>The two most commonly used attention functions are additive attention<a href="https://arxiv.org/abs/1409.0473">(cite)</a>, and dot-product(multiplicative) attention. Dot-product attention is identical to ouralgorithm, except for the scaling factor of <spanclass="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>. Additive attentioncomputes the compatibility function using a feed-forward network with asingle hidden layer. While the two are similar in theoreticalcomplexity, dot-product attention is much faster and morespace-efficient in practice, since it can be implemented using highlyoptimized matrix multiplication code.</p><p>While for small values of <span class="math inline">\(d_k\)</span>the two mechanisms perform similarly, additive attention outperforms dotproduct attention without scaling for larger values of <spanclass="math inline">\(d_k\)</span> <ahref="https://arxiv.org/abs/1703.03906">(cite)</a>. We suspect that forlarge values of <span class="math inline">\(d_k\)</span>, the dotproducts grow large in magnitude, pushing the softmax function intoregions where it has extremely small gradients (To illustrate why thedot products get large, assume that the components of <spanclass="math inline">\(q\)</span> and <spanclass="math inline">\(k\)</span> are independent random variables withmean <span class="math inline">\(0\)</span> and variance <spanclass="math inline">\(1\)</span>. Then their dot product, <spanclass="math inline">\(q \cdot k = \sum_{i=1}^{d_k} q_ik_i\)</span>, hasmean <span class="math inline">\(0\)</span> and variance <spanclass="math inline">\(d_k\)</span>.). To counteract this effect, wescale the dot products by <spanclass="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>.</p><blockquote><p>翻译</p></blockquote><p>最常用的两个 attention 是<code>additive attention</code>和<code>dot-product(multiplicative) attention</code>。除了使用放缩因子<span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>之外，本文中使用的算法与<code>Dot-product attention</code>算法完全一致。<code>Additive attention</code>使用一个只有一层的前向反馈网络计算<code>compatibility function</code>。然而两者在理论复杂度上是相似的，实际上，<code>dot-product attention</code>更快些，且空间效率更高些，这是因为它可以使用高度优化的矩阵乘法代码来实现。</p><p>而对于<spanclass="math inline">\(d_k\)</span>的较小值，这两种机制的表现相似，但加法注意力比（<spanclass="math inline">\(d_k\)</span>值较大而没有缩放的）点积注意力更胜一筹。我们怀疑，对于大的<spanclass="math inline">\(d_k\)</span>值，点积的量级会变大，导致softmax函数进入到了梯度极小的区域。</p><p>为了说明点积变大的原因。假设<spanclass="math inline">\(q\)</span>和<spanclass="math inline">\(k\)</span>的组成部分是独立的随机变量的组成部分是独立的随机变量，其均值为<span class="math inline">\(0\)</span>，方差为 <spanclass="math inline">\(1\)</span>。 那么它们的点积, <spanclass="math inline">\(q \cdot k = \sum_{i=1}^{d_k}q_ik_i\)</span>，其均值为<spanclass="math inline">\(0\)</span>，方差为<spanclass="math inline">\(d_k\)</span>。</p><p>为了抵消这种影响，我们将点积的比例定为 <spanclass="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>。</p><blockquote><p>关于 dot-product attention 为什么要 scale <spanclass="math inline">\(\frac{1}{\sqrt{d_k}}\)</span></p></blockquote><p>李沐老师在【Transformer论文逐段精读】中对这部分的解读摘录如下：</p><p>当<spanclass="math inline">\(d_k\)</span>不是很大的时候，除不除都没关系。但是当<spanclass="math inline">\(d_k\)</span>很大的时候，也就是向量较长，内积可能非常大。当内积值较大时，差距也会较大。</p><p>而又因为softmax的操作趋向于让大的更大，小的更小，也就是置信的地方更接近1，不置信的地方更接近0，由此得到收敛，因此梯度会很小甚至梯度消失，导致模型会很快“跑不动”，失去了学习的作用。</p><p><imgsrc="https://github.com/harvardnlp/annotated-transformer/blob/master/images/ModalNet-20.png?raw=1" /></p><p>Multi-head attention allows the model to jointly attend toinformation from different representation subspaces at differentpositions. With a single attention head, averaging inhibits this.</p><p><span class="math display">\[\mathrm{MultiHead}(Q, K, V) =    \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \\    \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i,VW^V_i)\]</span></p><p>Where the projections are parameter matrices <spanclass="math inline">\(W^Q_i \in \mathbb{R}^{d_{\text{model}} \timesd_k}\)</span>, <span class="math inline">\(W^K_i \in\mathbb{R}^{d_{\text{model}} \times d_k}\)</span>, <spanclass="math inline">\(W^V_i \in \mathbb{R}^{d_{\text{model}} \timesd_v}\)</span> and <span class="math inline">\(W^O \in \mathbb{R}^{hd_v\times d_{\text{model}}}\)</span>.</p><p>In this work we employ <span class="math inline">\(h=8\)</span>parallel attention layers, or heads. For each of these we use <spanclass="math inline">\(d_k=d_v=d_{\text{model}}/h=64\)</span>. Due to thereduced dimension of each head, the total computational cost is similarto that of single-head attention with full dimensionality.</p><blockquote><p>翻译</p></blockquote><p>Multi-head attention允许模型从不同位置的不同表示空间中联合利用信息。如果只是单头attention，那么平均将会抑制这种状况。</p><p><span class="math display">\[\mathrm{MultiHead}(Q, K, V) =    \mathrm{Concat}(\mathrm{head_1}, ..., \mathrm{head_h})W^O \\    \text{where}~\mathrm{head_i} = \mathrm{Attention}(QW^Q_i, KW^K_i,VW^V_i)\]</span></p><p>在这个文本中，投影矩阵分别为 <span class="math inline">\(W^Q_i \in\mathbb{R}^{d_{\text{model}} \times d_k}\)</span>、<spanclass="math inline">\(W^K_i \in \mathbb{R}^{d_{\text{model}} \timesd_k}\)</span>、<span class="math inline">\(W^V_i \in\mathbb{R}^{d_{\text{model}} \times d_v}\)</span> 和 <spanclass="math inline">\(W^O \in \mathbb{R}^{hd_v \timesd_{\text{model}}}\)</span>，其中 <spanclass="math inline">\(d_{\text{model}}\)</span> 表示模型的维度，<spanclass="math inline">\(d_k\)</span> 和 <spanclass="math inline">\(d_v\)</span>分别表示注意力机制中的查询、键和值向量的维度，<spanclass="math inline">\(h\)</span> 表示注意力头的数量。</p><p>在本论文中，我们使用<code>h = 8</code>个并行的注意力层，或者说注意力头。其中每一个注意力头，我们使用 <spanclass="math inline">\(d_k=d_v=d_{\text{model}}/h=64\)</span>。由于减少了每个头的维度，总的计算损失与单个完全维度的attention是相似的。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadedAttention</span>(nn.Module): </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, h, d_model, dropout=<span class="number">0.1</span></span>): </span><br><span class="line">    <span class="comment"># h=8, d_model=512</span></span><br><span class="line">    <span class="string">&quot;Take in model size and number of heads.&quot;</span> </span><br><span class="line">    <span class="built_in">super</span>(MultiHeadedAttention, self).__init__() </span><br><span class="line">    <span class="keyword">assert</span> d_model % h == <span class="number">0</span> <span class="comment"># We assume d_v always equals d_k 512%8=0</span></span><br><span class="line">    self.d_k = d_model // h <span class="comment"># d_k=512//8=64</span></span><br><span class="line">    self.h = h <span class="comment"># h=8</span></span><br><span class="line">    self.linears = clones(nn.Linear(d_model, d_model), <span class="number">4</span>) </span><br><span class="line">    <span class="comment"># 定义四个Linear networks, 每个的大小是(512, 512)的，</span></span><br><span class="line">    <span class="comment"># 每个Linear network里面有两类可训练参数，Weights，</span></span><br><span class="line">    <span class="comment"># 其大小为512*512，以及biases，其大小为d_model=512。</span></span><br><span class="line"></span><br><span class="line">    self.attn = <span class="literal">None</span> </span><br><span class="line">    self.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>): </span><br><span class="line">   <span class="comment"># 注意，输入query的形状类似于(30, 10, 512)，</span></span><br><span class="line">   <span class="comment"># key.size() ~ (30, 11, 512), </span></span><br><span class="line">   <span class="comment"># 以及value.size() ~ (30, 11, 512)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>: <span class="comment"># Same mask applied to all h heads. </span></span><br><span class="line">      mask = mask.unsqueeze(<span class="number">1</span>) <span class="comment"># mask后续细细分解。</span></span><br><span class="line">    nbatches = query.size(<span class="number">0</span>) <span class="comment">#e.g., nbatches=30</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1) Do all the linear projections in batch from </span></span><br><span class="line">    <span class="comment"># d_model =&gt; h x d_k </span></span><br><span class="line">    query, key, value = [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k)</span><br><span class="line">      .transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="keyword">for</span> l, x <span class="keyword">in</span> </span><br><span class="line">      <span class="built_in">zip</span>(self.linears, (query, key, value))] </span><br><span class="line">      <span class="comment"># 这里是形成Q、K、V矩阵的过程，也是前三个Linear Networks的具体应用，</span></span><br><span class="line">      <span class="comment"># 例如query=(30, 10, 512) -&gt; Linear network -&gt; (30, 10, 512) </span></span><br><span class="line">      <span class="comment"># -&gt; view -&gt; (30, 10, 8, 64) -&gt; transpose(1,2) -&gt; (30, 8, 10, 64)</span></span><br><span class="line">      <span class="comment"># 对于key和value，也是类似的，从(30, 11, 512) -&gt; (30, 8, 11, 64)。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2) Apply attention on all the projected vectors in batch. </span></span><br><span class="line">    x, self.attn = attention(query, key, value, mask=mask, </span><br><span class="line">      dropout=self.dropout) </span><br><span class="line">      <span class="comment"># 调用上面定义好的attention函数，输出的x形状为(30, 8, 10, 64)；</span></span><br><span class="line">      <span class="comment"># attn的形状为(30, 8, 10=target.seq.len, 11=src.seq.len)</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3) &quot;Concat&quot; using a view and apply a final linear. </span></span><br><span class="line">    x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(nbatches, -<span class="number">1</span>, self.h * self.d_k) </span><br><span class="line">      <span class="comment"># x ~ (30, 8, 10, 64) -&gt; transpose(1,2) -&gt; </span></span><br><span class="line">      <span class="comment"># (30, 10, 8, 64) -&gt; contiguous() and view -&gt; </span></span><br><span class="line">      <span class="comment"># (30, 10, 8*64) = (30, 10, 512)</span></span><br><span class="line">    <span class="keyword">return</span> self.linears[-<span class="number">1</span>](x) </span><br><span class="line">      <span class="comment"># 执行第四个Linear network，把(30, 10, 512)经过一次linear network，</span></span><br><span class="line">      <span class="comment"># 得到(30, 10, 512).</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">query, key, value = [l(x).view(nbatches, -<span class="number">1</span>, self.h, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>) </span><br><span class="line"><span class="keyword">for</span> l, x <span class="keyword">in</span> <span class="built_in">zip</span>(self.linears, (query, key, value))] </span><br></pre></td></tr></table></figure><p>该代码的意思是将传入的Q、K、V三维矩阵经过一层全连接层后，重塑为四维矩阵，并且四维矩阵的第二、三维转置。<strong>torch.view函数的功能和numpy.reshape的功能差不多。</strong>下图可以很好的解释数据结构：</p><p><imgsrc="https://img-blog.csdnimg.cn/img_convert/6caa97d4f13601dc7a8f2f6ccb96181c.png" /></p><p>如图所示，最后得到的结果是个四维矩阵，维度依次是B、h、F、d_k。</p><ul><li>B是Batch-size</li><li>h是多头自注意力机制中的头数</li><li>F是一个样本中的字符数，可以理解为一个句子中的单词个数</li><li>d_k是每个字符对应的embedding长度。</li></ul><p>理解了这个类，Transformer的精髓也就理解得差不多了</p><h3 id="applications-of-attention-in-our-model">Applications ofAttention in our Model</h3><p>The Transformer uses multi-head attention in three different ways: 1)In "encoder-decoder attention" layers, the queries come from theprevious decoder layer, and the memory keys and values come from theoutput of the encoder. This allows every position in the decoder toattend over all positions in the input sequence. This mimics the typicalencoder-decoder attention mechanisms in sequence-to-sequence models suchas <a href="https://arxiv.org/abs/1609.08144">(cite)</a>.</p><ol start="2" type="1"><li><p>The encoder contains self-attention layers. In a self-attentionlayer all of the keys, values and queries come from the same place, inthis case, the output of the previous layer in the encoder. Eachposition in the encoder can attend to all positions in the previouslayer of the encoder.</p></li><li><p>Similarly, self-attention layers in the decoder allow eachposition in the decoder to attend to all positions in the decoder up toand including that position. We need to prevent leftward informationflow in the decoder to preserve the auto-regressive property. Weimplement this inside of scaled dot-product attention by masking out(setting to <span class="math inline">\(-\infty\)</span>) all values inthe input of the softmax which correspond to illegalconnections.</p></li></ol><h2 id="position-wise-feed-forward-networks">Position-wise Feed-ForwardNetworks</h2><p>In addition to attention sub-layers, each of the layers in ourencoder and decoder contains a fully connected feed-forward network,which is applied to each position separately and identically. Thisconsists of two linear transformations with a ReLU activation inbetween.</p><p><span class="math display">\[\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2+ b_2\]</span></p><p>While the linear transformations are the same across differentpositions, they use different parameters from layer to layer. Anotherway of describing this is as two convolutions with kernel size 1. Thedimensionality of input and output is <spanclass="math inline">\(d_{\text{model}}=512\)</span>, and the inner-layerhas dimensionality <span class="math inline">\(d_{ff}=2048\)</span>.</p><blockquote><p>翻译</p></blockquote><p>除了 <code>attention sub-layers</code> 【这里的attentionsub-layers应该是一个名词】之外，encoder 和 decoder的每层包含了一个全连接前馈网络，它分别相同地应用到每一个位置。这包含一个用<code>ReLu</code>做连接的两个线性转换操作。</p><p><span class="math display">\[\mathrm{FFN}(x)=\max(0, xW_1 + b_1) W_2+ b_2\]</span></p><p>尽管不同的位置有着相同的线性转换，但是它们使用不同的参数从一层到另一层。另一种描述这个的方法是：可以将这个看成是两个卷积核大小为1的卷积。输入和输出的维度都是<spanclass="math inline">\(d_{\text{model}}=512\)</span>，同时，内部层维度是<span class="math inline">\(d_{ff}=2048\)</span></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implements FFN equation.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="comment"># d_model = 512</span></span><br><span class="line">        <span class="comment"># d_ff = 2048 = 512*4</span></span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, self).__init__()</span><br><span class="line">        self.w_1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        <span class="comment"># 构建第一个全连接层，(512, 2048)，其中有两种可训练参数：</span></span><br><span class="line">        <span class="comment"># weights矩阵，(512, 2048)，以及</span></span><br><span class="line">        <span class="comment"># biases偏移向量, (2048)</span></span><br><span class="line">        self.w_2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        <span class="comment"># 构建第二个全连接层, (2048, 512)，两种可训练参数：</span></span><br><span class="line">        <span class="comment"># weights矩阵，(2048, 512)，以及</span></span><br><span class="line">        <span class="comment"># biases偏移向量, (512)</span></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x shape = (batch.size, sequence.len, 512)</span></span><br><span class="line">        <span class="comment"># 例如, (30, 10, 512)</span></span><br><span class="line">        <span class="keyword">return</span> self.w_2(self.dropout(F.relu(self.w_1(x))))</span><br><span class="line">        <span class="comment"># x (30, 10, 512) -&gt; self.w_1 -&gt; (30, 10, 2048)</span></span><br><span class="line">        <span class="comment"># -&gt; relu -&gt; (30, 10, 2048) </span></span><br><span class="line">        <span class="comment"># -&gt; dropout -&gt; (30, 10, 2048)</span></span><br><span class="line">        <span class="comment"># -&gt; self.w_2 -&gt; (30, 10, 512)是输出的shape</span></span><br></pre></td></tr></table></figure><blockquote><p>关于FFN的理解</p></blockquote><p>实际上这就是一个MLP，但它是对每个词单独作用，并且保证对不同词作用的MLP参数相同。因此FFN实际就是一个线性层加上一个ReLU再加上一个线性层。<br />单隐藏层的 MLP，中间扩维到4倍 2048，最后投影回到 512维度大小，便于残差连接。</p><p><img src="https://img-blog.csdnimg.cn/2b45b8a6dbd44d29805b8b42268c2d93.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5omT552A54Gv56y85pG46buR,size_20,color_FFFFFF,t_70,g_se,x_16" width = "50%" /></p><p><img src="https://pic3.zhimg.com/80/v2-c9259cedcf5f77e244cb4ac946108cc6_1440w.webp" width = "50%" /></p><h2 id="embeddings-and-softmax">Embeddings and Softmax</h2><p>Similarly to other sequence transduction models, we use learnedembeddings to convert the input tokens and output tokens to vectors ofdimension <span class="math inline">\(d_{\text{model}}\)</span>. We alsouse the usual learned linear transformation and softmax function toconvert the decoder output to predicted next-token probabilities. In ourmodel, we share the same weight matrix between the two embedding layersand the pre-softmax linear transformation, similar to <ahref="https://arxiv.org/abs/1608.05859">(cite)</a>. In the embeddinglayers, we multiply those weights by <spanclass="math inline">\(\sqrt{d_{\text{model}}}\)</span>.</p><blockquote><p>翻译</p></blockquote><p>与序列推导模型相似，我们使用embeddings 去将 input tokens和 outputtokens转换成维度是 <span class="math inline">\({d_{model}}\)</span>的向量，我们同时使用通常学习的线性转换和softmax 函数，用于将decoder输出转换成一个可预测的接下来单词的概率。在我们的模型中：在两个embeddinglayers 和 pre-softmax 线性转换中共用相同的权重矩阵。在embeddinglayers，我们将这些权重乘以<span class="math inline">\(\sqrt{d_{model}}\)</span></p><blockquote><p>关于共享两个 Embedding 和 pre-softmax 权重矩阵</p></blockquote><p>换句话说，同一组权重被用来将源语言和目标语言的词转换为连续矢量表示，以及将连续表示转换为目标语言词的预测。这种共享权重的做法有助于模型更有效地学习，因为它可以在源语言和目标语言中使用共同的模式。</p><p>共享权重是否有用，取决于源语言和目标语言之间的关系。如果这些语言有类似的结构和许多共同的词根，就像你提到的欧洲语系一样（本文用到了英法、英德翻译），那么共享权重就会有好处。然而，如果这些语言非常不同，就像中文和英文，那么共享权重可能没有用，甚至可能损害性能。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,d_model,vocab</span>):</span><br><span class="line">    <span class="comment"># d_model=512, vocab=当前语言的词表大小</span></span><br><span class="line">    <span class="built_in">super</span>(Embeddings,self).__init__()</span><br><span class="line">    self.lut=nn.Embedding(vocab,d_model) </span><br><span class="line">    <span class="comment"># one-hot转词嵌入，这里有一个待训练的矩阵lut，大小是vocab*d_model</span></span><br><span class="line">    self.d_model=d_model <span class="comment"># 512</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>): </span><br><span class="line">     <span class="comment"># x ~ (batch.size, sequence.length, one-hot), </span></span><br><span class="line">     <span class="comment"># one-hot大小=vocab，当前语言的词表大小</span></span><br><span class="line"></span><br><span class="line">     <span class="keyword">return</span> self.lut(x)*math.sqrt(self.d_model) </span><br><span class="line">     <span class="comment"># 输出的tensor大小类似于(batch.size, sequence.length, 512)</span></span><br><span class="line">     <span class="comment"># 每个元素主动乘以sqrt(512)=22.6缩放比例</span></span><br></pre></td></tr></table></figure><p>Embeddings类的forward函数中的<code>math.sqrt(d_model)</code>的乘法是一种归一化技术，用于缩放embedding。这个缩放系数用于确保两个embedding之间的点积与模型内的其他点积具有相似的大小，这有助于稳定训练过程，防止梯度变得过大或过小。</p><p>在实践中，比例因子的具体数值并不关键，不同的模型和实验中也使用了不同的数值。选择<code>math.sqrt(d_model)</code>作为比例因子是基于变换器的模型领域中常见的，它被用来确保嵌入中的值与模型的其他部分处于相似的尺度。然而，如果需要的话，你可以使用一个不同的比例系数，或者甚至完全不使用比例系数，这取决于你的具体使用情况和要求。</p><h2 id="positional-encoding">Positional Encoding</h2><p>Since our model contains no recurrence and no convolution, in orderfor the model to make use of the order of the sequence, we must injectsome information about the relative or absolute position of the tokensin the sequence. To this end, we add "positional encodings" to the inputembeddings at the bottoms of the encoder and decoder stacks. Thepositional encodings have the same dimension <spanclass="math inline">\(d_{\text{model}}\)</span> as the embeddings, sothat the two can be summed. There are many choices of positionalencodings, learned and fixed <ahref="https://arxiv.org/pdf/1705.03122.pdf">(cite)</a>.</p><p>In this work, we use sine and cosine functions of differentfrequencies:</p><p><span class="math display">\[PE_{(pos,2i)} = \sin(pos /10000^{2i/d_{\text{model}}})\]</span></p><p><span class="math display">\[PE_{(pos,2i+1)} = \cos(pos /10000^{2i/d_{\text{model}}})\]</span></p><p>where <span class="math inline">\(pos\)</span> is the position and<span class="math inline">\(i\)</span> is the dimension. That is, eachdimension of the positional encoding corresponds to a sinusoid. Thewavelengths form a geometric progression from <spanclass="math inline">\(2\pi\)</span> to <span class="math inline">\(10000\cdot 2\pi\)</span>. We chose this function because we hypothesized itwould allow the model to easily learn to attend by relative positions,since for any fixed offset <span class="math inline">\(k\)</span>, <spanclass="math inline">\(PE_{pos+k}\)</span> can be represented as a linearfunction of <span class="math inline">\(PE_{pos}\)</span>.</p><p>In addition, we apply dropout to the sums of the embeddings and thepositional encodings in both the encoder and decoder stacks. For thebase model, we use a rate of <spanclass="math inline">\(P_{drop}=0.1\)</span>.</p><p>因为我们的模型没有包含RNN和CNN，为了让模型充分利用序列的顺序信息，我们必须获取一些信息关于tokens在序列中相对或者绝对的位置。为了这个目的，我们在encoder 和 decoder栈的底部 加上了<code>positional encodings</code>到 inputembeddings中。这个<code>positional embedding</code> 与<code>embedding</code>有相同的维度 <spanclass="math inline">\(d_{model}\)</span>。有许多关于<code>positional ecodings</code>的选择。</p><p>在本论文中，我们使用不同频率的sine 和 cosine 函数。公式如下：<br /><span class="math display">\[\begin{aligned} PE_{pos,2i} = sin( \frac{pos}{10000^{2i/d_{model}}}) \\PE_{pos,2i+1} = cos(\frac{pos}{10000^{2i/d_{model}}}) \end{aligned}\]</span></p><p>其中<code>pos</code>是位置，<code>i</code>是维度。也就是说：每个位置编码的维度对应一个正弦曲线。波长形成了一个几何级数从2π到 10000*2π。选择这个函数的原因是：我们假设它让我们的模型容易学习到位置信息，因为对任何一个固定的偏移k，<span class="math inline">\(PE_{pos+k}\)</span> 可以代表成一个 <spanclass="math inline">\(PE_{pos}\)</span> 的线性函数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module): </span><br><span class="line">  <span class="string">&quot;Implement the PE function.&quot;</span> </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>): </span><br><span class="line">    <span class="comment"># d_model=512,dropout=0.1,</span></span><br><span class="line">    <span class="comment"># max_len=5000，代表事先准备好长度为5000的序列的位置编码，其实没必要，一般100或者200足够了。</span></span><br><span class="line">    <span class="built_in">super</span>(PositionalEncoding, self).__init__() </span><br><span class="line">    self.dropout = nn.Dropout(p=dropout) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the positional encodings once in log space. </span></span><br><span class="line">    pe = torch.zeros(max_len, d_model) </span><br><span class="line">    <span class="comment"># (5000,512)矩阵，保持每个位置的位置编码，一共5000个位置，</span></span><br><span class="line">    <span class="comment"># 每个位置用一个512维度向量来表示其位置编码</span></span><br><span class="line">    position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>) </span><br><span class="line">    <span class="comment"># (5000) -&gt; (5000,1)</span></span><br><span class="line">    div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) * </span><br><span class="line">      -(math.log(<span class="number">10000.0</span>) / d_model)) </span><br><span class="line">    <span class="comment"># (0,2,…, 4998)一共准备2500个值，供sin, cos调用</span></span><br><span class="line">    <span class="comment"># 公式做了变形，详见下方说明</span></span><br><span class="line">    pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term) <span class="comment"># 偶数下标的位置</span></span><br><span class="line">    pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term) <span class="comment"># 奇数下标的位置</span></span><br><span class="line">    pe = pe.unsqueeze(<span class="number">0</span>) </span><br><span class="line">    <span class="comment"># (5000, 512) -&gt; (1, 5000, 512) 为batch.size留出位置</span></span><br><span class="line">    self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe) </span><br><span class="line">    <span class="comment"># The pe tensor is then registered as a buffer, </span></span><br><span class="line">    <span class="comment"># which means it will not be part of the model&#x27;s parameters </span></span><br><span class="line">    <span class="comment"># and will not require gradients during the backward pass.</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>): </span><br><span class="line">    x = x + Variable(self.pe[:, :x.size(<span class="number">1</span>)], requires_grad=<span class="literal">False</span>) </span><br><span class="line">    <span class="comment"># 接受 Embeddings 的词嵌入结果x，然后把自己的位置编码pe缩放成x.size(1)的长度，(封装成torch的Variable)，加上去</span></span><br><span class="line">    <span class="comment"># 由于self.pe[:, :x.size(1)]不是一个可学习的参数，所以设置requires_grad=False，表示不需要梯度。</span></span><br><span class="line">    <span class="comment"># 例如，假设x是(30,10,512)的一个tensor，30是batch.size, 10是该batch的序列长度, 512是每个词的词嵌入向量；</span></span><br><span class="line">    <span class="comment"># 则该行代码的第二项是(1, min(10, 5000), 512)=(1,10,512)，</span></span><br><span class="line">    <span class="comment"># 在具体相加的时候，会扩展(1,10,512)为(30,10,512)，保证一个batch中的30个序列，都使用（叠加）一样的位置编码。</span></span><br><span class="line">    <span class="keyword">return</span> self.dropout(x) <span class="comment"># 增加一次dropout操作</span></span><br><span class="line"><span class="comment"># 注意，位置编码不会更新，是写死的，所以这个class里面没有可训练的参数。</span></span><br></pre></td></tr></table></figure><p>为了计算这个公式，上面的公式做了一系列变形，因此代码可能第一眼看上去和公式不像，下面以<spanclass="math inline">\(2i\)</span>（偶数情况）为例子说明：</p><p><imgsrc="https://pic3.zhimg.com/v2-acd068d4806fbbe420f81210b495a432_b.jpg" /></p><p><imgsrc="https://pic3.zhimg.com/80/v2-acd068d4806fbbe420f81210b495a432_1440w.webp" /></p><p><strong>注意，位置编码不会更新，是写死的，所以这个class里面没有可训练的参数。</strong></p><p>有了 <code>Embeddings</code> 和 <code>PositionalEncoding</code></p><p>在具体使用的时候，是通过torch.nn.Sequential来把他们两个串起来的：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># in Full Model</span></span><br><span class="line"><span class="comment"># nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span></span><br><span class="line"><span class="comment"># nn.Sequential(Embeddings(d_model, tgt_vocab), c(position))</span></span><br></pre></td></tr></table></figure><blockquote><p>Below the positional encoding will add in a sine wave based onposition. The frequency and offset of the wave is different for eachdimension.</p></blockquote><blockquote><p>关于位置编码的可视化</p></blockquote><p>下面的位置编码将添加一个基于位置的正弦波。波的频率和偏移对于每个维度都是不同的。（最多）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">example_positional</span>():</span><br><span class="line">    pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>) </span><br><span class="line">    <span class="comment"># d_model=20, dropout=0,max_len=5000</span></span><br><span class="line">    y = pe.forward(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))</span><br><span class="line">    <span class="comment"># x是(1, 100, 20)的一个tensor，1是batch.size, 100是该batch的序列长度, 20是每个词的词嵌入向量d_model；</span></span><br><span class="line">    <span class="comment"># y (1, 100, 20),从d_model中选取dim</span></span><br><span class="line">    data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;embedding&quot;</span>: y[<span class="number">0</span>, :, dim],</span><br><span class="line">                    <span class="string">&quot;dimension&quot;</span>: dim,</span><br><span class="line">                    <span class="string">&quot;position&quot;</span>: <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">100</span>)),</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> dim <span class="keyword">in</span> [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">800</span>)</span><br><span class="line">        .encode(x=<span class="string">&quot;position&quot;</span>, y=<span class="string">&quot;embedding&quot;</span>, color=<span class="string">&quot;dimension:N&quot;</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(example_positional)</span><br></pre></td></tr></table></figure><p>We also experimented with using learned positional embeddings <ahref="https://arxiv.org/pdf/1705.03122.pdf">(cite)</a> instead, andfound that the two versions produced nearly identical results. We chosethe sinusoidal version because it may allow the model to extrapolate tosequence lengths longer than the ones encountered during training.</p><blockquote><p>翻译</p></blockquote><p>我们还尝试使用学习到的位置嵌入（另一种方式得到的positionalembeddding）来代替，发现这两个版本产生了几乎相同的结果。我们选择正弦版本是因为它可以让模型推断出比训练期间遇到的序列长度更长的序列长度。</p><blockquote><p>关于positional embedding</p></blockquote><p>位置嵌入是一个符号在序列中的位置的连续表示。它们被添加到符号的连续表示中（即词嵌入），以告知模型每个符号在输入序列中的相对或绝对位置。</p><p>有两种方法可以产生位置嵌入：学习型和正弦型。学习型位置嵌入是由神经网络生成的，并与模型的其他部分一起训练。正弦位置嵌入是使用一个数学函数生成的，如正弦或余弦，它将序列中的每个位置映射到一个独特的连续表示。</p><p>该论文的作者发现，这两个版本的位置嵌入在他们的实验中产生了几乎相同的结果。然而，他们选择了正弦波版本，因为它可能允许模型推断出比训练期间遇到的序列长度更长的序列。这是因为正弦函数是周期性的，可以无限期地重复模式，而学习的位置嵌入是由一个固定大小的神经网络产生的，可能不能很好地推广到更长的序列。</p><h2 id="full-model">Full Model</h2><blockquote><p>Here we define a function from hyperparameters to a full model.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_model</span>(<span class="params">src_vocab, tgt_vocab, N=<span class="number">6</span>, </span></span><br><span class="line"><span class="params">               d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;Helper: Construct a model from hyperparameters.&quot;</span></span><br><span class="line">    <span class="comment"># src_vocab = 源语言词表大小</span></span><br><span class="line">    <span class="comment"># tgt_vocab = 目标语言词表大小</span></span><br><span class="line">    </span><br><span class="line">    c = copy.deepcopy <span class="comment"># 对象的深度copy/clone</span></span><br><span class="line">    attn = MultiHeadedAttention(h, d_model) <span class="comment"># 8, 512</span></span><br><span class="line">    <span class="comment"># 构造一个MultiHeadAttention对象</span></span><br><span class="line">    </span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    <span class="comment"># 512, 2048, 0.1</span></span><br><span class="line">    <span class="comment"># 构造一个feed forward对象</span></span><br><span class="line"></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    <span class="comment"># 位置编码</span></span><br><span class="line"></span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), </span><br><span class="line">                             c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># This was important from their code. </span></span><br><span class="line">    <span class="comment"># Initialize parameters with Glorot / fan_avg.</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform(p)</span><br><span class="line">    <span class="keyword">return</span> model <span class="comment"># EncoderDecoder 对象</span></span><br></pre></td></tr></table></figure><blockquote><p>关于参数初始化</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">  <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">    nn.init.xavier_uniform(p)</span><br></pre></td></tr></table></figure><p>这段代码的作用是初始化Transformer模型的参数。"Glorot /fan_avg"是一种常用的初始化方法，也被称为"Xavierinitialization"。该方法的目的是使得模型的参数有合适的初始值，以避免梯度消失和爆炸的问题。</p><p>因此，这段代码的重要性在于它为Transformer模型的学习提供了合适的初始条件，从而提高模型的学习效率和精度。在机器学习领域，初始参数的选择对于模型的效果有很大影响，因此该代码是很重要的。</p><h2 id="inference">Inference:</h2><blockquote><p>Here we make a forward step to generate a prediction of the model. Wetry to use our transformer to memorize the input. As you will see theoutput is randomly generated due to the fact that the model is nottrained yet. In the next tutorial we will build the training functionand try to train our model to memorize the numbers from 1 to 10.</p></blockquote><blockquote><p>在这里，我们向前迈出一步以生成模型的预测。我们尝试使用我们的转换器来记住输入。正如您将看到的那样，由于模型尚未训练，输出是随机生成的。在下一个教程中，我们将构建训练函数并尝试训练我们的模型来记住从1 到 10 的数字。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inference_test</span>():</span><br><span class="line">    test_model = make_model(<span class="number">11</span>, <span class="number">11</span>, <span class="number">2</span>)</span><br><span class="line">    test_model.<span class="built_in">eval</span>()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    memory = test_model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">        out = test_model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = test_model.generator(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.empty(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Example Untrained Model Prediction:&quot;</span>, ys)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_tests</span>():</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        inference_test()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(run_tests)</span><br></pre></td></tr></table></figure><h1 id="第二部分-模型训练">第二部分: 模型训练</h1><h1 id="训练">训练</h1><p>This section describes the training regime （训练机制） for ourmodels.</p><blockquote><p>We stop for a quick interlude to introduce some of the tools neededto train a standard encoder decoder model. First we define a batchobject that holds the src and target sentences for training, as well asconstructing the masks.</p></blockquote><blockquote><p>我们暂时停下来介绍一些训练标准编码器解码器模型所需的工具。首先，我们定义一个批处理对象，其中包含用于训练的src 和目标句子，以及构建掩码。</p></blockquote><h2 id="batches-and-masking">Batches and Masking</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Batch</span>:</span><br><span class="line">    <span class="string">&quot;Object for holding a batch of data with mask during training.&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, src, trg=<span class="literal">None</span>, pad=<span class="number">0</span></span>):</span><br><span class="line">        <span class="comment"># src: 源语言序列，(batch.size, src.seq.len)</span></span><br><span class="line">        <span class="comment"># 二维tensor，第一维度是batch.size；第二个维度是源语言句子的长度</span></span><br><span class="line">        <span class="comment"># 例如：[ [2,1,3,4], [2,3,1,4] ]这样的二行四列的，</span></span><br><span class="line">        <span class="comment"># 1-4代表每个单词word的id</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># trg: 目标语言序列，默认为空，其shape和src类似</span></span><br><span class="line">        <span class="comment"># (batch.size, trg.seq.len)，</span></span><br><span class="line">        <span class="comment"># 二维tensor，第一维度是batch.size；第二个维度是目标语言句子的长度</span></span><br><span class="line">        <span class="comment"># 例如trg=[ [2,1,3,4], [2,3,1,4] ] for a &quot;copy network&quot;</span></span><br><span class="line">        <span class="comment"># (输出序列和输入序列完全相同）</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># pad: 源语言和目标语言统一使用的 位置填充符号，&#x27;&lt;blank&gt;&#x27;</span></span><br><span class="line">        <span class="comment"># 所对应的id，这里默认为0</span></span><br><span class="line">        <span class="comment"># 例如，如果一个source sequence，长度不到4，则在右边补0</span></span><br><span class="line">        <span class="comment"># [1,2] -&gt; [1,2,0,0]</span></span><br><span class="line">        </span><br><span class="line">        self.src = src</span><br><span class="line">        self.src_mask = (src != pad).unsqueeze(-<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># src = (batch.size, seq.len) -&gt; != pad -&gt; </span></span><br><span class="line">        <span class="comment"># (batch.size, seq.len) -&gt; usnqueeze -&gt;</span></span><br><span class="line">        <span class="comment"># (batch.size, 1, seq.len) 相当于在倒数第二个维度扩展</span></span><br><span class="line">        <span class="comment"># e.g., src=[ [2,1,3,4], [2,3,1,4] ]对应的是</span></span><br><span class="line">        <span class="comment"># src_mask=[ [[1,1,1,1], [1,1,1,1]] ]</span></span><br><span class="line">        <span class="keyword">if</span> trg <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            self.trg = trg[:, :-<span class="number">1</span>] <span class="comment"># 重要</span></span><br><span class="line">            <span class="comment"># trg 相当于目标序列的前N-1个单词的序列</span></span><br><span class="line">            <span class="comment">#（去掉了最后一个词）</span></span><br><span class="line">            self.trg_y = trg[:, <span class="number">1</span>:]</span><br><span class="line">            <span class="comment"># trg_y 相当于目标序列的后N-1个单词的序列</span></span><br><span class="line">            <span class="comment"># (去掉了第一个词&lt;start&gt;）</span></span><br><span class="line">            <span class="comment"># 目的是(src + trg) 来预测出(trg_y)，</span></span><br><span class="line">            <span class="comment"># 这个在illustrated transformer中详细图示过。</span></span><br><span class="line">            self.trg_mask = \</span><br><span class="line">                self.make_std_mask(self.trg, pad)</span><br><span class="line">            self.ntokens = (self.trg_y != pad).data.<span class="built_in">sum</span>()</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_std_mask</span>(<span class="params">tgt, pad</span>):</span><br><span class="line">        <span class="string">&quot;Create a mask to hide padding and future words.&quot;</span></span><br><span class="line">        <span class="comment"># 这里的tgt类似于：</span></span><br><span class="line">        <span class="comment"># [ [2,1,3], [2,3,1] ] （最初的输入目标序列，分别去掉了最后一个词</span></span><br><span class="line">        <span class="comment"># pad=0, &#x27;&lt;blank&gt;&#x27;的id编号</span></span><br><span class="line">        tgt_mask = (tgt != pad).unsqueeze(-<span class="number">2</span>)</span><br><span class="line">        <span class="comment"># 得到的tgt_mask类似于</span></span><br><span class="line">        <span class="comment"># tgt_mask = tensor([[[1, 1, 1]],[[1, 1, 1]]], dtype=torch.uint8)</span></span><br><span class="line">        <span class="comment"># shape=(2,1,3)</span></span><br><span class="line">        tgt_mask = tgt_mask &amp; Variable(</span><br><span class="line">            subsequent_mask(tgt.size(-<span class="number">1</span>)).type_as(tgt_mask.data))</span><br><span class="line">        <span class="comment"># 先看subsequent_mask, 其输入的是tgt.size(-1)=3</span></span><br><span class="line">        <span class="comment"># 这个函数的输出为= tensor([[[1, 0, 0],</span></span><br><span class="line">        <span class="comment"># [1, 1, 0],</span></span><br><span class="line">        <span class="comment"># [1, 1, 1]]], dtype=torch.uint8)</span></span><br><span class="line">        <span class="comment"># type_as 把这个tensor转成tgt_mask.data的type(也是torch.uint8)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 这样的话，&amp;的两边的tensor.shape分别是(2,1,3), (1,3,3);</span></span><br><span class="line">        <span class="comment"># tgt_mask = tensor([[[1, 1, 1]],[[1, 1, 1]]], dtype=torch.uint8)</span></span><br><span class="line">        <span class="comment"># and</span></span><br><span class="line">        <span class="comment"># tensor([[[1, 0, 0], [1, 1, 0], [1, 1, 1]]], dtype=torch.uint8)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># (2,3,3)就是得到的tensor</span></span><br><span class="line">        <span class="comment"># tgt_mask.data = tensor(</span></span><br><span class="line">        <span class="comment"># [[1, 0, 0],</span></span><br><span class="line">        <span class="comment"># [1, 1, 0],</span></span><br><span class="line">        <span class="comment"># [1, 1, 1]]], dtype=torch.uint8)</span></span><br><span class="line">        <span class="keyword">return</span> tgt_mask</span><br></pre></td></tr></table></figure><blockquote><p>关于"Batch"类</p></blockquote><p>用于存储和处理训练数据</p><p><code>__init__</code>方法初始化了Batch对象的各个成员变量：</p><ul><li>src：源语言序列</li><li>src_mask：源语言序列的掩码，用于表示序列中的某些位置是否有效</li><li>trg：目标语言序列（可选）</li><li>trg_y：目标语言序列的标签，即目标语言序列的下一个词</li><li>trg_mask：目标语言序列的掩码，表示该序列中哪些位置是有效的，用于注意力机制</li><li>ntokens：表示目标语言序列中有效词的数量</li></ul><p><code>make_std_mask</code>是一个静态方法，用于创建目标语言序列的mask。该方法的作用是防止注意力机制关注未来的预测词。它创建了一个mask，通过将目标语言序列中有效位置与subsequent_mask进行位运算来实现该目的。</p><blockquote><p>Next we create a generic training and scoring function to keep trackof loss. We pass in a generic loss compute function that also handlesparameter updates.</p></blockquote><blockquote><p>接下来我们创建一个通用的训练和评分函数来跟踪损失。我们传入了一个通用的损失计算函数，该函数也处理参数更新。</p></blockquote><h2 id="training-loop">Training Loop</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TrainState</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Track number of steps, examples, and tokens processed&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    step: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># Steps in the current epoch</span></span><br><span class="line">    accum_step: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># Number of gradient accumulation steps</span></span><br><span class="line">    samples: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># total # of examples used</span></span><br><span class="line">    tokens: <span class="built_in">int</span> = <span class="number">0</span>  <span class="comment"># total # of tokens processed</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">run_epoch</span>(<span class="params"></span></span><br><span class="line"><span class="params">    data_iter,</span></span><br><span class="line"><span class="params">    model,</span></span><br><span class="line"><span class="params">    loss_compute,</span></span><br><span class="line"><span class="params">    optimizer,</span></span><br><span class="line"><span class="params">    scheduler,</span></span><br><span class="line"><span class="params">    mode=<span class="string">&quot;train&quot;</span>,</span></span><br><span class="line"><span class="params">    accum_iter=<span class="number">1</span>,</span></span><br><span class="line"><span class="params">    train_state=TrainState(<span class="params"></span>),</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;Train a single epoch&quot;&quot;&quot;</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    total_tokens = <span class="number">0</span></span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    tokens = <span class="number">0</span></span><br><span class="line">    n_accum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(data_iter):</span><br><span class="line">        out = model.forward(</span><br><span class="line">            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask</span><br><span class="line">        )</span><br><span class="line">        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)</span><br><span class="line">        <span class="comment"># loss_node = loss_node / accum_iter</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span> <span class="keyword">or</span> mode == <span class="string">&quot;train+log&quot;</span>:</span><br><span class="line">            loss_node.backward()</span><br><span class="line">            train_state.step += <span class="number">1</span></span><br><span class="line">            train_state.samples += batch.src.shape[<span class="number">0</span>]</span><br><span class="line">            train_state.tokens += batch.ntokens</span><br><span class="line">            <span class="keyword">if</span> i % accum_iter == <span class="number">0</span>:</span><br><span class="line">                optimizer.step()</span><br><span class="line">                optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line">                n_accum += <span class="number">1</span></span><br><span class="line">                train_state.accum_step += <span class="number">1</span></span><br><span class="line">            scheduler.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss</span><br><span class="line">        total_tokens += batch.ntokens</span><br><span class="line">        tokens += batch.ntokens</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">40</span> == <span class="number">1</span> <span class="keyword">and</span> (mode == <span class="string">&quot;train&quot;</span> <span class="keyword">or</span> mode == <span class="string">&quot;train+log&quot;</span>):</span><br><span class="line">            lr = optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>]</span><br><span class="line">            elapsed = time.time() - start</span><br><span class="line">            <span class="built_in">print</span>(</span><br><span class="line">                (</span><br><span class="line">                    <span class="string">&quot;Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f &quot;</span></span><br><span class="line">                    + <span class="string">&quot;| Tokens / Sec: %7.1f | Learning Rate: %6.1e&quot;</span></span><br><span class="line">                )</span><br><span class="line">                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)</span><br><span class="line">            )</span><br><span class="line">            start = time.time()</span><br><span class="line">            tokens = <span class="number">0</span></span><br><span class="line">        <span class="keyword">del</span> loss</span><br><span class="line">        <span class="keyword">del</span> loss_node</span><br><span class="line">    <span class="keyword">return</span> total_loss / total_tokens, train_state</span><br></pre></td></tr></table></figure><blockquote><p>关于 run_epoch</p></blockquote><p>这段代码实现了单个epoch的训练。它遍历了data_iter中的所有batch，对于每个batch，它调用模型的forward方法，获得输出，然后计算loss并使用loss进行backward。在每个batch上，它使用优化器(本文中为Adam)更新模型的参数，并使用学习率调度器调整学习率。</p><p>需要注意的是，在训练模式下，如果批次数是指定的累积次数的倍数，则进行一次优化步骤，并将梯度归零。此外，如果指定了"train+log"模式，则每隔40个批次，将输出当前的学习率，损失，以及每秒处理的token数。</p><p>最后，该函数返回平均每个token的损失和训练状态。</p><h2 id="training-data-and-batching">Training Data and Batching</h2><p>We trained on the standard WMT 2014 English-German dataset consistingof about 4.5 million sentence pairs. Sentences were encoded usingbyte-pair encoding, which has a shared source-target vocabulary of about37000 tokens. For English-French, we used the significantly larger WMT2014 English-French dataset consisting of 36M sentences and split tokensinto a 32000 word-piece vocabulary.</p><p>Sentence pairs were batched together by approximate sequence length.Each training batch contained a set of sentence pairs containingapproximately 25000 source tokens and 25000 target tokens.</p><h2 id="hardware-and-schedule">Hardware and Schedule</h2><p>We trained our models on one machine with 8 NVIDIA P100 GPUs. For ourbase models using the hyperparameters described throughout the paper,each training step took about 0.4 seconds. We trained the base modelsfor a total of 100,000 steps or 12 hours. For our big models, step timewas 1.0 seconds. The big models were trained for 300,000 steps (3.5days).</p><h2 id="optimizer">Optimizer</h2><p>We used the Adam optimizer <ahref="https://arxiv.org/abs/1412.6980">(cite)</a> with <spanclass="math inline">\(\beta_1=0.9\)</span>, <spanclass="math inline">\(\beta_2=0.98\)</span> and <spanclass="math inline">\(\epsilon=10^{-9}\)</span>. We varied the learningrate over the course of training, according to the formula:</p><p><span class="math display">\[lrate = d_{\text{model}}^{-0.5} \cdot  \min({step\_num}^{-0.5},    {step\_num} \cdot {warmup\_steps}^{-1.5})\]</span></p><p>This corresponds to increasing the learning rate linearly for thefirst <span class="math inline">\(warmup\_steps\)</span> training steps,and decreasing it thereafter proportionally to the inverse square rootof the step number. We used <spanclass="math inline">\(warmup\_steps=4000\)</span>.</p><blockquote><p>Note: This part is very important. Need to train with this setup ofthe model.</p></blockquote><blockquote><p>Example of the curves of this model for different model sizes and foroptimization hyperparameters.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rate</span>(<span class="params">step, model_size, factor, warmup</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    we have to default the step to 1 for LambdaLR function</span></span><br><span class="line"><span class="string">    to avoid zero raising to negative power.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">        step = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> factor * (</span><br><span class="line">        model_size ** (-<span class="number">0.5</span>) * <span class="built_in">min</span>(step ** (-<span class="number">0.5</span>), step * warmup ** (-<span class="number">1.5</span>))</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><blockquote><p>关于 rate</p></blockquote><p>这段代码实现了一个学习率策略，具体来说是返回了一个学习率的值，该学习率策略被用于在训练中动态调整学习率以提高模型的性能。</p><p>具体来说，这个函数使用了两种不同的学习率策略。在开始的 warmup过程中，学习率随着训练步数增加而逐渐增加。这在模型开始训练之前有助于防止在非常小的学习率下的不稳定性。在warmup过程结束后，学习率随着训练步数增加而逐渐减小。这有助于在训练中更稳定地对模型进行优化。</p><p>在该函数中，参数 model_size 是模型的大小，参数 factor是用于调整学习率的系数，参数 warmup 是 warmup 过程的步数，参数 step是当前训练步数。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">example_learning_schedule</span>():</span><br><span class="line">    opts = [</span><br><span class="line">        [<span class="number">512</span>, <span class="number">1</span>, <span class="number">4000</span>],  <span class="comment"># example 1</span></span><br><span class="line">        [<span class="number">512</span>, <span class="number">1</span>, <span class="number">8000</span>],  <span class="comment"># example 2</span></span><br><span class="line">        [<span class="number">256</span>, <span class="number">1</span>, <span class="number">4000</span>],  <span class="comment"># example 3</span></span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    dummy_model = torch.nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    learning_rates = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># we have 3 examples in opts list.</span></span><br><span class="line">    <span class="keyword">for</span> idx, example <span class="keyword">in</span> <span class="built_in">enumerate</span>(opts):</span><br><span class="line">        <span class="comment"># run 20000 epoch for each example</span></span><br><span class="line">        optimizer = torch.optim.Adam(</span><br><span class="line">            dummy_model.parameters(), lr=<span class="number">1</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">        )</span><br><span class="line">        lr_scheduler = LambdaLR(</span><br><span class="line">            optimizer=optimizer, lr_lambda=<span class="keyword">lambda</span> step: rate(step, *example)</span><br><span class="line">        )</span><br><span class="line">        tmp = []</span><br><span class="line">        <span class="comment"># take 20K dummy training steps, save the learning rate at each step</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20000</span>):</span><br><span class="line">            tmp.append(optimizer.param_groups[<span class="number">0</span>][<span class="string">&quot;lr&quot;</span>])</span><br><span class="line">            optimizer.step()</span><br><span class="line">            lr_scheduler.step()</span><br><span class="line">        learning_rates.append(tmp)</span><br><span class="line"></span><br><span class="line">    learning_rates = torch.tensor(learning_rates)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Enable altair to handle more than 5000 rows</span></span><br><span class="line">    alt.data_transformers.disable_max_rows()</span><br><span class="line"></span><br><span class="line">    opts_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;Learning Rate&quot;</span>: learning_rates[warmup_idx, :],</span><br><span class="line">                    <span class="string">&quot;model_size:warmup&quot;</span>: [<span class="string">&quot;512:4000&quot;</span>, <span class="string">&quot;512:8000&quot;</span>, <span class="string">&quot;256:4000&quot;</span>][</span><br><span class="line">                        warmup_idx</span><br><span class="line">                    ],</span><br><span class="line">                    <span class="string">&quot;step&quot;</span>: <span class="built_in">range</span>(<span class="number">20000</span>),</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> warmup_idx <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(opts_data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">600</span>)</span><br><span class="line">        .encode(x=<span class="string">&quot;step&quot;</span>, y=<span class="string">&quot;Learning Rate&quot;</span>, color=<span class="string">&quot;model_size:warmup:N&quot;</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">example_learning_schedule()</span><br></pre></td></tr></table></figure><p>这个函数生成的图像说明的是不同的模型大小和不同的 warmup设置对学习率变化的影响。代码中有三个 example，分别对应 model_size 为512，256， warmup 为 4000，8000，4000。运行代码会返回一张图，横轴是20,000步训练中每一步，纵轴是当前的学习率，图上每条曲线代表不同的模型大小和warmup 设置。颜色代表了不同的模型大小和 warmup设置的组合。该图显示了学习率的变化趋势，从大到小，最终逐渐接近一个稳定值。</p><h2 id="regularization">Regularization</h2><h3 id="label-smoothing">Label Smoothing</h3><p>During training, we employed label smoothing of value <spanclass="math inline">\(\epsilon_{ls}=0.1\)</span> <ahref="https://arxiv.org/abs/1512.00567">(cite)</a>. This hurtsperplexity, as the model learns to be more unsure, but improves accuracyand BLEU score.</p><blockquote><p>We implement label smoothing using the KL div loss. Instead of usinga one-hot target distribution, we create a distribution that has<code>confidence</code> of the correct word and the rest of the<code>smoothing</code> mass distributed throughout the vocabulary.</p></blockquote><blockquote><p>我们使用 KL div损失实现标签平滑。我们没有使用one-hot目标分布，而是创建了一个分布，该分布对正确的词具有置信度，其余的平滑质量分布在整个词汇表中。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LabelSmoothing</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implement label smoothing.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, padding_idx, smoothing=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LabelSmoothing, self).__init__()</span><br><span class="line">        self.criterion = nn.KLDivLoss(reduction=<span class="string">&quot;sum&quot;</span>)</span><br><span class="line">        self.padding_idx = padding_idx</span><br><span class="line">        self.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        self.smoothing = smoothing</span><br><span class="line">        self.size = size</span><br><span class="line">        self.true_dist = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, target</span>):</span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == self.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(self.smoothing / (self.size - <span class="number">2</span>))</span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), self.confidence)</span><br><span class="line">        true_dist[:, self.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == self.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        self.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> self.criterion(x, true_dist.clone().detach())</span><br></pre></td></tr></table></figure><blockquote><p>关于 Label Smoothing</p></blockquote><p>LabelSmoothing是一种常用的数据增强方法，目的是通过降低标签的置信度来抵抗过拟合。</p><p>上面的代码实现了这一方法，在构造函数中定义了一个KLDivLoss损失函数，表示用于计算两个概率分布的KL散度的损失。然后定义了几个基本的参数：size，表示每个输入的预测分布的维数；padding_idx，表示序列的填充值在预测分布中的位置；smoothing，表示要在每个预测分布上进行的平滑的系数；confidence，表示对真实标签的置信度。</p><p>在前向传播中，首先确保输入的维数是正确的，然后复制预测分布并将其全部填充为平滑系数除以size-2的值。接下来，在真实标签的位置增加置信度，在填充值的位置设置为0。然后使用蒙版确保对填充值进行正确的处理。最后计算KLDivLoss损失。</p><blockquote><p>Here we can see an example of how the mass is distributed to thewords based on confidence.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Example of label smoothing.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_label_smoothing</span>():</span><br><span class="line">    crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.4</span>)</span><br><span class="line">    predict = torch.FloatTensor(</span><br><span class="line">        [</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line">    crit(x=predict.log(), target=torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">3</span>]))</span><br><span class="line">    LS_data = pd.concat(</span><br><span class="line">        [</span><br><span class="line">            pd.DataFrame(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;target distribution&quot;</span>: crit.true_dist[x, y].flatten(),</span><br><span class="line">                    <span class="string">&quot;columns&quot;</span>: y,</span><br><span class="line">                    <span class="string">&quot;rows&quot;</span>: x,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)</span><br><span class="line">            <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>)</span><br><span class="line">        ]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(LS_data)</span><br><span class="line">        .mark_rect(color=<span class="string">&quot;Blue&quot;</span>, opacity=<span class="number">1</span>)</span><br><span class="line">        .properties(height=<span class="number">200</span>, width=<span class="number">200</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            alt.X(<span class="string">&quot;columns:O&quot;</span>, title=<span class="literal">None</span>),</span><br><span class="line">            alt.Y(<span class="string">&quot;rows:O&quot;</span>, title=<span class="literal">None</span>),</span><br><span class="line">            alt.Color(</span><br><span class="line">                <span class="string">&quot;target distribution:Q&quot;</span>, scale=alt.Scale(scheme=<span class="string">&quot;viridis&quot;</span>)</span><br><span class="line">            ),</span><br><span class="line">        )</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(example_label_smoothing)</span><br></pre></td></tr></table></figure><blockquote><p>关于 example_label_smoothing 的图像解释</p></blockquote><p>这张图是使用Altair库生成的，展示了一个模拟的LabelSmoothing例子中的目标分布与预测分布之间的差异。</p><p>它使用了5个类别和一组预测分布，并计算了通过LabelSmoothing损失函数产生的目标分布。目标分布是一个5x5的矩阵，每个数字代表了一个类别的置信度。</p><p>图中的矩形的颜色代表了目标分布的值，从浅到深的颜色代表了从小到大的置信度。由于图中的数字是实际的数字，因此可以使用Altair生成的交互式图像来查看每个数字的实际值。</p><blockquote><p>Label smoothing actually starts to penalize the model if it gets veryconfident about a given choice.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x, crit</span>):</span><br><span class="line">    d = x + <span class="number">3</span> * <span class="number">1</span></span><br><span class="line">    predict = torch.FloatTensor([[<span class="number">0</span>, x / d, <span class="number">1</span> / d, <span class="number">1</span> / d, <span class="number">1</span> / d]])</span><br><span class="line">    <span class="keyword">return</span> crit(predict.log(), torch.LongTensor([<span class="number">1</span>])).data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">penalization_visualization</span>():</span><br><span class="line">    crit = LabelSmoothing(<span class="number">5</span>, <span class="number">0</span>, <span class="number">0.1</span>)</span><br><span class="line">    loss_data = pd.DataFrame(</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;Loss&quot;</span>: [loss(x, crit) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">100</span>)],</span><br><span class="line">            <span class="string">&quot;Steps&quot;</span>: <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="number">99</span>)),</span><br><span class="line">        &#125;</span><br><span class="line">    ).astype(<span class="string">&quot;float&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(loss_data)</span><br><span class="line">        .mark_line()</span><br><span class="line">        .properties(width=<span class="number">350</span>)</span><br><span class="line">        .encode(</span><br><span class="line">            x=<span class="string">&quot;Steps&quot;</span>,</span><br><span class="line">            y=<span class="string">&quot;Loss&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(penalization_visualization)</span><br></pre></td></tr></table></figure><blockquote><p>关于 penalization_visualization 的图像解释</p></blockquote><p>这张图展示了一组使用标签平滑的损失函数的结果，它显示了标签平滑的惩罚效果。</p><p>x 表示对预测为正确类别的信心程度，从 1 到 100增加，代表对预测为正确类别的信心程度不断增加。损失函数 Loss 由函数 loss计算得出。图上显示的折线图每一步增加 x，相应地，Loss 也在不断减少。</p><p>这表明，随着对正确类别的信心程度的增加，标签平滑算法惩罚效果越弱。因此，如果对正确类别的预测信心很高，那么损失值就很低；如果对正确类别的预测信心很低，则损失值会很高。</p><p>总的来说，这张图显示了标签平滑惩罚效果的变化情况，可以帮助我们更好地理解标签平滑算法的工作原理。</p><h1 id="第一个例子">第一个例子</h1><blockquote><p>We can begin by trying out a simple copy-task. Given a random set ofinput symbols from a small vocabulary, the goal is to generate backthose same symbols.</p></blockquote><h2 id="synthetic-data">Synthetic Data</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_gen</span>(<span class="params">V, batch_size, nbatches</span>):</span><br><span class="line">    <span class="string">&quot;Generate random data for a src-tgt copy task.&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(nbatches):</span><br><span class="line">        data = torch.randint(<span class="number">1</span>, V, size=(batch_size, <span class="number">10</span>))</span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        src = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        tgt = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="loss-computation">Loss Computation</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleLossCompute</span>:</span><br><span class="line">    <span class="string">&quot;A simple loss compute and train function.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, generator, criterion</span>):</span><br><span class="line">        self.generator = generator</span><br><span class="line">        self.criterion = criterion</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x, y, norm</span>):</span><br><span class="line">        x = self.generator(x)</span><br><span class="line">        sloss = (</span><br><span class="line">            self.criterion(</span><br><span class="line">                x.contiguous().view(-<span class="number">1</span>, x.size(-<span class="number">1</span>)), y.contiguous().view(-<span class="number">1</span>)</span><br><span class="line">            )</span><br><span class="line">            / norm</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span> sloss.data * norm, sloss</span><br></pre></td></tr></table></figure><h2 id="greedy-decoding">Greedy Decoding</h2><blockquote><p>This code predicts a translation using greedy decoding forsimplicity.</p></blockquote><blockquote><p>为简单起见，此代码使用贪婪解码预测翻译。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decode</span>(<span class="params">model, src, src_mask, max_len, start_symbol</span>):</span><br><span class="line">    memory = model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_len - <span class="number">1</span>):</span><br><span class="line">        out = model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = model.generator(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> ys</span><br></pre></td></tr></table></figure><blockquote><p>关于 greedy_decode</p></blockquote><p>这段代码实现了贪心算法的解码，这是机器翻译等任务中常用的算法。</p><p>该函数接受以下参数：</p><ul><li>model：神经网络模型，需要有encode()和decode()方法</li><li>src：输入的数据</li><li>src_mask：输入数据的掩码</li><li>max_len：解码的最大长度</li><li>start_symbol：开始符</li></ul><p>该函数通过调用模型的encode方法，将输入编码为内存。然后，在一个for循环中进行解码，每一步用model的decode方法计算输出，再用model的generator方法预测下一个词的概率分布。最后选择概率最大的词作为下一个词，继续迭代。最终将解码的结果返回。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Train the simple copy task.</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example_simple_model</span>():</span><br><span class="line">    V = <span class="number">11</span></span><br><span class="line">    criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line">    model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        model.parameters(), lr=<span class="number">0.5</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = LambdaLR(</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        lr_lambda=<span class="keyword">lambda</span> step: rate(</span><br><span class="line">            step, model_size=model.src_embed[<span class="number">0</span>].d_model, factor=<span class="number">1.0</span>, warmup=<span class="number">400</span></span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">80</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">        model.train()</span><br><span class="line">        run_epoch(</span><br><span class="line">            data_gen(V, batch_size, <span class="number">20</span>),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(model.generator, criterion),</span><br><span class="line">            optimizer,</span><br><span class="line">            lr_scheduler,</span><br><span class="line">            mode=<span class="string">&quot;train&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        run_epoch(</span><br><span class="line">            data_gen(V, batch_size, <span class="number">5</span>),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(model.generator, criterion),</span><br><span class="line">            DummyOptimizer(),</span><br><span class="line">            DummyScheduler(),</span><br><span class="line">            mode=<span class="string">&quot;eval&quot;</span>,</span><br><span class="line">        )[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">    max_len = src.shape[<span class="number">1</span>]</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, max_len)</span><br><span class="line">    <span class="built_in">print</span>(greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># execute_example(example_simple_model)</span></span><br></pre></td></tr></table></figure><blockquote><p>关于 example_simple_model</p></blockquote><p>这是一个简单的模型训练的示例，它使用了一个称为 LabelSmoothing的损失函数，一个名为 make_model 的模型生成函数以及一个名为 data_gen的数据生成器。</p><p>其中，模型会被训练 20 次，每次运行一个 epoch，并使用 run_epoch函数进行处理。训练时使用 Adam 优化器，学习率可以使用 LambdaLR动态调整。批次大小为 80，每个 epoch 由 20 批数据训练，以及 5批数据评估。</p><p>最后，使用 greedy_decode函数，根据训练好的模型进行解码，并将结果打印输出。</p><h1 id="第三部分-真实世界的例子">第三部分: 真实世界的例子</h1><blockquote><p>Now we consider a real-world example using the Multi30kGerman-English Translation task. This task is much smaller than the WMTtask considered in the paper, but it illustrates the whole system. Wealso show how to use multi-gpu processing to make it really fast.</p></blockquote><blockquote><p>现在我们考虑使用 Multi30k德语-英语翻译任务的真实示例。该任务比论文中考虑的 WMT任务小得多，但它说明了整个系统。我们还展示了如何使用多 GPU处理使其真正快速。</p></blockquote><h2 id="data-loading">Data Loading</h2><blockquote><p>We will load the dataset using torchtext and spacy fortokenization.</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Load spacy tokenizer models, download them if they haven&#x27;t been</span></span><br><span class="line"><span class="comment"># downloaded already</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_tokenizers</span>():</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spacy_de = spacy.load(<span class="string">&quot;de_core_news_sm&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        os.system(<span class="string">&quot;python -m spacy download de_core_news_sm&quot;</span>)</span><br><span class="line">        spacy_de = spacy.load(<span class="string">&quot;de_core_news_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        spacy_en = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        os.system(<span class="string">&quot;python -m spacy download en_core_web_sm&quot;</span>)</span><br><span class="line">        spacy_en = spacy.load(<span class="string">&quot;en_core_web_sm&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> spacy_de, spacy_en</span><br></pre></td></tr></table></figure><blockquote><p>关于 load_tokenizers</p></blockquote><p>这个函数的作用是加载 spacy语言模型，首先尝试加载德语（de_core_news_sm）和英语（en_core_web_sm）的spacy 语言模型。如果没有安装这两个语言模型，则使用 os.system 函数执行"python -m spacy download de_core_news_sm" 和 "python -m spacy downloaden_core_web_sm" 进行下载。最后返回加载好的两个 spacy 语言模型。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text, tokenizer</span>):</span><br><span class="line">    <span class="keyword">return</span> [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> tokenizer.tokenizer(text)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">yield_tokens</span>(<span class="params">data_iter, tokenizer, index</span>):</span><br><span class="line">    <span class="keyword">for</span> from_to_tuple <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">yield</span> tokenizer(from_to_tuple[index])</span><br></pre></td></tr></table></figure><blockquote><p>关于分词</p></blockquote><p>这两个函数都是进行分词操作的。</p><p>tokenize 函数接收一个文本字符串和一个 tokenizer 对象，并使用tokenizer 对文本进行分词，返回分词后的结果，结果是一个字符串列表。</p><p>yield_tokens 函数接收一个数据迭代器、一个 tokenizer和一个索引值，对每个数据迭代器中的元组进行分词，并使用 yield语句产生分词后的结果，该函数可以生成一个分词的生成器。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_vocabulary</span>(<span class="params">spacy_de, spacy_en</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_de</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_de)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_en</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_en)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Building German Vocabulary ...&quot;</span>)</span><br><span class="line">    train, val, test = datasets.Multi30k(language_pair=(<span class="string">&quot;de&quot;</span>, <span class="string">&quot;en&quot;</span>))</span><br><span class="line">    vocab_src = build_vocab_from_iterator(</span><br><span class="line">        yield_tokens(train + val + test, tokenize_de, index=<span class="number">0</span>),</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        specials=[<span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>, <span class="string">&quot;&lt;blank&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Building English Vocabulary ...&quot;</span>)</span><br><span class="line">    train, val, test = datasets.Multi30k(language_pair=(<span class="string">&quot;de&quot;</span>, <span class="string">&quot;en&quot;</span>))</span><br><span class="line">    vocab_tgt = build_vocab_from_iterator(</span><br><span class="line">        yield_tokens(train + val + test, tokenize_en, index=<span class="number">1</span>),</span><br><span class="line">        min_freq=<span class="number">2</span>,</span><br><span class="line">        specials=[<span class="string">&quot;&lt;s&gt;&quot;</span>, <span class="string">&quot;&lt;/s&gt;&quot;</span>, <span class="string">&quot;&lt;blank&gt;&quot;</span>, <span class="string">&quot;&lt;unk&gt;&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    vocab_src.set_default_index(vocab_src[<span class="string">&quot;&lt;unk&gt;&quot;</span>])</span><br><span class="line">    vocab_tgt.set_default_index(vocab_tgt[<span class="string">&quot;&lt;unk&gt;&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab_src, vocab_tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_vocab</span>(<span class="params">spacy_de, spacy_en</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> exists(<span class="string">&quot;vocab.pt&quot;</span>):</span><br><span class="line">        vocab_src, vocab_tgt = build_vocabulary(spacy_de, spacy_en)</span><br><span class="line">        torch.save((vocab_src, vocab_tgt), <span class="string">&quot;vocab.pt&quot;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        vocab_src, vocab_tgt = torch.load(<span class="string">&quot;vocab.pt&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Finished.\nVocabulary sizes:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab_src))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab_tgt))</span><br><span class="line">    <span class="keyword">return</span> vocab_src, vocab_tgt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_interactive_notebook():</span><br><span class="line">    <span class="comment"># global variables used later in the script</span></span><br><span class="line">    spacy_de, spacy_en = show_example(load_tokenizers)</span><br><span class="line">    vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_de, spacy_en])</span><br></pre></td></tr></table></figure><blockquote><p>关于词汇表建立</p></blockquote><p>build_vocabulary 函数实现了建立英德语言对的词汇表。它使用了两个 Spacy库加载了两个语言的语言模型：德语模型 "de_core_news_sm" 和英语模型"en_core_web_sm"。如果语言模型没有被下载，代码会自动下载。然后，使用tokenize 函数和 yield_tokens 函数从 Multi30k数据集中的训练、验证和测试数据构建词汇表。最后，返回德语词汇表和英语词汇表。</p><p>load_vocab 函数的目的是加载词汇表。它会检查是否存在 "vocab.pt"文件，如果不存在则通过调用 build_vocabulary 函数构建词汇表，并保存为"vocab.pt" 文件；如果存在则从文件中加载词汇表。</p><p>最后，在 Jupyter Notebook 中运行的情况下，会调用 show_example函数加载分词器，并通过调用 load_vocab 函数加载词汇表。</p><blockquote><p>Batching matters a ton for speed. We want to have very evenly dividedbatches, with absolutely minimal padding. To do this we have to hack abit around the default torchtext batching. This code patches theirdefault batching to make sure we search over enough sentences to findtight batches.</p></blockquote><h2 id="iterators">Iterators</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">collate_batch</span>(<span class="params"></span></span><br><span class="line"><span class="params">    batch,</span></span><br><span class="line"><span class="params">    src_pipeline,</span></span><br><span class="line"><span class="params">    tgt_pipeline,</span></span><br><span class="line"><span class="params">    src_vocab,</span></span><br><span class="line"><span class="params">    tgt_vocab,</span></span><br><span class="line"><span class="params">    device,</span></span><br><span class="line"><span class="params">    max_padding=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">    pad_id=<span class="number">2</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    bs_id = torch.tensor([<span class="number">0</span>], device=device)  <span class="comment"># &lt;s&gt; token id</span></span><br><span class="line">    eos_id = torch.tensor([<span class="number">1</span>], device=device)  <span class="comment"># &lt;/s&gt; token id</span></span><br><span class="line">    src_list, tgt_list = [], []</span><br><span class="line">    <span class="keyword">for</span> (_src, _tgt) <span class="keyword">in</span> batch:</span><br><span class="line">        processed_src = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                bs_id,</span><br><span class="line">                torch.tensor(</span><br><span class="line">                    src_vocab(src_pipeline(_src)),</span><br><span class="line">                    dtype=torch.int64,</span><br><span class="line">                    device=device,</span><br><span class="line">                ),</span><br><span class="line">                eos_id,</span><br><span class="line">            ],</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        processed_tgt = torch.cat(</span><br><span class="line">            [</span><br><span class="line">                bs_id,</span><br><span class="line">                torch.tensor(</span><br><span class="line">                    tgt_vocab(tgt_pipeline(_tgt)),</span><br><span class="line">                    dtype=torch.int64,</span><br><span class="line">                    device=device,</span><br><span class="line">                ),</span><br><span class="line">                eos_id,</span><br><span class="line">            ],</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">        )</span><br><span class="line">        src_list.append(</span><br><span class="line">            <span class="comment"># warning - overwrites values for negative values of padding - len</span></span><br><span class="line">            pad(</span><br><span class="line">                processed_src,</span><br><span class="line">                (</span><br><span class="line">                    <span class="number">0</span>,</span><br><span class="line">                    max_padding - <span class="built_in">len</span>(processed_src),</span><br><span class="line">                ),</span><br><span class="line">                value=pad_id,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line">        tgt_list.append(</span><br><span class="line">            pad(</span><br><span class="line">                processed_tgt,</span><br><span class="line">                (<span class="number">0</span>, max_padding - <span class="built_in">len</span>(processed_tgt)),</span><br><span class="line">                value=pad_id,</span><br><span class="line">            )</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    src = torch.stack(src_list)</span><br><span class="line">    tgt = torch.stack(tgt_list)</span><br><span class="line">    <span class="keyword">return</span> (src, tgt)</span><br></pre></td></tr></table></figure><blockquote><p>关于 collate_batch</p></blockquote><p>collate_batch函数的目的是将一个batch（即一个数据集的一部分）中的每一个数据对进行预处理，并返回处理后的结果。</p><p>该函数接收六个参数：</p><ul><li><code>batch</code>：包含数据对的batch；</li><li><code>src_pipeline</code>：预处理源语言的函数；</li><li><code>tgt_pipeline</code>：预处理目标语言的函数；</li><li><code>src_vocab</code>：源语言的词汇表；</li><li><code>tgt_vocab</code>：目标语言的词汇表；</li><li><code>device</code>：指定的设备，如cpu或gpu。</li></ul><p>对于每个数据对，它首先使用<code>src_pipeline</code>对源语言文本进行预处理，然后使用<code>src_vocab</code>将预处理后的源语言文本转换为数字id。同样地，它也会对目标语言文本进行相同的操作。</p><p>然后，它在每个处理后的语言文本开头加上一个&lt;s&gt;标记，在结尾加上一个&lt;eos&gt;标记。</p><p>最后，它会调用pad函数将每个处理后的语言文本补全到指定长度，并返回处理后的结果（两个语言文本组成的元组）。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">create_dataloaders</span>(<span class="params"></span></span><br><span class="line"><span class="params">    device,</span></span><br><span class="line"><span class="params">    vocab_src,</span></span><br><span class="line"><span class="params">    vocab_tgt,</span></span><br><span class="line"><span class="params">    spacy_de,</span></span><br><span class="line"><span class="params">    spacy_en,</span></span><br><span class="line"><span class="params">    batch_size=<span class="number">12000</span>,</span></span><br><span class="line"><span class="params">    max_padding=<span class="number">128</span>,</span></span><br><span class="line"><span class="params">    is_distributed=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="comment"># def create_dataloaders(batch_size=12000):</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_de</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_de)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">tokenize_en</span>(<span class="params">text</span>):</span><br><span class="line">        <span class="keyword">return</span> tokenize(text, spacy_en)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">        <span class="keyword">return</span> collate_batch(</span><br><span class="line">            batch,</span><br><span class="line">            tokenize_de,</span><br><span class="line">            tokenize_en,</span><br><span class="line">            vocab_src,</span><br><span class="line">            vocab_tgt,</span><br><span class="line">            device,</span><br><span class="line">            max_padding=max_padding,</span><br><span class="line">            pad_id=vocab_src.get_stoi()[<span class="string">&quot;&lt;blank&gt;&quot;</span>],</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    train_iter, valid_iter, test_iter = datasets.Multi30k(</span><br><span class="line">        language_pair=(<span class="string">&quot;de&quot;</span>, <span class="string">&quot;en&quot;</span>)</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_iter_map = to_map_style_dataset(</span><br><span class="line">        train_iter</span><br><span class="line">    )  <span class="comment"># DistributedSampler needs a dataset len()</span></span><br><span class="line">    train_sampler = (</span><br><span class="line">        DistributedSampler(train_iter_map) <span class="keyword">if</span> is_distributed <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    )</span><br><span class="line">    valid_iter_map = to_map_style_dataset(valid_iter)</span><br><span class="line">    valid_sampler = (</span><br><span class="line">        DistributedSampler(valid_iter_map) <span class="keyword">if</span> is_distributed <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    train_dataloader = DataLoader(</span><br><span class="line">        train_iter_map,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=(train_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">        sampler=train_sampler,</span><br><span class="line">        collate_fn=collate_fn,</span><br><span class="line">    )</span><br><span class="line">    valid_dataloader = DataLoader(</span><br><span class="line">        valid_iter_map,</span><br><span class="line">        batch_size=batch_size,</span><br><span class="line">        shuffle=(valid_sampler <span class="keyword">is</span> <span class="literal">None</span>),</span><br><span class="line">        sampler=valid_sampler,</span><br><span class="line">        collate_fn=collate_fn,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> train_dataloader, valid_dataloader</span><br></pre></td></tr></table></figure><blockquote><p>关于 create_dataloaders</p></blockquote><p>这是一个创建数据加载器的函数，其目的是为训练和验证数据生成PyTorch数据加载器。</p><p>该函数定义了以下步骤：</p><ol type="1"><li><p>定义了tokenize_de和tokenize_en函数，用于分词。</p></li><li><p>定义了collate_fn函数，用于在生成数据加载器之前整理数据。</p></li><li><p>使用Multi30k类加载数据集，生成训练、验证和测试数据迭代器。</p></li><li><p>将数据迭代器转换为可以处理的数据集，并生成分布式样本器（如果需要）。</p></li><li><p>使用PyTorch的DataLoader类生成训练和验证数据加载器。</p></li></ol><p>最后，该函数返回训练和验证数据加载器。</p><h2 id="training-the-system">Training the System</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_worker</span>(<span class="params"></span></span><br><span class="line"><span class="params">    gpu,</span></span><br><span class="line"><span class="params">    ngpus_per_node,</span></span><br><span class="line"><span class="params">    vocab_src,</span></span><br><span class="line"><span class="params">    vocab_tgt,</span></span><br><span class="line"><span class="params">    spacy_de,</span></span><br><span class="line"><span class="params">    spacy_en,</span></span><br><span class="line"><span class="params">    config,</span></span><br><span class="line"><span class="params">    is_distributed=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Train worker process using GPU: <span class="subst">&#123;gpu&#125;</span> for training&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">    torch.cuda.set_device(gpu)</span><br><span class="line"></span><br><span class="line">    pad_idx = vocab_tgt[<span class="string">&quot;&lt;blank&gt;&quot;</span>]</span><br><span class="line">    d_model = <span class="number">512</span></span><br><span class="line">    model = make_model(<span class="built_in">len</span>(vocab_src), <span class="built_in">len</span>(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.cuda(gpu)</span><br><span class="line">    module = model</span><br><span class="line">    is_main_process = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> is_distributed:</span><br><span class="line">        dist.init_process_group(</span><br><span class="line">            <span class="string">&quot;nccl&quot;</span>, init_method=<span class="string">&quot;env://&quot;</span>, rank=gpu, world_size=ngpus_per_node</span><br><span class="line">        )</span><br><span class="line">        model = DDP(model, device_ids=[gpu])</span><br><span class="line">        module = model.module</span><br><span class="line">        is_main_process = gpu == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    criterion = LabelSmoothing(</span><br><span class="line">        size=<span class="built_in">len</span>(vocab_tgt), padding_idx=pad_idx, smoothing=<span class="number">0.1</span></span><br><span class="line">    )</span><br><span class="line">    criterion.cuda(gpu)</span><br><span class="line"></span><br><span class="line">    train_dataloader, valid_dataloader = create_dataloaders(</span><br><span class="line">        gpu,</span><br><span class="line">        vocab_src,</span><br><span class="line">        vocab_tgt,</span><br><span class="line">        spacy_de,</span><br><span class="line">        spacy_en,</span><br><span class="line">        batch_size=config[<span class="string">&quot;batch_size&quot;</span>] // ngpus_per_node,</span><br><span class="line">        max_padding=config[<span class="string">&quot;max_padding&quot;</span>],</span><br><span class="line">        is_distributed=is_distributed,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    optimizer = torch.optim.Adam(</span><br><span class="line">        model.parameters(), lr=config[<span class="string">&quot;base_lr&quot;</span>], betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">    )</span><br><span class="line">    lr_scheduler = LambdaLR(</span><br><span class="line">        optimizer=optimizer,</span><br><span class="line">        lr_lambda=<span class="keyword">lambda</span> step: rate(</span><br><span class="line">            step, d_model, factor=<span class="number">1</span>, warmup=config[<span class="string">&quot;warmup&quot;</span>]</span><br><span class="line">        ),</span><br><span class="line">    )</span><br><span class="line">    train_state = TrainState()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(config[<span class="string">&quot;num_epochs&quot;</span>]):</span><br><span class="line">        <span class="keyword">if</span> is_distributed:</span><br><span class="line">            train_dataloader.sampler.set_epoch(epoch)</span><br><span class="line">            valid_dataloader.sampler.set_epoch(epoch)</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[GPU<span class="subst">&#123;gpu&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> Training ====&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        _, train_state = run_epoch(</span><br><span class="line">            (Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx) <span class="keyword">for</span> b <span class="keyword">in</span> train_dataloader),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(module.generator, criterion),</span><br><span class="line">            optimizer,</span><br><span class="line">            lr_scheduler,</span><br><span class="line">            mode=<span class="string">&quot;train+log&quot;</span>,</span><br><span class="line">            accum_iter=config[<span class="string">&quot;accum_iter&quot;</span>],</span><br><span class="line">            train_state=train_state,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        GPUtil.showUtilization()</span><br><span class="line">        <span class="keyword">if</span> is_main_process:</span><br><span class="line">            file_path = <span class="string">&quot;%s%.2d.pt&quot;</span> % (config[<span class="string">&quot;file_prefix&quot;</span>], epoch)</span><br><span class="line">            torch.save(module.state_dict(), file_path)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;[GPU<span class="subst">&#123;gpu&#125;</span>] Epoch <span class="subst">&#123;epoch&#125;</span> Validation ====&quot;</span>, flush=<span class="literal">True</span>)</span><br><span class="line">        model.<span class="built_in">eval</span>()</span><br><span class="line">        sloss = run_epoch(</span><br><span class="line">            (Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx) <span class="keyword">for</span> b <span class="keyword">in</span> valid_dataloader),</span><br><span class="line">            model,</span><br><span class="line">            SimpleLossCompute(module.generator, criterion),</span><br><span class="line">            DummyOptimizer(),</span><br><span class="line">            DummyScheduler(),</span><br><span class="line">            mode=<span class="string">&quot;eval&quot;</span>,</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(sloss)</span><br><span class="line">        torch.cuda.empty_cache()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_main_process:</span><br><span class="line">        file_path = <span class="string">&quot;%sfinal.pt&quot;</span> % config[<span class="string">&quot;file_prefix&quot;</span>]</span><br><span class="line">        torch.save(module.state_dict(), file_path)</span><br></pre></td></tr></table></figure><blockquote><p>关于 train_worker</p></blockquote><p>这段代码实现了一个训练工作进程，它是在训练神经机器翻译（NMT）模型时使用的。</p><p>代码流程如下：</p><ol type="1"><li>设置当前 GPU 设备。</li><li>根据给定的词汇，创建一个 NMT 模型，并将其移动到 GPU 设备上。</li><li>如果处于分布式环境，则将模型进行分布式数据并行（DDP）处理。</li><li>创建一个简单的损失计算函数，并移动到 GPU 设备上。</li><li>创建训练数据和验证数据加载器。</li><li>创建一个 Adam 优化器，以及一个学习率调度器。</li><li>循环训练 NMT 模型，共进行指定数量的训练轮数（num_epochs）。</li><li>在每个训练轮结束时，显示 GPU利用率，并在主进程中将当前模型的参数存储到文件。</li><li>在验证集上评估模型的性能。</li><li>在主进程结束时，将最终的模型参数存储到文件。</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_distributed_model</span>(<span class="params">vocab_src, vocab_tgt, spacy_de, spacy_en, config</span>):</span><br><span class="line">    <span class="keyword">from</span> the_annotated_transformer <span class="keyword">import</span> train_worker</span><br><span class="line"></span><br><span class="line">    ngpus = torch.cuda.device_count()</span><br><span class="line">    os.environ[<span class="string">&quot;MASTER_ADDR&quot;</span>] = <span class="string">&quot;localhost&quot;</span></span><br><span class="line">    os.environ[<span class="string">&quot;MASTER_PORT&quot;</span>] = <span class="string">&quot;12356&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Number of GPUs detected: <span class="subst">&#123;ngpus&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Spawning training processes ...&quot;</span>)</span><br><span class="line">    mp.spawn(</span><br><span class="line">        train_worker,</span><br><span class="line">        nprocs=ngpus,</span><br><span class="line">        args=(ngpus, vocab_src, vocab_tgt, spacy_de, spacy_en, config, <span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">vocab_src, vocab_tgt, spacy_de, spacy_en, config</span>):</span><br><span class="line">    <span class="keyword">if</span> config[<span class="string">&quot;distributed&quot;</span>]:</span><br><span class="line">        train_distributed_model(</span><br><span class="line">            vocab_src, vocab_tgt, spacy_de, spacy_en, config</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        train_worker(</span><br><span class="line">            <span class="number">0</span>, <span class="number">1</span>, vocab_src, vocab_tgt, spacy_de, spacy_en, config, <span class="literal">False</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_trained_model</span>():</span><br><span class="line">    config = &#123;</span><br><span class="line">        <span class="string">&quot;batch_size&quot;</span>: <span class="number">32</span>,</span><br><span class="line">        <span class="string">&quot;distributed&quot;</span>: <span class="literal">False</span>,</span><br><span class="line">        <span class="string">&quot;num_epochs&quot;</span>: <span class="number">8</span>,</span><br><span class="line">        <span class="string">&quot;accum_iter&quot;</span>: <span class="number">10</span>,</span><br><span class="line">        <span class="string">&quot;base_lr&quot;</span>: <span class="number">1.0</span>,</span><br><span class="line">        <span class="string">&quot;max_padding&quot;</span>: <span class="number">72</span>,</span><br><span class="line">        <span class="string">&quot;warmup&quot;</span>: <span class="number">3000</span>,</span><br><span class="line">        <span class="string">&quot;file_prefix&quot;</span>: <span class="string">&quot;multi30k_model_&quot;</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    model_path = <span class="string">&quot;multi30k_model_final.pt&quot;</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> exists(model_path):</span><br><span class="line">        train_model(vocab_src, vocab_tgt, spacy_de, spacy_en, config)</span><br><span class="line"></span><br><span class="line">    model = make_model(<span class="built_in">len</span>(vocab_src), <span class="built_in">len</span>(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.load_state_dict(torch.load(<span class="string">&quot;multi30k_model_final.pt&quot;</span>))</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> is_interactive_notebook():</span><br><span class="line">    model = load_trained_model()</span><br></pre></td></tr></table></figure><blockquote><p>关于 Real World Example 的训练</p></blockquote><p>train_distributed_model是一个以分布式方式在多个GPU上训练深度学习模型的函数。这个函数使用PyTorch的torch.nn.DataParallel类来包装模型，并在GPU上分割数据。</p><p>train_model是一个训练深度学习模型的函数。它检查是否应该以分布式方式进行训练（通过检查config["distributed"]的值），并调用train_distributed_model或train_worker。</p><p>load_trained_model是一个从磁盘加载预训练的深度学习模型的函数。它首先检查模型是否存在，如果不存在，则使用train_model 函数训练模型。然后使用torch.load加载模型。</p><p>ifis_interactive_notebook()语句检查代码是否在交互式Jupyter笔记本中运行。如果是，它就通过调用load_trained_model来创建模型的实例。</p><blockquote><p>Once trained we can decode the model to produce a set oftranslations. Here we simply translate the first sentence in thevalidation set. This dataset is pretty small so the translations withgreedy search are reasonably accurate.</p></blockquote><blockquote><p>一旦训练完成，我们就可以解码模型以生成一组翻译。这里我们简单翻译验证集中的第一句话。这个数据集非常小，所以贪婪搜索的翻译相当准确。</p></blockquote><h1 id="附加组件-bpe-search-averaging">附加组件: BPE, Search,Averaging</h1><blockquote><p>So this mostly covers the transformer model itself. There are fouraspects that we didn't cover explicitly. We also have all theseadditional features implemented in <ahref="https://github.com/opennmt/opennmt-py">OpenNMT-py</a>.</p></blockquote><blockquote><p>以上是Transformer的模型构建，以下是四个原文没涉及到的细节实现部分：</p></blockquote><blockquote><p>(1) BPE/ Word-piece: We can use a library to first preprocess thedata into subword units. See Rico Sennrich's <ahref="https://github.com/rsennrich/subword-nmt">subword-nmt</a>implementation. These models will transform the training data to looklike this:</p></blockquote><p>▁Die ▁Protokoll datei ▁kann ▁ heimlich ▁per ▁E - Mail ▁oder ▁FTP ▁an▁einen ▁bestimmte n ▁Empfänger ▁gesendet ▁werden .</p><blockquote><p>关于 (1) BPE 数据预处理</p></blockquote><p><code>subword-nmt</code> : sentence <code>-&gt;</code> subword</p><blockquote><p>(2) Shared Embeddings: When using BPE with shared vocabulary we canshare the same weight vectors between the source / target / generator.See the <a href="https://arxiv.org/abs/1608.05859">(cite)</a> fordetails. To add this to the model simply do this:</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="literal">False</span>:</span><br><span class="line">    model.src_embed[<span class="number">0</span>].lut.weight = model.tgt_embeddings[<span class="number">0</span>].lut.weight</span><br><span class="line">    model.generator.lut.weight = model.tgt_embed[<span class="number">0</span>].lut.weight</span><br></pre></td></tr></table></figure><blockquote><p>关于 (2) 共享 Embedding 权重的实现</p></blockquote><p>这段话和代码指的是在使用BPE（Byte-PairEncoding）分词方法，并且使用了共享词汇表时，源语言、目标语言和生成器可以共享同一组权重矩阵。参考论文"Usingthe Output Embedding to Improve LanguageModels"(https://arxiv.org/abs/1608.05859) 。</p><p>如果想在模型中添加这一功能，可以执行代码中的操作：将源语言的词嵌入矩阵、目标语言的词嵌入矩阵和生成器的词嵌入矩阵共享赋值为目标语言的词嵌入矩阵。</p><blockquote><p>(3) Beam Search: This is a bit too complicated to cover here. See the<a href="https://github.com/OpenNMT/OpenNMT-py/">OpenNMT-py</a> for apytorch implementation.</p></blockquote><blockquote><p>关于 (3) 集束搜索</p></blockquote><p>在The AnnotatedTransformer的背景下，波束搜索被用作解码算法，以生成机器翻译任务中的目标序列。</p><p>集束搜索是一种用于序列生成任务（例如机器翻译、图像字幕和语音识别）的搜索算法，用于在给定一组输入数据的情况下找到最佳输出集（即得分最高的单词序列）。它是一种启发式搜索算法，用于通过在每个时间步维护一个波束或一组K 个候选输出来生成高质量输出，其中 K 是波束宽度。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义计算得分的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_score</span>(<span class="params">sequence, weights</span>):</span><br><span class="line">    score = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sequence)):</span><br><span class="line">        score += sequence[i] * weights[i]</span><br><span class="line">    <span class="keyword">return</span> score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义搜索函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">beam_search</span>(<span class="params">weights, k</span>):</span><br><span class="line">    <span class="comment"># 存储当前最优的k个序列</span></span><br><span class="line">    best_sequences = [[]]</span><br><span class="line">    <span class="comment"># 存储当前最优的k个得分</span></span><br><span class="line">    best_scores = [<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 定义循环次数，以此来限制生成的序列长度</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(weights)):</span><br><span class="line">        all_candidates = []</span><br><span class="line">        <span class="comment"># 遍历当前最优的k个序列</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(best_sequences)):</span><br><span class="line">            sequence = best_sequences[i]</span><br><span class="line">            score = best_scores[i]</span><br><span class="line">            <span class="comment"># 生成0或1的候选序列</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> [<span class="number">0</span>, <span class="number">1</span>]:</span><br><span class="line">                candidate = sequence + [j]</span><br><span class="line">                candidate_score = score + j * weights[t]</span><br><span class="line">                all_candidates.append([candidate, candidate_score])</span><br><span class="line">        <span class="comment"># 根据得分排序</span></span><br><span class="line">        ordered = <span class="built_in">sorted</span>(all_candidates, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], reverse=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 选择得分最高的k个候选序列</span></span><br><span class="line">        best_sequences = [x[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> ordered[:k]]</span><br><span class="line">        best_scores = [x[<span class="number">1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> ordered[:k]]</span><br><span class="line">    <span class="keyword">return</span> best_sequences[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试beam search算法</span></span><br><span class="line">weights = [<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.1</span>, <span class="number">0.1</span>]</span><br><span class="line">result = beam_search(weights, <span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最优序列：&quot;</span>, result)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;最优得分：&quot;</span>, calculate_score(result, weights))</span><br></pre></td></tr></table></figure><p>calculate_score函数是根据输入的序列和一些预先定义好的权重，来计算该序列的得分。beam_search函数是执行beam search算法的主体，该函数接受两个参数：k 和 max_steps。 k是 beam宽，表示在每个时刻保留的候选序列数； max_steps是算法最大执行步数。</p><p>在这个例子中，算法的停止条件是搜索的步数到达了max_steps，或者只剩下一个候选序列，此时算法终止。</p><p>该函数的主要逻辑是：初始化候选序列的列表，每个候选序列的初始状态是一个长度为1的0。在每一步，扩展候选序列，并对扩展后的序列计算得分。根据得分对所有候选序列排序，保留得分前k 名的候选序列。如果算法达到了最大执行步数或者只剩下了一个候选序列，则终止。</p><p>最终，该算法返回得分最高的序列，即最终的答案。</p><blockquote><p>(4) Model Averaging: The paper averages the last k checkpoints tocreate an ensembling effect. We can do this after the fact if we have abunch of models:</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">average</span>(<span class="params">model, models</span>):</span><br><span class="line">    <span class="string">&quot;Average models into model&quot;</span></span><br><span class="line">    <span class="keyword">for</span> ps <span class="keyword">in</span> <span class="built_in">zip</span>(*[m.params() <span class="keyword">for</span> m <span class="keyword">in</span> [model] + models]):</span><br><span class="line">        ps[<span class="number">0</span>].copy_(torch.<span class="built_in">sum</span>(*ps[<span class="number">1</span>:]) / <span class="built_in">len</span>(ps[<span class="number">1</span>:]))</span><br></pre></td></tr></table></figure><blockquote><p>关于 （4）模型平均</p></blockquote><p>这段代码实现了一个名为 average 的函数，用于平均多个模型的参数。</p><p>这个函数接收两个参数：model 和 models。model 是一个模型，models是一个模型的列表。该函数将所有模型的参数进行求和，再除以模型数量，最后将结果复制到model 中。</p><p>这段代码可能用于在The Annotated Transformer by Harvard NLP中训练多个模型，然后将它们的参数进行平均以得到一个最终的模型。这种方法常用于集成学习，可以提高模型的稳定性和准确率。</p><h1 id="结果">结果</h1><p>On the WMT 2014 English-to-German translation task, the bigtransformer model (Transformer (big) in Table 2) outperforms the bestpreviously reported models (including ensembles) by more than 2.0 BLEU,establishing a new state-of-the-art BLEU score of 28.4. Theconfiguration of this model is listed in the bottom line of Table 3.Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses allpreviously published models and ensembles, at a fraction of the trainingcost of any of the competitive models.</p><p>On the WMT 2014 English-to-French translation task, our big modelachieves a BLEU score of 41.0, outperforming all of the previouslypublished single models, at less than 1/4 the training cost of theprevious state-of-the-art model. The Transformer (big) model trained forEnglish-to-French used dropout rate Pdrop = 0.1, instead of 0.3.</p><blockquote><p>With the addtional extensions in the last section, the OpenNMT-pyreplication gets to 26.9 on EN-DE WMT. Here I have loaded in thoseparameters to our reimplemenation.</p></blockquote><blockquote><p>通过上一节中的附加扩展，OpenNMT-py 复制在 EN-DE WMT 上达到26.9。在这里，我已将这些参数加载到我们的重新实现中。</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Load data and model for output checks</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">check_outputs</span>(<span class="params"></span></span><br><span class="line"><span class="params">    valid_dataloader,</span></span><br><span class="line"><span class="params">    model,</span></span><br><span class="line"><span class="params">    vocab_src,</span></span><br><span class="line"><span class="params">    vocab_tgt,</span></span><br><span class="line"><span class="params">    n_examples=<span class="number">15</span>,</span></span><br><span class="line"><span class="params">    pad_idx=<span class="number">2</span>,</span></span><br><span class="line"><span class="params">    eos_string=<span class="string">&quot;&lt;/s&gt;&quot;</span>,</span></span><br><span class="line"><span class="params"></span>):</span><br><span class="line">    results = [()] * n_examples</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(n_examples):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\nExample %d ========\n&quot;</span> % idx)</span><br><span class="line">        b = <span class="built_in">next</span>(<span class="built_in">iter</span>(valid_dataloader))</span><br><span class="line">        rb = Batch(b[<span class="number">0</span>], b[<span class="number">1</span>], pad_idx)</span><br><span class="line">        greedy_decode(model, rb.src, rb.src_mask, <span class="number">64</span>, <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        src_tokens = [</span><br><span class="line">            vocab_src.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> rb.src[<span class="number">0</span>] <span class="keyword">if</span> x != pad_idx</span><br><span class="line">        ]</span><br><span class="line">        tgt_tokens = [</span><br><span class="line">            vocab_tgt.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> rb.tgt[<span class="number">0</span>] <span class="keyword">if</span> x != pad_idx</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Source Text (Input)        : &quot;</span></span><br><span class="line">            + <span class="string">&quot; &quot;</span>.join(src_tokens).replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(</span><br><span class="line">            <span class="string">&quot;Target Text (Ground Truth) : &quot;</span></span><br><span class="line">            + <span class="string">&quot; &quot;</span>.join(tgt_tokens).replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>)</span><br><span class="line">        )</span><br><span class="line">        model_out = greedy_decode(model, rb.src, rb.src_mask, <span class="number">72</span>, <span class="number">0</span>)[<span class="number">0</span>]</span><br><span class="line">        model_txt = (</span><br><span class="line">            <span class="string">&quot; &quot;</span>.join(</span><br><span class="line">                [vocab_tgt.get_itos()[x] <span class="keyword">for</span> x <span class="keyword">in</span> model_out <span class="keyword">if</span> x != pad_idx]</span><br><span class="line">            ).split(eos_string, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            + eos_string</span><br><span class="line">        )</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Model Output               : &quot;</span> + model_txt.replace(<span class="string">&quot;\n&quot;</span>, <span class="string">&quot;&quot;</span>))</span><br><span class="line">        results[idx] = (rb, src_tokens, tgt_tokens, model_out, model_txt)</span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_model_example</span>(<span class="params">n_examples=<span class="number">5</span></span>):</span><br><span class="line">    <span class="keyword">global</span> vocab_src, vocab_tgt, spacy_de, spacy_en</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Preparing Data ...&quot;</span>)</span><br><span class="line">    _, valid_dataloader = create_dataloaders(</span><br><span class="line">        torch.device(<span class="string">&quot;cpu&quot;</span>),</span><br><span class="line">        vocab_src,</span><br><span class="line">        vocab_tgt,</span><br><span class="line">        spacy_de,</span><br><span class="line">        spacy_en,</span><br><span class="line">        batch_size=<span class="number">1</span>,</span><br><span class="line">        is_distributed=<span class="literal">False</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Loading Trained Model ...&quot;</span>)</span><br><span class="line"></span><br><span class="line">    model = make_model(<span class="built_in">len</span>(vocab_src), <span class="built_in">len</span>(vocab_tgt), N=<span class="number">6</span>)</span><br><span class="line">    model.load_state_dict(</span><br><span class="line">        torch.load(<span class="string">&quot;multi30k_model_final.pt&quot;</span>, map_location=torch.device(<span class="string">&quot;cpu&quot;</span>))</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Checking Model Outputs:&quot;</span>)</span><br><span class="line">    example_data = check_outputs(</span><br><span class="line">        valid_dataloader, model, vocab_src, vocab_tgt, n_examples=n_examples</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> model, example_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># execute_example(run_model_example)</span></span><br></pre></td></tr></table></figure><h2 id="attention-visualization">Attention Visualization</h2><blockquote><p>Even with a greedy decoder the translation looks pretty good. We canfurther visualize it to see what is happening at each layer of theattention</p></blockquote><blockquote><p>即使使用贪心解码器，翻译看起来也很不错。我们可以进一步可视化它，看看每一层注意力都发生了什么</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mtx2df</span>(<span class="params">m, max_row, max_col, row_tokens, col_tokens</span>):</span><br><span class="line">    <span class="string">&quot;convert a dense matrix to a data frame with row and column indices&quot;</span></span><br><span class="line">    <span class="keyword">return</span> pd.DataFrame(</span><br><span class="line">        [</span><br><span class="line">            (</span><br><span class="line">                r,</span><br><span class="line">                c,</span><br><span class="line">                <span class="built_in">float</span>(m[r, c]),</span><br><span class="line">                <span class="string">&quot;%.3d %s&quot;</span></span><br><span class="line">                % (r, row_tokens[r] <span class="keyword">if</span> <span class="built_in">len</span>(row_tokens) &gt; r <span class="keyword">else</span> <span class="string">&quot;&lt;blank&gt;&quot;</span>),</span><br><span class="line">                <span class="string">&quot;%.3d %s&quot;</span></span><br><span class="line">                % (c, col_tokens[c] <span class="keyword">if</span> <span class="built_in">len</span>(col_tokens) &gt; c <span class="keyword">else</span> <span class="string">&quot;&lt;blank&gt;&quot;</span>),</span><br><span class="line">            )</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(m.shape[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">if</span> r &lt; max_row <span class="keyword">and</span> c &lt; max_col</span><br><span class="line">        ],</span><br><span class="line">        <span class="comment"># if float(m[r,c]) != 0 and r &lt; max_row and c &lt; max_col],</span></span><br><span class="line">        columns=[<span class="string">&quot;row&quot;</span>, <span class="string">&quot;column&quot;</span>, <span class="string">&quot;value&quot;</span>, <span class="string">&quot;row_token&quot;</span>, <span class="string">&quot;col_token&quot;</span>],</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attn_map</span>(<span class="params">attn, layer, head, row_tokens, col_tokens, max_dim=<span class="number">30</span></span>):</span><br><span class="line">    df = mtx2df(</span><br><span class="line">        attn[<span class="number">0</span>, head].data,</span><br><span class="line">        max_dim,</span><br><span class="line">        max_dim,</span><br><span class="line">        row_tokens,</span><br><span class="line">        col_tokens,</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> (</span><br><span class="line">        alt.Chart(data=df)</span><br><span class="line">        .mark_rect()</span><br><span class="line">        .encode(</span><br><span class="line">            x=alt.X(<span class="string">&quot;col_token&quot;</span>, axis=alt.Axis(title=<span class="string">&quot;&quot;</span>)),</span><br><span class="line">            y=alt.Y(<span class="string">&quot;row_token&quot;</span>, axis=alt.Axis(title=<span class="string">&quot;&quot;</span>)),</span><br><span class="line">            color=<span class="string">&quot;value&quot;</span>,</span><br><span class="line">            tooltip=[<span class="string">&quot;row&quot;</span>, <span class="string">&quot;column&quot;</span>, <span class="string">&quot;value&quot;</span>, <span class="string">&quot;row_token&quot;</span>, <span class="string">&quot;col_token&quot;</span>],</span><br><span class="line">        )</span><br><span class="line">        .properties(height=<span class="number">400</span>, width=<span class="number">400</span>)</span><br><span class="line">        .interactive()</span><br><span class="line">    )</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_encoder</span>(<span class="params">model, layer</span>):</span><br><span class="line">    <span class="keyword">return</span> model.encoder.layers[layer].self_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_decoder_self</span>(<span class="params">model, layer</span>):</span><br><span class="line">    <span class="keyword">return</span> model.decoder.layers[layer].self_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_decoder_src</span>(<span class="params">model, layer</span>):</span><br><span class="line">    <span class="keyword">return</span> model.decoder.layers[layer].src_attn.attn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_layer</span>(<span class="params">model, layer, getter_fn, ntokens, row_tokens, col_tokens</span>):</span><br><span class="line">    <span class="comment"># ntokens = last_example[0].ntokens</span></span><br><span class="line">    attn = getter_fn(model, layer)</span><br><span class="line">    n_heads = attn.shape[<span class="number">1</span>]</span><br><span class="line">    charts = [</span><br><span class="line">        attn_map(</span><br><span class="line">            attn,</span><br><span class="line">            <span class="number">0</span>,</span><br><span class="line">            h,</span><br><span class="line">            row_tokens=row_tokens,</span><br><span class="line">            col_tokens=col_tokens,</span><br><span class="line">            max_dim=ntokens,</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> h <span class="keyword">in</span> <span class="built_in">range</span>(n_heads)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">assert</span> n_heads == <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> alt.vconcat(</span><br><span class="line">        charts[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># | charts[1]</span></span><br><span class="line">        | charts[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># | charts[3]</span></span><br><span class="line">        | charts[<span class="number">4</span>]</span><br><span class="line">        <span class="comment"># | charts[5]</span></span><br><span class="line">        | charts[<span class="number">6</span>]</span><br><span class="line">        <span class="comment"># | charts[7]</span></span><br><span class="line">        <span class="comment"># layer + 1 due to 0-indexing</span></span><br><span class="line">    ).properties(title=<span class="string">&quot;Layer %d&quot;</span> % (layer + <span class="number">1</span>))</span><br></pre></td></tr></table></figure><h2 id="encoder-self-attention">Encoder Self Attention</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">viz_encoder_self</span>():</span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[</span><br><span class="line">        <span class="built_in">len</span>(example_data) - <span class="number">1</span></span><br><span class="line">    ]  <span class="comment"># batch object for the final example</span></span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model, layer, get_encoder, <span class="built_in">len</span>(example[<span class="number">1</span>]), example[<span class="number">1</span>], example[<span class="number">1</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        <span class="comment"># &amp; layer_viz[1]</span></span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        <span class="comment"># &amp; layer_viz[3]</span></span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        <span class="comment"># &amp; layer_viz[5]</span></span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_encoder_self)</span><br></pre></td></tr></table></figure><blockquote><p>关于 Encoder Self Attention</p></blockquote><p>生成的可视化显示了Transformer模型中编码器部分的每个层的激活情况。可视化的层是通过visualize_layer函数的listlayer_viz指定的。这个函数生成了一个激活的热图，颜色越亮，激活值越高。</p><p>在这个具体的实现中，可视化只限于输入数据的最后一个例子（example_data）。viz_encoder_self函数返回0、2、4层的激活值的连接。最后的结果用show_example函数显示。</p><p>注意：可视化可以让我们了解编码器是如何处理输入序列的，以及激活是如何在Transformer的每一层上变化的。</p><h2 id="decoder-self-attention">Decoder Self Attention</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">viz_decoder_self</span>():</span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[<span class="built_in">len</span>(example_data) - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model,</span><br><span class="line">            layer,</span><br><span class="line">            get_decoder_self,</span><br><span class="line">            <span class="built_in">len</span>(example[<span class="number">1</span>]),</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">1</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">3</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">5</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_decoder_self)</span><br></pre></td></tr></table></figure><h2 id="decoder-src-attention">Decoder Src Attention</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">viz_decoder_src</span>():</span><br><span class="line">    model, example_data = run_model_example(n_examples=<span class="number">1</span>)</span><br><span class="line">    example = example_data[<span class="built_in">len</span>(example_data) - <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    layer_viz = [</span><br><span class="line">        visualize_layer(</span><br><span class="line">            model,</span><br><span class="line">            layer,</span><br><span class="line">            get_decoder_src,</span><br><span class="line">            <span class="built_in">max</span>(<span class="built_in">len</span>(example[<span class="number">1</span>]), <span class="built_in">len</span>(example[<span class="number">2</span>])),</span><br><span class="line">            example[<span class="number">1</span>],</span><br><span class="line">            example[<span class="number">2</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>)</span><br><span class="line">    ]</span><br><span class="line">    <span class="keyword">return</span> alt.hconcat(</span><br><span class="line">        layer_viz[<span class="number">0</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">1</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">2</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">3</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">4</span>]</span><br><span class="line">        &amp; layer_viz[<span class="number">5</span>]</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">show_example(viz_decoder_src)</span><br></pre></td></tr></table></figure><h1 id="参考链接">参考链接</h1><h2 id="完整代码实现">完整代码实现</h2><ul><li><p>哈佛大学NLP组的notebook，很详细文字和代码描述，用pytorch实现</p><p><ahref="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p></li></ul><h2 id="注释说明参考网页">注释说明参考网页</h2><ol type="1"><li><p>哈佛大学NLP组的colab notebook：The Annotated "Attention is AllYou Need".ipynb</p><p><ahref="https://colab.research.google.com/drive/1xQXSv6mtAOLXxEMi8RvaW8TW-7bvYBDF">https://colab.research.google.com/drive/1xQXSv6mtAOLXxEMi8RvaW8TW-7bvYBDF</a></p></li><li><p>The Annotated Transformer的中文注释版:</p><p><ahref="https://zhuanlan.zhihu.com/p/107889011">https://zhuanlan.zhihu.com/p/107889011</a></p></li><li><p>论文阅读笔记（结合李沐视频）--- Attention is all youneed（Transformer）逐段精读</p><p><ahref="https://blog.csdn.net/qq_39594939/article/details/122823436">https://blog.csdn.net/qq_39594939/article/details/122823436</a></p></li><li><p>庖丁解牛式读《Attention is all your need》</p><p><ahref="https://blog.csdn.net/liu16659/article/details/108141534">https://blog.csdn.net/liu16659/article/details/108141534</a></p></li><li><p>Transformer代码阅读</p><p><ahref="http://fancyerii.github.io/2019/03/09/transformer-codes/">http://fancyerii.github.io/2019/03/09/transformer-codes/</a></p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;导航&quot;&gt;导航&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;这篇文章是 Transformer
完整版复现代码的深度解析。&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客的 Transformer 系列文章共计四篇，导航如下：&lt;/p&gt;
&lt;/block</summary>
      
    
    
    
    <category term="笔记, 教程" scheme="https://le-ying.github.io/categories/%E7%AC%94%E8%AE%B0-%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="科研" scheme="https://le-ying.github.io/tags/%E7%A7%91%E7%A0%94/"/>
    
    <category term="transformer" scheme="https://le-ying.github.io/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）</title>
    <link href="https://le-ying.github.io/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/"/>
    <id>https://le-ying.github.io/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/</id>
    <published>2023-01-31T09:00:10.000Z</published>
    <updated>2023-02-14T11:53:27.563Z</updated>
    
    <content type="html"><![CDATA[<h1 id="导航">导航</h1><p><strong>这篇文章是基于 Pytorch 的 Transformer简易版复现教程。</strong></p><blockquote><p>本博客的 Transformer 系列文章共计四篇，导航如下：</p></blockquote><ul><li><p><ahref="https://serika-onoe.github.io/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/">Transformer零基础解析教程，从Encoder-Decoder架构说起（1/4）</a></p></li><li><p><ahref="https://serika-onoe.github.io/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/">Transformer零基础解析教程，剥洋葱般层层剖析内在原理（2/4）</a></p></li><li><p><ahref="https://serika-onoe.github.io/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/">Transformer零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）<strong>本篇</strong></a></p></li><li><p><ahref="https://serika-onoe.github.io/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/">Transformer零基础解析教程，完整版代码最终挑战（4/4）</a></p></li></ul><h1 id="前言">前言</h1><p><strong>该教程代码初始来源于JeffJung，我阅读了一些博客和视频后做了大量的注释和修改，更加方便阅读和复现。</strong></p><p>代码中为了加快可读性和运行速度，并没有用到大型的数据集，而是手动输入了两对中文→英语的句子，还有每个字的索引也是手动硬编码上去的，主要是为了降低代码执行速度和阅读难度，哪怕用普通的笔记本CPU也能在1分钟以内完成，从而方便读者<strong>把重点放到模型实现的部分！</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ======================================</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612, modify by shwei</span></span><br><span class="line"><span class="string">Reference: https://github.com/jadore801120/attention-is-all-you-need-pytorch</span></span><br><span class="line"><span class="string">           https://github.com/JayParks/transformer</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ====================================================================================================</span></span><br></pre></td></tr></table></figure><h1 id="数据预处理">数据预处理</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">device = <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="comment"># device = &#x27;cuda&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># transformer epochs</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"><span class="comment"># epochs = 1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里我没有用什么大型的数据集，而是手动输入了两对中文→英语的句子</span></span><br><span class="line"><span class="comment"># 还有每个字的索引也是我手动硬编码上去的，主要是为了降低代码阅读难度</span></span><br><span class="line"><span class="comment"># S: Symbol that shows starting of decoding input</span></span><br><span class="line"><span class="comment"># E: Symbol that shows starting of decoding output</span></span><br><span class="line"><span class="comment"># P: Symbol that will fill in blank sequence if current batch data size is shorter than time steps</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="comment"># 中文和英语的单词个数不要求相同</span></span><br><span class="line">    <span class="comment"># enc_input                dec_input           dec_output</span></span><br><span class="line">    [<span class="string">&#x27;我 有 一 个 女 朋 友&#x27;</span>, <span class="string">&#x27;S i have a girl friend . &#x27;</span>, <span class="string">&#x27;i have a girl friend . E&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;我 有 零 个 好 朋 友&#x27;</span>, <span class="string">&#x27;S i have zero good friend .&#x27;</span>, <span class="string">&#x27;i have zero good friend . E&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中文和英语的单词要分开建立词库</span></span><br><span class="line"><span class="comment"># Padding Should be Zero</span></span><br><span class="line">src_vocab = &#123;</span><br><span class="line">            <span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;我&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;有&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;一&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;个&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;好&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;朋&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;友&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;零&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;女&#x27;</span>: <span class="number">9</span>&#125;</span><br><span class="line">src_idx2word = &#123;</span><br><span class="line">            i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(src_vocab)&#125;</span><br><span class="line">src_vocab_size = <span class="built_in">len</span>(src_vocab)</span><br><span class="line"></span><br><span class="line">tgt_vocab = &#123;</span><br><span class="line">            <span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;have&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;good&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;friend&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;zero&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;girl&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;S&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">9</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line">idx2word = &#123;</span><br><span class="line">            i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(tgt_vocab)&#125;</span><br><span class="line">tgt_vocab_size = <span class="built_in">len</span>(tgt_vocab)</span><br><span class="line"></span><br><span class="line">src_len = <span class="number">8</span>  <span class="comment"># （源句子的长度）enc_input max sequence length</span></span><br><span class="line">tgt_len = <span class="number">7</span>  <span class="comment"># dec_input(=dec_output) max sequence length</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Transformer Parameters</span></span><br><span class="line">d_model = <span class="number">512</span>  <span class="comment"># Embedding Size（token embedding和position编码的维度）</span></span><br><span class="line">d_ff = <span class="number">2048</span>  <span class="comment"># FeedForward dimension (两次线性层中的隐藏层 512-&gt;2048-&gt;512，线性层是用来做特征提取的），当然最后会再接一个projection层</span></span><br><span class="line">d_k = d_v = <span class="number">64</span>  <span class="comment"># dimension of K(=Q), V（Q和K的维度需要相同，这里为了方便让K=V）</span></span><br><span class="line">n_layers = <span class="number">6</span>  <span class="comment"># number of Encoder of Decoder Layer（Block的个数）</span></span><br><span class="line">n_heads = <span class="number">8</span>  <span class="comment"># number of heads in Multi-Head Attention（有几套头）</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==============================================================================================</span></span><br><span class="line"><span class="comment"># 数据构建</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentences</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;把单词序列转换为数字序列&quot;&quot;&quot;</span></span><br><span class="line">    enc_inputs, dec_inputs, dec_outputs = [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences)):</span><br><span class="line">        enc_input = [[src_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">0</span>].split()]]  <span class="comment"># [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]</span></span><br><span class="line">        dec_input = [[tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">1</span>].split()]]  <span class="comment"># [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]</span></span><br><span class="line">        dec_output = [[tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">2</span>].split()]]  <span class="comment"># [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]</span></span><br><span class="line"></span><br><span class="line">        enc_inputs.extend(enc_input)</span><br><span class="line">        dec_inputs.extend(dec_input)</span><br><span class="line">        dec_outputs.extend(dec_output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataSet</span>(Data.Dataset):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义DataLoader&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_inputs, dec_inputs, dec_outputs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDataSet, self).__init__()</span><br><span class="line">        self.enc_inputs = enc_inputs</span><br><span class="line">        self.dec_inputs = dec_inputs</span><br><span class="line">        self.dec_outputs = dec_outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.enc_inputs.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 这行代码创建了一个PyTorch数据加载器，用于在训练机器学习模型时加载数据。DataLoader是PyTorch的核心库torch.utils.data中的函数。它的作用是将</span></span><br><span class="line"><span class="comment"># 数据集（此处为MyDataSet类的实例，传入的enc_inputs、dec_inputs和dec_outputs作为数据）拆分成小批次以提高加载效率。</span></span><br><span class="line"><span class="comment"># 具体参数说明：</span></span><br><span class="line"><span class="comment"># batch_size: 2，每批加载的样本数。</span></span><br><span class="line"><span class="comment"># shuffle: True，是否打乱数据顺序。</span></span><br></pre></td></tr></table></figure><p>上面都比较简单，下面开始涉及到模型就比较复杂了，因此我会将模型拆分成以下几个部分进行讲解</p><ul><li>Positional Encoding</li><li>PadMask（序列本身固定长度，不够长的序列需要填充（pad），也就是'P'）</li><li>Subsequence Mask（Decoder input 不能看到未来时刻单词信息，因此需要mask）</li><li>ScaledDotProductAttention</li><li>Multi-Head Attention</li><li>FeedForward Layer</li><li>Encoder Layer</li><li>Encoder</li><li>Decoder Layer</li><li>Decoder</li><li>Transformer</li></ul><p>关于代码中的注释，如果值为 <code>src_len</code> 或者<code>tgt_len</code> 的，我一定会写清楚，但是有些函数或者类，Encoder 和Decoder 都有可能调用，因此就不能确定究竟是 <code>src_len</code> 还是<code>tgt_len</code>，对于不确定的，我会记作 <code>seq_len</code></p><h1 id="模型构建">模型构建</h1><h2 id="positional-encoding">Positional Encoding</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在dropout函数中，参数p是指丢弃元素的概率。它决定了输入张量的元素在丢弃操作中被设置为零的比率。</span></span><br><span class="line"><span class="string">        例如，如果p=0.1，那么10%的元素将在丢弃操作中被设置为零。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        通过调用self.register_buffer(&#x27;pe&#x27;, pe)，我们将这个张量注册为模型的一个可学习参数，</span></span><br><span class="line"><span class="string">        这意味着这个张量在模型训练过程中不需要更新其梯度，即不需要在损失函数的计算中考虑这个张量的梯度。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: [seq_len, batch_size, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><h2 id="pad-mask">Pad Mask</h2><p>由于在 Encoder 和 Decoder 中都需要进行 mask操作，因此就无法确定这个函数的参数中 <code>seq_len</code> 的值，如果是在Encoder 中调用的，<code>seq_len</code> 就等于<code>src_len</code>；如果是在 Decoder 中调用的，<code>seq_len</code>就有可能等于 <code>src_len</code>，也有可能等于<code>tgt_len</code>（因为 Decoder 有两次 mask）</p><p><strong>padmask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量。</strong></p><p>这个函数最核心的一句代码是<code>seq_k.data.eq(0)</code>，这句的作用是返回一个大小和<code>seq_k</code> 一样的 tensor，只不过里面的值只有 True 和 False。如果<code>seq_k</code> 某个位置的值等于 0，那么对应位置就是 True，否则即为False。举个例子，输入为<code>seq_data = [1, 2, 3, 4, 0]</code>，<code>seq_data.data.eq(0)</code>就会返回 <code>[False, False, False, False, True]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span><br><span class="line">    <span class="comment"># pad mask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;这里的q,k表示的是两个序列（跟注意力机制的q,k没有关系），例如encoder_inputs (x1,x2,..xm)和encoder_inputs (x1,x2..xm)</span></span><br><span class="line"><span class="string">    encoder和decoder都可能调用这个函数，所以seq_len视情况而定</span></span><br><span class="line"><span class="string">    seq_q: [batch_size, seq_len]</span></span><br><span class="line"><span class="string">    seq_k: [batch_size, seq_len]</span></span><br><span class="line"><span class="string">    seq_len could be src_len or it could be tgt_len</span></span><br><span class="line"><span class="string">    seq_len in seq_q and seq_len in seq_k maybe not equal</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    batch_size, len_q = seq_q.size()  <span class="comment"># 这个seq_q只是用来expand维度的</span></span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    <span class="comment"># eq(zero) is PAD token</span></span><br><span class="line">    <span class="comment"># 例如:seq_k = [[1,2,3,4,0], [1,2,3,5,0]]</span></span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># [batch_size, 1, len_k], True is masked</span></span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k)  <span class="comment"># [batch_size, len_q, len_k] 构成一个立方体(batch_size个这样的矩阵)</span></span><br></pre></td></tr></table></figure><h2 id="subsequence-mask">Subsequence Mask</h2><p><strong>Subsequence Mask 只有 Decoder会用到，主要作用是屏蔽未来时刻单词的信息。</strong></p><p>这段代码实现了获得一个注意力子序列掩码。它使用Numpy函数np.triu生成一个上三角矩阵，并用np.ones初始化这个矩阵。attn_shape变量储存了这个矩阵的形状，它是一个三维数组，分别是batch_size、tgt_len、tgt_len。然后，np.triu将这个矩阵初始化为上三角形，并通过参数k=1使对角线上的元素为0。最后，将这个矩阵转换为PyTorchtensor，并返回该张量。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequence_mask</span>(<span class="params">seq</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;建议打印出来看看是什么样的输出（一目了然）</span></span><br><span class="line"><span class="string">    seq: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]</span><br><span class="line">    <span class="comment"># attn_shape: [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">    subsequence_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>)  <span class="comment"># 生成一个上三角矩阵</span></span><br><span class="line">    subsequence_mask = torch.from_numpy(subsequence_mask).byte()</span><br><span class="line">    <span class="keyword">return</span> subsequence_mask  <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br></pre></td></tr></table></figure><h2 id="scaleddotproductattention">ScaledDotProductAttention</h2><p><strong>"Scaled Dot-Product Attention"是一种用于自注意力机制的注意力机制方法，通过将输入矩阵Q、K和V与自身转置进行点积运算，得到关于每一个词的注意力分数。</strong></p><p>使用Q、K、V三个变量作为输入，经过一系列矩阵运算后得到context和attn，其中context是计算出的注意力张量，attn是对应的注意力稀疏矩阵，并返回这两个结果。</p><p>具体地，在forward函数中，使用Q、K做矩阵乘法得到scores矩阵，scores中的每一个元素都是对应Q中词与K中词的相似程度。</p><p>下一步，通过使用mask矩阵对scores中的元素进行赋值，将与mask矩阵中值为1的元素相对应的scores元素赋值为-1e9，使其不被softmax计算。</p><p>最后，使用softmax对scores最后一维（也就是v）做软归一化，得到注意力稀疏矩阵attn。最后，使用attn矩阵对V做矩阵乘法，得到context矩阵，其中每一行对应一个词的向量表示。</p><p>（matmul函数是矩阵乘法，它返回两个矩阵的点积，即将两个矩阵对应元素相乘并相加。它对应的矩阵乘法操作是：C= A * B，其中C是乘积矩阵，A是左矩阵，B是右矩阵。）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line"><span class="string">        K: [batch_size, n_heads, len_k, d_k]</span></span><br><span class="line"><span class="string">        V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line"><span class="string">        attn_mask: [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line"><span class="string">        说明：在encoder-decoder的Attention层中len_q(q1,..qt)和len_k(k1,...km)可能不同</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(d_k)  <span class="comment"># scores : [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        <span class="comment"># mask矩阵填充scores（用-1e9填充scores中与attn_mask中值为1位置相对应的元素）</span></span><br><span class="line">        scores.masked_fill_(attn_mask, -<span class="number">1e9</span>)  <span class="comment"># Fills elements of self tensor with value where mask is True.</span></span><br><span class="line"></span><br><span class="line">        attn = nn.Softmax(dim=-<span class="number">1</span>)(scores)  <span class="comment"># 对最后一个维度(v)做softmax</span></span><br><span class="line">        <span class="comment"># scores : [batch_size, n_heads, len_q, len_k] * V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line">        context = torch.matmul(attn, V)  <span class="comment"># context: [batch_size, n_heads, len_q, d_v]</span></span><br><span class="line">        <span class="comment"># context：[[z1,z2,...],[...]]向量, attn注意力稀疏矩阵（用于可视化的）</span></span><br><span class="line">        <span class="keyword">return</span> context, attn</span><br></pre></td></tr></table></figure><h2 id="multiheadattention">MultiHeadAttention</h2><p>完整代码中一定会有三处地方调用<code>MultiHeadAttention()</code>，Encoder Layer 调用一次，传入的<code>input_Q</code>、<code>input_K</code>、<code>input_V</code>全部都是 <code>enc_inputs</code>；Decoder Layer中两次调用，第一次传入的全是 <code>dec_inputs</code>，第二次传入的分别是<code>dec_outputs</code>，<code>enc_outputs</code>，<code>enc_outputs</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;这个Attention类可以实现:</span></span><br><span class="line"><span class="string">    Encoder的Self-Attention</span></span><br><span class="line"><span class="string">    Decoder的Masked Self-Attention</span></span><br><span class="line"><span class="string">    Encoder-Decoder的Attention</span></span><br><span class="line"><span class="string">    输入：seq_len x d_model</span></span><br><span class="line"><span class="string">    输出：seq_len x d_model</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)  <span class="comment"># q,k必须维度相同，不然无法做点积</span></span><br><span class="line">        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 这个全连接层可以保证多头attention的输出仍然是seq_len x d_model</span></span><br><span class="line">        self.fc = nn.Linear(n_heads * d_v, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        nn.Linear 函数是 PyTorch 模型中的一种全连接层 (fully connected layer) 的实现。</span></span><br><span class="line"><span class="string">        它的作用是对输入数据进行线性变换，即 y = Wx + b，其中 W 是线性变换的系数矩阵，b 是偏移量，x 是输入数据。</span></span><br><span class="line"><span class="string">        torch.nn.Linear(in_features, # 输入的神经元个数</span></span><br><span class="line"><span class="string">           out_features, # 输出神经元个数</span></span><br><span class="line"><span class="string">           bias=True # 是否包含偏置</span></span><br><span class="line"><span class="string">           )</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_Q, input_K, input_V, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        input_Q: [batch_size, len_q, d_model]</span></span><br><span class="line"><span class="string">        input_K: [batch_size, len_k, d_model]</span></span><br><span class="line"><span class="string">        input_V: [batch_size, len_v(=len_k), d_model]</span></span><br><span class="line"><span class="string">        attn_mask: [batch_size, seq_len, seq_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        residual, batch_size = input_Q, input_Q.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 下面的多头的参数矩阵是放在一起做线性变换的，然后再拆成多个头，这是工程实现的技巧</span></span><br><span class="line">        <span class="comment"># B: batch_size, S:seq_len, D: dim</span></span><br><span class="line">        <span class="comment"># (B, S, D) -proj-&gt; (B, S, D_new) -split-&gt; (B, S, Head, W) -trans-&gt; (B, Head, S, W)</span></span><br><span class="line">        <span class="comment">#           线性变换               拆成多头</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line">        Q = self.W_Q(input_Q).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># K: [batch_size, n_heads, len_k, d_k] # K和V的长度一定相同，维度可以不同</span></span><br><span class="line">        K = self.W_K(input_K).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line">        V = self.W_V(input_V).view(batch_size, -<span class="number">1</span>, n_heads, d_v).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 因为是多头，所以mask矩阵要扩充成4维的</span></span><br><span class="line">        <span class="comment"># attn_mask: [batch_size, seq_len, seq_len] -&gt; [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, n_heads, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)</span><br><span class="line">        <span class="comment"># 下面将不同头的输出向量拼接在一起</span></span><br><span class="line">        <span class="comment"># context: [batch_size, n_heads, len_q, d_v] -&gt; [batch_size, len_q, n_heads * d_v]</span></span><br><span class="line">        context = context.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(batch_size, -<span class="number">1</span>, n_heads * d_v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这个全连接层可以保证多头attention的输出仍然是seq_len x d_model</span></span><br><span class="line">        output = self.fc(context)  <span class="comment"># [batch_size, len_q, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_model).to(device)(output + residual), attn</span><br></pre></td></tr></table></figure><h2 id="feedforward-layer">FeedForward Layer</h2><p><strong>这段代码非常简单，就是做两次线性变换，残差连接后再跟一个Layer Norm。用于实现Transformer模型中的前馈网络。</strong></p><p>该网络由两个全连接层（nn.Linear）和一个 ReLU激活函数（nn.ReLU）组成。第一个全连接层将输入从 d_model 维度转换到 d_ff维度，第二个全连接层将输入从 d_ff 维度转换回 d_model 维度。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PoswiseFeedForwardNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(PoswiseFeedForwardNet, self).__init__()</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(d_model, d_ff, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(d_ff, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        inputs: [batch_size, seq_len, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        residual = inputs</span><br><span class="line">        output = self.fc(inputs)</span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_model).to(device)(output + residual)  <span class="comment"># [batch_size, seq_len, d_model]</span></span><br></pre></td></tr></table></figure><h2 id="encoder-layer-encoder">Encoder Layer &amp; Encoder</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.enc_self_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, enc_self_attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len, d_model]</span></span><br><span class="line"><span class="string">        enc_self_attn_mask: [batch_size, src_len, src_len]  mask矩阵(pad mask or sequence mask)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">        <span class="comment"># 第一个enc_inputs * W_Q = Q</span></span><br><span class="line">        <span class="comment"># 第二个enc_inputs * W_K = K</span></span><br><span class="line">        <span class="comment"># 第三个enc_inputs * W_V = V</span></span><br><span class="line">        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs,</span><br><span class="line">                                               enc_self_attn_mask)  <span class="comment"># enc_inputs to same Q,K,V（未线性变换前）</span></span><br><span class="line">        enc_outputs = self.pos_ffn(enc_outputs)</span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs, attn</span><br></pre></td></tr></table></figure><p><code>nn.ModuleList()</code> 列表里面存了 <code>n_layers</code> 个Encoder Layer。由于我们控制好了 Encoder Layer的输入和输出维度相同，所以可以直接用个 for 循环以嵌套的方式，将上一次Encoder Layer 的输出作为下一次 Encoder Layer 的输入。</p><p><strong>将<code>n_layers</code>个（本文为6个）EncoderLayer组件逐个拼起来，就是一个完整的Encoder。</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.src_emb = nn.Embedding(src_vocab_size, d_model)  <span class="comment"># token Embedding</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Embedding解释</span></span><br><span class="line"><span class="string">        例如：如果你有一个词语表(vocabulary)，其中包含了3个词语：&quot;dog&quot;, &quot;cat&quot;, &quot;bird&quot;。并且你指定了src_vocab_size = 3</span></span><br><span class="line"><span class="string">        和d_model = 5，那么这个Embedding层就可以将每一个词语表示成一个5维的实数向量，比如：&quot;dog&quot;</span></span><br><span class="line"><span class="string">        可以表示为[0.1, 0.2, 0.3, 0.4, 0.5]。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)  <span class="comment"># Transformer中位置编码时固定的，不需要学习</span></span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs)  <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        enc_outputs = self.pos_emb(enc_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        <span class="comment"># Encoder输入序列的pad mask矩阵</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)  <span class="comment"># [batch_size, src_len, src_len]</span></span><br><span class="line">        enc_self_attns = []  <span class="comment"># 在计算中不需要用到，它主要用来保存你接下来返回的attention的值（这个主要是为了你画热力图等，用来看各个词之间的关系</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:  <span class="comment"># for循环访问nn.ModuleList对象</span></span><br><span class="line">            <span class="comment"># 上一个block的输出enc_outputs作为当前block的输入</span></span><br><span class="line">            <span class="comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">            enc_outputs, enc_self_attn = layer(enc_outputs,</span><br><span class="line">                                               enc_self_attn_mask)  <span class="comment"># 传入的enc_outputs其实是input，传入mask矩阵是因为你要做self attention</span></span><br><span class="line">            enc_self_attns.append(enc_self_attn)  <span class="comment"># 这个只是为了可视化</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs, enc_self_attns</span><br></pre></td></tr></table></figure><h1 id="decoder-layer-decoder">Decoder Layer &amp; Decoder</h1><p>在 Decoder Layer 中会调用两次<code>MultiHeadAttention</code>，第一次是计算 Decoder Input 的self-attention，得到输出 <code>dec_outputs</code>。然后将<code>dec_outputs</code> 作为生成 Q 的元素，<code>enc_outputs</code>作为生成 K 和 V 的元素，再调用一次<code>MultiHeadAttention</code>，得到的是 Encoder 和 Decoder Layer之间的 context vector。最后将 <code>dec_outptus</code>做一次维度变换，然后返回。</p><p><strong>将<code>n_layers</code>个（本文为6个）DecoderLayer组件逐个拼起来，就是一个完整的Decoder。</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.dec_self_attn = MultiHeadAttention()</span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len, d_model]</span></span><br><span class="line"><span class="string">        enc_outputs: [batch_size, src_len, d_model]</span></span><br><span class="line"><span class="string">        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"><span class="string">        dec_enc_attn_mask: [batch_size, tgt_len, src_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]</span></span><br><span class="line">        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs,</span><br><span class="line">                                                        dec_self_attn_mask)  <span class="comment"># 这里的Q,K,V全是Decoder自己的输入</span></span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span></span><br><span class="line">        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs,</span><br><span class="line">                                                      dec_enc_attn_mask)  <span class="comment"># Attention层的Q(来自decoder) 和 K,V(来自encoder)</span></span><br><span class="line">        dec_outputs = self.pos_ffn(dec_outputs)  <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attn, dec_enc_attn  <span class="comment"># dec_self_attn, dec_enc_attn这两个是为了可视化的</span></span><br></pre></td></tr></table></figure><p>Decoder 中不仅要把 "pad"mask 掉，还要 mask未来时刻的信息，因此就有了下面这三行代码，其中<code>torch.gt(a, value)</code> 的意思是，将 a 中各个位置上的元素和value 比较，若大于 value，则该位置取 1，否则取 0</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)  <span class="comment"># Decoder输入的embed词表</span></span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])  <span class="comment"># Decoder的blocks</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        enc_outputs: [batch_size, src_len, d_model]   # 用在Encoder-Decoder Attention层</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs)  <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_outputs = self.pos_emb(dec_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>).to(</span><br><span class="line">            device)  <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="comment"># Decoder输入序列的pad mask矩阵（这个例子中decoder是没有加pad的，实际应用中都是有pad填充的）</span></span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).to(device)  <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">        <span class="comment"># Masked Self_Attention：当前时刻是看不到未来的信息的</span></span><br><span class="line">        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).to(</span><br><span class="line">            device)  <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Decoder中把两种mask矩阵相加（既屏蔽了pad的信息，也屏蔽了未来时刻的信息）</span></span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask),</span><br><span class="line">                                      <span class="number">0</span>).to(device)  <span class="comment"># [batch_size, tgt_len, tgt_len]; torch.gt比较两个矩阵的元素，大于则返回1，否则返回0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这个mask主要用于encoder-decoder attention层</span></span><br><span class="line">        <span class="comment"># get_attn_pad_mask主要是enc_inputs的pad mask矩阵(因为enc是处理K,V的，求Attention时是用v1,v2,..vm去加权的，要把pad对应的v_i的相关系数设为0，这样注意力就不会关注pad向量)</span></span><br><span class="line">        <span class="comment">#                       dec_inputs只是提供expand的size的</span></span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)  <span class="comment"># [batc_size, tgt_len, src_len]</span></span><br><span class="line"></span><br><span class="line">        dec_self_attns, dec_enc_attns = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span></span><br><span class="line">            <span class="comment"># Decoder的Block是上一个Block的输出dec_outputs（变化）和Encoder网络的输出enc_outputs（固定）</span></span><br><span class="line">            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask,</span><br><span class="line">                                                             dec_enc_attn_mask)</span><br><span class="line">            dec_self_attns.append(dec_self_attn)</span><br><span class="line">            dec_enc_attns.append(dec_enc_attn)</span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure><h2 id="transformer">Transformer</h2><p><strong>这段代码实现了Transformer类，用到的三种架构--前面定义过的Encoder和Decoder，以及下面新定义的投影层Projection（projection实现的是decoder后面的linear，之所以没有实现softmax是因为后续的贪婪解码器替代了softmax层的工作，直接得到概率最大值的词表索引并输出）。</strong></p><p>在输入经过Encoder网络和Decoder网络处理后，得到的输出分别是enc_outputs和dec_outputs。最后再经过一个投影层，将dec_outputs映射成dec_logits，表示每个单词的词概率分布。返回dec_logits, enc_self_attns, dec_self_attns, dec_enc_attns。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        <span class="comment"># Transformer类继承自PyTorch的nn.Module类。</span></span><br><span class="line">        <span class="comment"># 通过在Transformer的构造函数中调用super(Transformer, self).init()，</span></span><br><span class="line">        <span class="comment"># 可以调用nn.Module的构造函数，以便初始化nn.Module的一些内部状态，以及设置Transformer类对象的一些公共属性。</span></span><br><span class="line">        self.encoder = Encoder().to(device)</span><br><span class="line">        self.decoder = Decoder().to(device)</span><br><span class="line">        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=<span class="literal">False</span>).to(device)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, dec_inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Transformers的输入：两个序列</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># tensor to store decoder outputs</span></span><br><span class="line">        <span class="comment"># outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">        <span class="comment"># 经过Encoder网络后，得到的输出还是[batch_size, src_len, d_model]</span></span><br><span class="line">        enc_outputs, enc_self_attns = self.encoder(enc_inputs)</span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]</span></span><br><span class="line">        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)</span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model] -&gt; dec_logits: [batch_size, tgt_len, tgt_vocab_size]</span></span><br><span class="line">        dec_logits = self.projection(dec_outputs)</span><br><span class="line">        <span class="keyword">return</span> dec_logits.view(-<span class="number">1</span>, dec_logits.size(-<span class="number">1</span>)), enc_self_attns, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure><p><strong>view函数说明</strong></p><p>view 方法是 PyTorch 中 tensor 的一种 reshape 操作。它将一个 tensor 的shape 变成给定的形状。</p><p>在这段代码中， dec_logits.view(-1, dec_logits.size(-1)) 表示将dec_logits tensor 从原来的 shape 变成了一个新的 shape，其中第一维是-1，这意味着该维的长度是自动计算的（其他维度的长度已经确定了），第二维是dec_logits.size(-1)，这是一个数字，代表 dec_logits tensor的最后一维的长度。</p><p><strong>forward函数说明：</strong></p><p>通过进行一次前向传播的操作（比如 output = model(input)）时，PyTorch内部会对模型中每一个模块（包括 Encoder 和 EncoderLayer）中的 forward函数进行调用，以计算输出结果。</p><p>如果不手动定义 forward 函数，那么模型将不会被调用。因此，forward函数是必须被定义的，用于计算模型的前向传播过程。</p><h1 id="模型调用-损失函数-优化器">模型调用 &amp; 损失函数 &amp;优化器</h1><p>这段代码调用了Transformer模型，并设置了交叉熵损失函数（将ignore_index参数设置为0），优化器使用随机梯度下降（SGD）算法。（优化器将使用model.parameters()作为参数进行优化，学习率为1e-3，动量为0.99）。</p><p>ignore_index参数被设置为0，这样损失计算将忽略任何索引为0的输入，这通常是为NLP模型中的paddingtoken保留的。</p><p>使用Adam算法对于较小的数据量效果很差</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Transformer().to(device)</span><br><span class="line"><span class="comment"># 这里的损失函数里面设置了一个参数 ignore_index=0，因为 &quot;pad&quot; 这个单词的索引为 0，这样设置以后，就不会计算 &quot;pad&quot; 的损失（因为本来 &quot;pad&quot; 也没有意义，不需要计算）</span></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-3</span>, momentum=<span class="number">0.99</span>)</span><br><span class="line"><span class="comment"># optimizer = optim.Adam(model.parameters(), lr=1e-9) # 用adam的话效果不好</span></span><br></pre></td></tr></table></figure><h1 id="训练">训练</h1><p><strong>最后三行代码是在进行一次反向传播迭代的操作。</strong></p><p>分三步执行：optimizer.zero_grad()：对梯度进行初始化，因为pytorch的梯度是累加的，所以每次计算前需要把梯度归零。loss.backward()：计算当前损失函数的梯度，并且完成反向传播。optimizer.step()：执行优化器的更新操作，根据梯度对模型参数进行更新。</p><p>总的来说，这三步代码是完成一次机器学习模型的参数优化的核心过程。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> enc_inputs, dec_inputs, dec_outputs <span class="keyword">in</span> loader:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        dec_outputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_inputs, dec_inputs, dec_outputs = enc_inputs.to(device), dec_inputs.to(device), dec_outputs.to(device)</span><br><span class="line">        <span class="comment"># outputs: [batch_size * tgt_len, tgt_vocab_size]</span></span><br><span class="line">        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)</span><br><span class="line">        loss = criterion(outputs, dec_outputs.view(-<span class="number">1</span>))  <span class="comment"># dec_outputs.view(-1):[batch_size * tgt_len * tgt_vocab_size]</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (epoch + <span class="number">1</span>), <span class="string">&#x27;loss =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure><p>训练输出如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># OUTPUT</span><br><span class="line">    Epoch: 0001 loss = 2.752963</span><br><span class="line">    Epoch: 0002 loss = 2.625792</span><br><span class="line">    Epoch: 0003 loss = 2.508290</span><br><span class="line">    Epoch: 0004 loss = 2.260287</span><br><span class="line">    Epoch: 0005 loss = 2.026300</span><br><span class="line">    Epoch: 0006 loss = 1.715278</span><br><span class="line">    Epoch: 0007 loss = 1.559423</span><br><span class="line">    Epoch: 0008 loss = 1.339594</span><br><span class="line">    Epoch: 0009 loss = 1.082600</span><br><span class="line">    Epoch: 0010 loss = 0.945365</span><br><span class="line">    Epoch: 0011 loss = 0.758650</span><br><span class="line">    Epoch: 0012 loss = 0.581669</span><br><span class="line">    Epoch: 0013 loss = 0.461126</span><br><span class="line">    Epoch: 0014 loss = 0.340352</span><br><span class="line">    Epoch: 0015 loss = 0.252243</span><br><span class="line">    Epoch: 0016 loss = 0.190971</span><br><span class="line">    Epoch: 0017 loss = 0.161172</span><br><span class="line">    Epoch: 0018 loss = 0.130554</span><br><span class="line">    Epoch: 0019 loss = 0.101027</span><br><span class="line">    Epoch: 0020 loss = 0.093038</span><br><span class="line">    Epoch: 0021 loss = 0.079294</span><br><span class="line">    Epoch: 0022 loss = 0.070799</span><br><span class="line">    Epoch: 0023 loss = 0.062795</span><br><span class="line">    Epoch: 0024 loss = 0.044259</span><br><span class="line">    Epoch: 0025 loss = 0.056274</span><br><span class="line">    Epoch: 0026 loss = 0.033928</span><br><span class="line">    Epoch: 0027 loss = 0.037328</span><br><span class="line">    Epoch: 0028 loss = 0.035663</span><br><span class="line">    Epoch: 0029 loss = 0.032550</span><br><span class="line">    Epoch: 0030 loss = 0.029425</span><br><span class="line">    Epoch: 0031 loss = 0.028057</span><br><span class="line">    Epoch: 0032 loss = 0.024588</span><br><span class="line">    Epoch: 0033 loss = 0.019545</span><br><span class="line">    Epoch: 0034 loss = 0.025953</span><br><span class="line">    Epoch: 0035 loss = 0.018335</span><br><span class="line">    Epoch: 0036 loss = 0.028104</span><br><span class="line">    Epoch: 0037 loss = 0.015952</span><br><span class="line">    Epoch: 0038 loss = 0.014356</span><br><span class="line">    Epoch: 0039 loss = 0.015536</span><br><span class="line">    Epoch: 0040 loss = 0.013210</span><br><span class="line">    Epoch: 0041 loss = 0.015791</span><br><span class="line">    Epoch: 0042 loss = 0.013085</span><br><span class="line">    Epoch: 0043 loss = 0.011149</span><br><span class="line">    Epoch: 0044 loss = 0.009110</span><br><span class="line">    Epoch: 0045 loss = 0.007416</span><br><span class="line">    Epoch: 0046 loss = 0.005960</span><br><span class="line">    Epoch: 0047 loss = 0.006156</span><br><span class="line">    Epoch: 0048 loss = 0.004907</span><br><span class="line">    Epoch: 0049 loss = 0.004867</span><br><span class="line">    Epoch: 0050 loss = 0.005042</span><br><span class="line">    Epoch: 0051 loss = 0.005796</span><br><span class="line">    Epoch: 0052 loss = 0.005398</span><br><span class="line">    Epoch: 0053 loss = 0.004669</span><br><span class="line">    Epoch: 0054 loss = 0.004401</span><br><span class="line">    Epoch: 0055 loss = 0.003372</span><br><span class="line">    Epoch: 0056 loss = 0.002630</span><br><span class="line">    Epoch: 0057 loss = 0.002565</span><br><span class="line">    Epoch: 0058 loss = 0.002309</span><br><span class="line">    Epoch: 0059 loss = 0.003040</span><br><span class="line">    Epoch: 0060 loss = 0.002470</span><br><span class="line">    Epoch: 0061 loss = 0.002096</span><br><span class="line">    Epoch: 0062 loss = 0.002189</span><br><span class="line">    Epoch: 0063 loss = 0.002061</span><br><span class="line">    Epoch: 0064 loss = 0.001174</span><br><span class="line">    Epoch: 0065 loss = 0.001599</span><br><span class="line">    Epoch: 0066 loss = 0.001527</span><br><span class="line">    Epoch: 0067 loss = 0.001685</span><br><span class="line">    Epoch: 0068 loss = 0.001565</span><br><span class="line">    Epoch: 0069 loss = 0.001718</span><br><span class="line">    Epoch: 0070 loss = 0.001291</span><br><span class="line">    Epoch: 0071 loss = 0.001259</span><br><span class="line">    Epoch: 0072 loss = 0.001222</span><br><span class="line">    Epoch: 0073 loss = 0.001179</span><br><span class="line">    Epoch: 0074 loss = 0.000965</span><br><span class="line">    Epoch: 0075 loss = 0.001888</span><br><span class="line">    Epoch: 0076 loss = 0.001052</span><br><span class="line">    Epoch: 0077 loss = 0.000888</span><br><span class="line">    Epoch: 0078 loss = 0.001349</span><br><span class="line">    Epoch: 0079 loss = 0.000916</span><br><span class="line">    Epoch: 0080 loss = 0.001315</span><br><span class="line">    Epoch: 0081 loss = 0.001191</span><br><span class="line">    Epoch: 0082 loss = 0.001341</span><br><span class="line">    Epoch: 0083 loss = 0.001674</span><br><span class="line">    Epoch: 0084 loss = 0.001122</span><br><span class="line">    Epoch: 0085 loss = 0.001133</span><br><span class="line">    Epoch: 0086 loss = 0.000839</span><br><span class="line">    Epoch: 0087 loss = 0.001059</span><br><span class="line">    Epoch: 0088 loss = 0.001204</span><br><span class="line">    Epoch: 0089 loss = 0.001092</span><br><span class="line">    Epoch: 0090 loss = 0.000943</span><br><span class="line">    Epoch: 0091 loss = 0.000699</span><br><span class="line">    Epoch: 0092 loss = 0.001015</span><br><span class="line">    Epoch: 0093 loss = 0.000730</span><br><span class="line">    Epoch: 0094 loss = 0.000795</span><br><span class="line">    Epoch: 0095 loss = 0.000926</span><br><span class="line">    Epoch: 0096 loss = 0.000948</span><br><span class="line">    Epoch: 0097 loss = 0.000945</span><br><span class="line">    Epoch: 0098 loss = 0.000730</span><br><span class="line">    Epoch: 0099 loss = 0.000747</span><br><span class="line">    Epoch: 0100 loss = 0.000749</span><br></pre></td></tr></table></figure><h1 id="测试">测试</h1><p>这段代码是一个贪心解码器(greedydecoder)的实现，其作用是在给定编码输入(enc_input)和起始符号(start_symbol)的情况下，根据给定的模型(model)预测出目标序列(greedy_dec_predict)。</p><p>首先，编码器(encoder)对编码输入(enc_input)进行处理，生成编码输出(enc_outputs)和注意力权值(enc_self_attns)。</p><p>然后初始化解码器(decoder)的输入(dec_input)为一个空的tensor。</p><p>接着，在没有到达终止符的情况下，不断执行以下步骤：</p><ol type="1"><li>将解码器的输入(dec_input)拼接上当前的符号(next_symbol)。</li><li>解码器(decoder)对拼接后的输入(dec_input)、编码输入(enc_input)和编码输出(enc_outputs)进行处理，生成解码输出(dec_outputs)。</li><li>投影层(projection)将解码输出(dec_outputs)映射到词表上，生成预测概率分布(projected)。</li><li>根据预测概率分布(projected)，选择概率最大的下一个词，并将其作为下一个符号(next_symbol)。</li><li>如果下一个符号是终止符，终止循环。</li></ol><p>最后，返回除开初始符号以外的预测的目标序列(greedy_dec_predict)。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decoder</span>(<span class="params">model, enc_input, start_symbol</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;贪心编码</span></span><br><span class="line"><span class="string">    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don&#x27;t know the</span></span><br><span class="line"><span class="string">    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.</span></span><br><span class="line"><span class="string">    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding</span></span><br><span class="line"><span class="string">    :param model: Transformer Model</span></span><br><span class="line"><span class="string">    :param enc_input: The encoder input</span></span><br><span class="line"><span class="string">    :param start_symbol: The start symbol. In this example it is &#x27;S&#x27; which corresponds to index 8</span></span><br><span class="line"><span class="string">    :return: The target input</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    enc_outputs, enc_self_attns = model.encoder(enc_input)</span><br><span class="line">    dec_input = torch.zeros(<span class="number">1</span>, <span class="number">0</span>).type_as(enc_input.data)  <span class="comment"># 初始化一个空的tensor: tensor([], size=(1, 0), dtype=torch.int64)</span></span><br><span class="line">    terminal = <span class="literal">False</span></span><br><span class="line">    next_symbol = start_symbol</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> terminal:</span><br><span class="line">        <span class="comment"># 预测阶段：dec_input序列会一点点变长（每次添加一个新预测出来的单词）</span></span><br><span class="line">        dec_input = torch.cat([dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],</span><br><span class="line">                              -<span class="number">1</span>)</span><br><span class="line">        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)</span><br><span class="line">        projected = model.projection(dec_outputs)</span><br><span class="line">        prob = projected.squeeze(<span class="number">0</span>).<span class="built_in">max</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">False</span>)[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 这行代码替代了softmax层的工作，直接得到概率最大值的词表索引</span></span><br><span class="line">        <span class="comment"># 1. 从 tensor 中删除所有维度大小为1的维：projected.squeeze(0)；</span></span><br><span class="line">        <span class="comment"># 2. 通过 dim=-1 参数，在最后一维（即维度的索引为 -1）上，找到最大的值的索引：max(dim=-1)；</span></span><br><span class="line">        <span class="comment"># 3. 通过 keepdim=False 参数，将最后一维的维度删除，同时返回结果的最大值的索引：[1]。</span></span><br><span class="line">        <span class="comment"># 因此，该行代码得到prob 变量的结果，存储了 projected 处理后得到的最大值的索引。</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 增量更新（我们希望重复单词预测结果是一样的）</span></span><br><span class="line">        <span class="comment"># 我们在预测时会选择性忽略重复的预测的词，只摘取最新预测的单词拼接到输入序列中</span></span><br><span class="line">        next_word = prob.data[-<span class="number">1</span>]  <span class="comment"># 拿出当前预测的单词(数字)。我们用x_t对应的输出z_t去预测下一个单词的概率，不用z_1,z_2..z_&#123;t-1&#125;</span></span><br><span class="line">        next_symbol = next_word</span><br><span class="line">        <span class="keyword">if</span> next_symbol == tgt_vocab[<span class="string">&quot;E&quot;</span>]:</span><br><span class="line">            terminal = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># print(next_word)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># greedy_dec_predict = torch.cat(</span></span><br><span class="line">    <span class="comment">#     [dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],</span></span><br><span class="line">    <span class="comment">#     -1)</span></span><br><span class="line">    greedy_dec_predict = dec_input[:, <span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">return</span> greedy_dec_predict</span><br></pre></td></tr></table></figure><p>这段代码实现了一个简单的预测（因为数据量较小，因此测试集选用训练集中的一句）</p><ul><li>测试集（希望transformer能达到的效果）</li><li>输入："我 有 一 个 女 朋 友"</li><li>输出："i have a girl friend"</li></ul><p>过程如下：</p><ol type="1"><li>定义了一个句子列表sentences，其中包含一个中文句子和对应的空的英文句子。</li><li>使用make_data函数处理句子列表，获得编码句子、解码句子的输入和输出。</li><li>创建一个数据加载器test_loader，用于加载处理后的句子数据。</li><li>使用next函数从数据加载器中读取一个批次的数据，并将其分别赋给编码句子的输入。</li><li>对于每个编码句子，调用greedy_decoder函数，通过训练好的Transformer模型，将其翻译成英文句子。</li><li>最后，输出编码句子和对应的解码句子，以及它们的中英文词语对应关系。</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ==========================================================================================</span></span><br><span class="line"><span class="comment"># 预测阶段</span></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="comment"># enc_input                dec_input           dec_output</span></span><br><span class="line">    [<span class="string">&#x27;我 有 一 个 女 朋 友&#x27;</span>, <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line">test_loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line">enc_inputs, _, _ = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_loader))</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用训练好的Transformer模型将中文句子&#x27;我 有 一 个 女 朋 友&#x27; 翻译成英文句子: &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(enc_inputs)):</span><br><span class="line">    greedy_dec_predict = greedy_decoder(model, enc_inputs[i].view(<span class="number">1</span>, -<span class="number">1</span>).to(device), start_symbol=tgt_vocab[<span class="string">&quot;S&quot;</span>])</span><br><span class="line">    <span class="built_in">print</span>(enc_inputs[i], <span class="string">&#x27;-&gt;&#x27;</span>, greedy_dec_predict.squeeze())</span><br><span class="line">    <span class="built_in">print</span>([src_idx2word[t.item()] <span class="keyword">for</span> t <span class="keyword">in</span> enc_inputs[i]], <span class="string">&#x27;-&gt;&#x27;</span>,</span><br><span class="line">          [idx2word[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> greedy_dec_predict.squeeze()])</span><br></pre></td></tr></table></figure><pre><code>==============================利用训练好的Transformer模型将中文句子&#39;我 有 一 个 女 朋 友&#39; 翻译成英文句子: tensor([1, 2, 3, 4, 9, 6, 7]) -&gt; tensor([ 1,  2,  3,  7,  5, 10])[&#39;我&#39;, &#39;有&#39;, &#39;一&#39;, &#39;个&#39;, &#39;女&#39;, &#39;朋&#39;, &#39;友&#39;] -&gt; [&#39;i&#39;, &#39;have&#39;, &#39;a&#39;, &#39;girl&#39;, &#39;friend&#39;, &#39;.&#39;]</code></pre><p><strong>next 函数说明：</strong></p><p>"next"函数用于返回迭代器的下一个项目，即从迭代器中获取下一个数据项。在这段代码中，使用next(iter(test_loader)) 获取第一个批次的数据，赋值给 enc_inputs, _ , _三个变量。</p><p><strong>因为数据量较小，如果测试集选用新的句子，那么结果就不尽人意</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ==========================================================================================</span></span><br><span class="line"><span class="comment"># 预测阶段</span></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="comment"># enc_input                dec_input           dec_output</span></span><br><span class="line">    [<span class="string">&#x27;我 有 一 个 好 朋 友&#x27;</span>, <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line">test_loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line">enc_inputs, _, _ = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_loader))</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用训练好的Transformer模型将中文句子&#x27;我 有 一 个 好 朋 友&#x27; 翻译成英文句子: &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(enc_inputs)):</span><br><span class="line">    greedy_dec_predict = greedy_decoder(model, enc_inputs[i].view(<span class="number">1</span>, -<span class="number">1</span>).to(device), start_symbol=tgt_vocab[<span class="string">&quot;S&quot;</span>])</span><br><span class="line">    <span class="built_in">print</span>(enc_inputs[i], <span class="string">&#x27;-&gt;&#x27;</span>, greedy_dec_predict.squeeze())</span><br><span class="line">    <span class="built_in">print</span>([src_idx2word[t.item()] <span class="keyword">for</span> t <span class="keyword">in</span> enc_inputs[i]], <span class="string">&#x27;-&gt;&#x27;</span>,</span><br><span class="line">          [idx2word[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> greedy_dec_predict.squeeze()])</span><br></pre></td></tr></table></figure><pre><code>==============================利用训练好的Transformer模型将中文句子&#39;我 有 一 个 好 朋 友&#39; 翻译成英文句子: tensor([1, 2, 3, 4, 5, 6, 7]) -&gt; tensor([ 1,  2,  3,  7,  5, 10])[&#39;我&#39;, &#39;有&#39;, &#39;一&#39;, &#39;个&#39;, &#39;好&#39;, &#39;朋&#39;, &#39;友&#39;] -&gt; [&#39;i&#39;, &#39;have&#39;, &#39;a&#39;, &#39;girl&#39;, &#39;friend&#39;, &#39;.&#39;]</code></pre><h1 id="参考链接">参考链接</h1><h2 id="本文详细代码">本文详细代码</h2><ol type="1"><li><p>Colab:</p><p><ahref="https://github.com/serika-onoe/transformer_reproduction/blob/main/Transformer_Pytorch.ipynb">https://github.com/serika-onoe/transformer_reproduction/blob/main/Transformer_Pytorch.ipynb</a></p></li><li><p>Github:</p><p><ahref="https://github.com/serika-onoe/transformer_reproduction">https://github.com/serika-onoe/transformer_reproduction</a></p></li></ol><h2 id="参考网站">参考网站</h2><ol type="1"><li><p>Transformer 的 PyTorch 实现:</p><p><ahref="https://wmathor.com/index.php/archives/1455/">https://wmathor.com/index.php/archives/1455/</a></p></li><li><p>手把手教你用Pytorch代码实现Transformer模型（超详细的代码解读）:</p><p><ahref="https://blog.csdn.net/qq_43827595/article/details/120394042">https://blog.csdn.net/qq_43827595/article/details/120394042</a></p></li></ol><h2 id="完整代码实现">完整代码实现</h2><ul><li><p>哈佛大学NLP组的notebook，很详细文字和代码描述，用pytorch实现</p><p><ahref="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p></li><li><p>Google的TensorFlow官方的，用tf keras实现</p><p><ahref="https://www.tensorflow.org/tutorials/text/transformer">https://www.tensorflow.org/tutorials/text/transformer</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;导航&quot;&gt;导航&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;这篇文章是基于 Pytorch 的 Transformer
简易版复现教程。&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客的 Transformer 系列文章共计四篇，导航如下：&lt;/p&gt;
</summary>
      
    
    
    
    <category term="笔记, 教程" scheme="https://le-ying.github.io/categories/%E7%AC%94%E8%AE%B0-%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="科研" scheme="https://le-ying.github.io/tags/%E7%A7%91%E7%A0%94/"/>
    
    <category term="transformer" scheme="https://le-ying.github.io/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 零基础解析教程，剥洋葱般层层剖析内在原理（2/4）</title>
    <link href="https://le-ying.github.io/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/"/>
    <id>https://le-ying.github.io/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/</id>
    <published>2023-01-11T09:00:10.000Z</published>
    <updated>2023-02-26T03:35:12.223Z</updated>
    
    <content type="html"><![CDATA[<h1 id="导航">导航</h1><p><strong>这篇文章是关于Transformer的原理详细解读。</strong></p><blockquote><p>本博客的 Transformer 系列文章共计四篇，导航如下：</p></blockquote><ul><li><p><ahref="https://serika-onoe.github.io/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/">Transformer零基础解析教程，从Encoder-Decoder架构说起（1/4）</a></p></li><li><p><ahref="https://serika-onoe.github.io/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/">Transformer零基础解析教程，剥洋葱般层层剖析内在原理（2/4）<strong>本篇</strong></a></p></li><li><p><ahref="https://serika-onoe.github.io/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/">Transformer零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）</a></p></li><li><p><ahref="https://serika-onoe.github.io/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/">Transformer零基础解析教程，完整版代码最终挑战（4/4）</a></p></li></ul><h1 id="transformer之前的翻译模型">Transformer之前的翻译模型</h1><p>在Transformer之前，递归神经网络(RNN)一直是处理序列数据的首选方法，大家做机器翻译用的最多的就是基于RNN的Encoder-Decoder模型。</p><imgsrc="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-02.jpg" /><center><code>图1: RNN的工作方式</code></center><p>输入:</p><ul><li><p>输入向量 <span class="math inline">\(\vec{x_t}\)</span>(编码词)</p></li><li><p>隐藏状态向量 <spanclass="math inline">\(\vec{h_{t-1}}\)</span>（包含当前块之前的序列状态)</p></li></ul><p>输出：</p><ul><li>输出向量 <span class="math inline">\(\vec{o_t}\)</span></li></ul><p>权重：</p><ul><li><p><span class="math inline">\({W}\)</span>—— <spanclass="math inline">\(\vec{x_t}\)</span> 和 <spanclass="math inline">\(\vec{h_t}\)</span> 之间的权重</p></li><li><p><span class="math inline">\({V}\)</span>—— <spanclass="math inline">\(\vec{ h_{t-1} }\)</span> 和 <spanclass="math inline">\(\vec{h_t}\)</span> 之间的权重</p></li><li><p><span class="math inline">\({U}\)</span>—— <spanclass="math inline">\(\vec{h_t}\)</span> 和 <spanclass="math inline">\(\vec{o_t}\)</span> 之间的权重</p></li></ul><p>RNN的工作方式类似于前馈神经网络，它会将输入序列一个接一个地读取。因此在基于RNN的Encoder-Decoder模型中，编码器的目标是从顺序输入中提取数据，并将其编码为向量（即输入的表示形式）。而解码器代替输出固定长度向量的分类器，与单独使用输入中的每个符号的编码器一样，解码器在多个时间步长内生成每个输出符号。</p><imgsrc="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-03.jpg" /><center><code>图2: Encoder-Decoder进行英-法翻译的例子</code></center><p><br/><br/>例如，在机器翻译中，输入是英文句子，输出是翻译出的法语句子。Encoder将按顺序展开每个单词，并形成输入英文句子的固定长度向量表示（也就是上篇博客中的<spanclass="math inline">\(C\)</span>）。然后Decoder将固定长度的向量表示作为输入，依次产生每个法语单词，形成翻译后的法语句子。</p><h2 id="原有模型的缺陷">原有模型的缺陷</h2><p>原有的模型，即基于RNN的Encoder-Decoder存在一些问题：</p><ol type="1"><li><p>训练速度慢：输入数据需要一个接一个地顺序处理，这种串行的循环过程不适用于擅长并行计算的GPU。</p></li><li><p>难以处理长序列：</p><ul><li>如果输入序列太长，会出现梯度消失和爆炸问题。一般在训练过程中会在loss中看到NaN（Nota Number）。这些也称为 RNN 中的长期依赖问题。</li><li>上下文向量长度固定，使用固定长度的向量表示输入序列来解码一个全新的句子是很困难的。如果输入序列很大，则上下文向量无法存储所有信息。此外，也很难区分具有相似单词但具有不同含义的句子。</li></ul></li></ol><p>第1个问题很好理解，不过第2个问题中，关于梯度消失和爆炸的问题，可以先看看以下的补充说明。</p><h3 id="什么是梯度消失和梯度爆炸">什么是梯度消失和梯度爆炸</h3><p>高中数学有教过，我们可以利用微分的方法来求函数的最大值与最小值。在机器学习中，梯度是一个向量，它表示网络误差函数关于所有权重的偏导数。梯度优化算法就是<strong>通过不断计算梯度，并使用梯度优化算法来调整权重，使得代价函数CostFunction (预测结果究竟与实际答案差了多少)越来越小</strong>，也意味着网络能够更好地适应训练数据。当网络的CostFunction达到最小值时，网络就能对新的数据进行较好的预测。</p><p>当我们的代价函数是线性函数时，我们就能够用梯度下降法(GradientDescent)来快速的求出代价函数（在图中记为<spanclass="math inline">\(J(w)\)</span>）的最小值，如图：</p><img src="https://i.imgur.com/EFs14Nt.png" /><center><code>图3: 梯度下降法的示意图</code></center><p><br/><br/>而梯度消失和梯度爆炸是深度神经网络训练中的两种典型问题。</p><p><strong>梯度消失（vanishing gradient）</strong>：指在深层网络训练中，由于梯度的较小值逐层传递，导致较深层的权值参数的更新量非常小，趋近于0。这样会导致较深层网络的参数无法得到有效更新，从而使整个网络无法学习。</p><p><strong>梯度爆炸（exploding gradient）</strong>：指在深层网络训练中，由于梯度的较大值逐层传递，导致较深层的权值参数的更新量非常大，甚至无限大。这样会导致较深层网络的参数更新量过大，从而使整个网络无法学习。</p><p>如果有疑问，请参考下面RNN的消失和爆炸的原理说明：</p><h3 id="rnn梯度消失和爆炸的原理">RNN梯度消失和爆炸的原理</h3><p><strong>RNN的统一定义为</strong></p><p><span class="math display">\[\begin{equation}h_t = f\left(x_t, h_{t-1};\theta\right)\end{equation}\]</span></p><ul><li><spanclass="math inline">\(h_t\)</span>是每一步的输出，也就是隐藏状态，由当前输入<spanclass="math inline">\(x_t\)</span>和前一时刻的输出<spanclass="math inline">\(h_{t-1}\)</span>共同决定</li><li><span class="math inline">\(\theta\)</span>则是可训练的参数</li></ul><p>(在做基本分析时，我们可以假设<spanclass="math inline">\(h_t,x_t,\theta\)</span>都是一维的，这可以让我们获得最直观的理解，其结果对高维情形仍有参考价值。)</p><p><strong>RNN梯度的表达式为</strong></p><p><span class="math display">\[\begin{equation}\frac{d h_t}{d\theta} = \frac{\partial h_t}{\partialh_{t-1}}\frac{d h_{t-1}}{d\theta} + \frac{\partial h_t}{\partial\theta}\end{equation}\]</span></p><p>这个公式的意思是，我们可以递推地计算出每一个时间步的隐藏状态对于参数的偏导数，也就是梯度。这样就可以用梯度下降算法来更新网络中的参数，使得网络能够更好地适应训练数据。</p><p>可以看到，其实RNN的梯度也是一个RNN，当前时刻梯度<spanclass="math inline">\(\frac{d h_t}{d\theta}\)</span>是前一时刻梯度<spanclass="math inline">\(\frac{dh_{t-1}}{d\theta}\)</span>与当前运算梯度<spanclass="math inline">\(\frac{\partial h_t}{\partial\theta}\)</span>的函数。同时，从上式我们就可以看出，其实梯度消失或者梯度爆炸现象几乎是必然存在的：</p><ul><li>当<span class="math inline">\(\left|\frac{\partial h_t}{\partialh_{t-1}}\right| &lt;1\)</span>时，意味着历史的梯度信息逐步衰减，因此步数多了梯度必然消失（好比<spanclass="math inline">\(\lim\limits_{n\to\infty} 0.9^n \to0\)</span>）；</li><li>当<span class="math inline">\(\left|\frac{\partial h_t}{\partialh_{t-1}}\right| &gt;1\)</span>时，意味着历史的梯度信息逐步增强，因此步数多了梯度必然爆炸（好比<spanclass="math inline">\(\lim\limits_{n\to\infty} 1.1^n \to\infty\)</span>）</li><li>也有可能有些时刻大于1，有些时刻小于1，最终稳定在1附近，但这样概率很小，需要很精巧地设计模型和参数才行。</li></ul><imgsrc="https://editor.analyticsvidhya.com/uploads/51317greatlearning.png" /><center><code>图4: 梯度爆炸和梯度消失的示意图</code></center><p><br/><br/></p><blockquote><p>提问：梯度消失就是梯度变成零吗？</p></blockquote><p>并不是，我们刚刚说梯度消失是<spanclass="math inline">\(\left|\frac{\partial h_t}{\partialh_{t-1}}\right|\)</span>一直小于1，历史梯度不断衰减，但不意味着总的梯度就为0了，具体来说，一直迭代下去，我们有</p><p><span class="math display">\[\begin{equation}\begin{aligned}\frac{d h_t}{d\theta} =&amp;\frac{\partial h_t}{\partial h_{t-1}}\frac{d h_{t-1}}{d\theta} +\frac{\partial h_t}{\partial \theta}\\=&amp; \frac{\partial h_t}{\partial \theta}+\frac{\partial h_t}{\partialh_{t-1}}\frac{\partial h_{t-1}}{\partial \theta}+\frac{\partialh_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partialh_{t-2}}\frac{\partial h_{t-2}}{\partial \theta}+\dots\\\end{aligned}\end{equation}\]</span></p><p>显然，其实只要<span class="math inline">\(\frac{\partialh_t}{\partial\theta}\)</span>不为0，那么总梯度为0的概率其实是很小的；但是一直迭代下去的话，那么<spanclass="math inline">\(\frac{\partial h_1}{\partial\theta}\)</span>这一项前面的稀疏就是<spanclass="math inline">\(t-1\)</span>项的连乘<spanclass="math inline">\(\frac{\partial h_t}{\partialh_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}}\cdots\frac{\partialh_2}{\partialh_1}\)</span>，如果它们的绝对值都小于1，那么结果就会趋于0，这样一来，<spanclass="math inline">\(\frac{dh_t}{d\theta}\)</span>几乎就没有包含最初的梯度<spanclass="math inline">\(\frac{\partial h_1}{\partial\theta}\)</span>的信息了。</p><p>这才是RNN中梯度消失的含义：<strong>距离当前时间步越长，那么其反馈的梯度信号越不显著，最后可能完全没有起作用。</strong>这就意味着RNN对长距离语义的捕捉能力失效了。说白了，优化过程都跟长距离的反馈没关系，那我们怎么保证学习出来的模型能有效捕捉长距离呢？</p><p>所以对于一般的RNN模型来说，步数多了，梯度消失或爆炸几乎都是不可避免的，我们只能通过让RNN执行有限的步数来缓解这个问题。直到上世纪末提出的LSTM极大地改进了这个问题。</p><h3 id="针对梯度消失爆炸的改进lstm">针对梯度消失/爆炸的改进：LSTM</h3><p><strong>[论文1]:</strong> <ahref="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">Hochreiter,Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neuralcomputation 9.8 (1997): 1735-1780.</a> 引入了长短期记忆 (LSTM)网络，其明确设计用于避免长期依赖问题。每个LSTM单元允许过去的信息跳过当前单元的所有处理并移动到下一个单元；这允许内存保留更长时间，并使数据能够不变地与其一起流动。LSTM由一个决定要存储哪些新信息的输入门和一个决定要删除哪些信息的遗忘门组成。</p><imgsrc="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-05.jpg" /><center><code>图5: LSTM的工作方式</code></center><p><br/><br/> 当然，LSTM 具有改进的记忆力，能够处理比 RNN更长的序列。然而，由于LSTM更加复杂，使得LSTM与RNN相比运行更慢。</p><h3id="针对上下文向量长度固定的改进attention">针对上下文向量长度固定的改进：Attention</h3><imgsrc="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-06.jpg" /><center><code>图6: 长序列输入到原有模型的例子</code></center><p><br/><br/>假设有一段长文本（输入），将其记忆下来（转换为固定长度的向量），然后在不回顾这段文本的情况下，按顺序翻译出整段文本（输出）。这很难，也不是我们的目标做法。相反，当我们翻译一句话时，我们会一部分一部分地看，逐段关注句子的某一部分，从而保证翻译的准确性，这就引入了下文所提到的Attention机制。</p><p><strong>[论文2]</strong>: <ahref="https://arxiv.org/abs/1409.0473">Dzmitry Bahdanau, Kyunghyun Cho,Yoshua Bengio: “Neural Machine Translation by Jointly Learning to Alignand Translate”, 2014; [http://arxiv.org/abs/1409.0473arXiv:1409.0473].</a>提出了一种在编码器-解码器模型中搜索与预测目标词相关的源句子部分的方法，也就是Attention机制，我们可以使用Attention机制翻译相对较长的句子而不影响其性能。例如，翻译成“noir”（在法语中意为“黑色”），注意力机制将关注单词可能的“black”，而忽略句子中的其他单词。</p><img src="https://i.imgur.com/5y6SCvU.png" /><center><code>图7: Attention机制让输出单词关注相关的输入部分</code></center><imgsrc="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-07.jpg" /><center><code>图8: Bahadanau的论文模型在测试集上长序列的 BLEU 分数明显更优</code></center><p><br/><br/>由此可见，Attention机制提高了编码器-解码器网络的性能，但速度瓶颈仍然是RNN必须逐字处理的工作机制。</p><p>很自然的，我们会接着考虑：<strong>可以用更好的模型替换掉RNN这种顺序结构的模型吗？</strong></p><p>答案是：<strong>Yes, attention is all you need!</strong></p><p>哈哈，有点一语双关的感觉（说不定GoogleBrain团队当时起名的时候就是这么想的）。在2017年，我们得到了一个令人满意的答案，一款名为Transformer的“神器”横空出世，至今风头不减当年。它也是我们这篇文章要探讨的主题。</p><h3 id="transformer诞生对标rnn">Transformer诞生，对标RNN</h3><p><strong>[论文3]</strong>: <ahref="https://arxiv.org/abs/1706.03762">Ashish Vaswani, Noam Shazeer,Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, Illia Polosukhin: “Attention Is All You Need”, 2017;[http://arxiv.org/abs/1706.03762 arXiv:1706.03762].</a>第一次正式介绍了一款在翻译领域超越了RNN的新模型Transformer，<strong>Transformer是一种Encoder-Decoder架构，使用Attention机制来处理输入和生成输出。</strong></p><blockquote><p>We propose a new simple network architecture, the<strong>Transformer</strong>, based solely on <strong>attentionmechanisms</strong>, dispensing with recurrence and convolutionsentirely.</p><p><em>Reference: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I. (2017). </em><br><em>Attention is all you need.</em></p></blockquote><p>Transformer在定义上就表明，它抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。其实更准确地讲，<strong>Transformer由且仅由SelfAttenion和Feed Forward NeuralNetwork组成</strong>。采用Attention机制是因为考虑到RNN（或者LSTM，GRU等）的计算限制是顺序处理的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：</p><ol type="1"><li><p>时间<span class="math inline">\(t\)</span>的计算依赖<spanclass="math inline">\(t-1\)</span>时刻的计算结果，这样限制了模型的并行能力。</p></li><li><p>顺序计算的过程中RNN会因为长期依赖导致信息丢失问题。</p></li></ol><p>Transformer的提出解决了上面两个问题：</p><ol type="1"><li><p>不采用类似RNN的顺序结构，而是具有并行性，符合现有的GPU框架。</p></li><li><p>使用了Attention机制，将序列中的任意两个位置之间的距离缩小为一个常量，采用了和LSTM不同的思路，从而允许处理不同长度的输入序列，并且摆脱了长期依赖的影响。</p></li></ol><p>这也就是为什么它在机器翻译任务中打败了以前基于RNN的Encoder-Decoder模型，并且在其他应用领域也非常受欢迎的原因。</p><h1 id="transformer原理">Transformer原理</h1><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg" width = "50%" /><center><code>图9: Transformer模型的整体架构</code></center><p>上图是Transformer的整体架构图，结构上看起来和Encoder-Decoder模型很相似，左边是Encoder部分，右边是Decoder部分。为了方便理解，下面把Transformer分成四个部分进行详细说明:</p><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts.jpg" width = "50%" /><center><code>图10: Transformer模型可分为4个组成部分</code></center><p>简单介绍一下各部分的任务：</p><ul><li><p><strong>Input</strong>：输入是单词的Embedding再加上位置编码，然后进入编码器或解码器。</p></li><li><p><strong>Encoder</strong>：这个结构可以循环很多次（N次），也就是说有很多层（N层）。每一层又可以分成Attention层和全连接层，再额外加了一些处理，比如SkipConnection，做跳跃连接，然后还加了Normalization层。其实它本身的模型还是很简单的。</p></li><li><p><strong>Decoder</strong>：同样可以循环N次，第一次输入是前缀信息，之后的就是上一次产出的Embedding，加入位置编码，然后进入一个可以重复很多次的模块。该模块可以分成三块来看，第一块也是Attention层，第二块是CrossAttention，不是SelfAttention，第三块是全连接层。也用了跳跃连接和Normalization。</p></li><li><p><strong>Output</strong>：最后的输出要通过Linear层（全连接层），再通过Softmax做预测。</p></li></ul><p>我们可以对Transformer的工作过程1~6进行可视化，如下所示:</p><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_process.png" width = "80%" /><center><code>图11: 图解Transformer的工作过程</code></center><p><br><br>以英-中翻译为例：假设我们输入<code>"Why do we work?"</code>，输出可以是<code>"为什么我们要工作？"</code>。那么Transformer的工作步骤是：</p><ol type="1"><li>输入自然语言序列到编码器: Why do we work?</li><li>编码器先输出隐藏层, 再输入到解码器;</li><li>输入<spanclass="math inline">\(&lt;start&gt;\)</span>(起始)符号到解码器;</li><li>得到第一个字"为";</li><li>将得到的第一个字"为"落下来再输入到解码器;</li><li>得到第二个字"什";</li><li>将得到的第二字再落下来,重复5、6步的相关动作依次生成“么”、“我”、“们”、“要”、“工”、“作”、“？”，直到解码器输出<spanclass="math inline">\(&lt;end&gt;\)</span>(终止符),则代表序列生成完成。</li></ol><h2 id="input">Input</h2><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(1).png" width = "100%" /><center><code>图12: Transformer模型的输入部分</code></center><p>我们同样以上篇采用过的英-中翻译例子，我们输入"Tom chaseJerry"，期待输出的翻译结果为"汤姆追逐杰瑞"。</p><p>Transformer的Decoder的输入处理方法与Encoder的输入处理方法步骤是相似的，但不完全一样。Encoder接受源语言数据，并通过Self-Attention和Feed-Forward网络进行编码；Decoder接受目标语言数据，并在Self-Attention、Encoder-DecoderAttention和Feed-Forward网络的基础上进行解码。在有监督训练时，Decoder的输入包括TargetEmbedding和一个Masked Multi-HeadAttention，用于生成目标语言的下一个词。输出Embedding在预测时用于生成下一个目标词。那么下面，以Encoder为例，描述输入过程和细节分析：</p><ol type="1"><li><p>首先，向Transformer输入文本 <code>"Tom chase Jerry"</code>。</p></li><li><p>随后，Transformer会将原始的英文句子"Tom chaseJerry"进行分词(tokenization)，比如得到单词序列['&lt;START&gt;', 'Tom','chase', 'Jerry','&lt;END&gt;']。请注意，分词后的token包括'&lt;START&gt;'和'&lt;END&gt;'标记。</p></li><li><p>接下来，将每个单词映射到对应的词向量上。<strong>实际上Transformer使用的是512维的向量</strong>，那么假设我们使用4维的词向量表示单词，那么对于单词'&lt;START&gt;','Tom', 'chase', 'Jerry', '&lt;END&gt;'，它们的词向量可能是：</p></li></ol><p>        <span class="math inline">\(v_{&lt;START&gt;} :[-0.1,0.2,-0.3,0.4]\)</span></p><p>        <span class="math inline">\(v_{Tom} : [0.5, 0.2, -0.1,0.3]\)</span></p><p>        <span class="math inline">\(v_{chase} : [-0.2, 0.4, 0.1,0.6]\)</span></p><p>        <span class="math inline">\(v_{Jerry} : [-0.2, 0.3, 0.1,-0.5]\)</span></p><p>        <span class="math inline">\(v_{&lt;END&gt;} :[0.3,-0.2,0.1,-0.4]\)</span></p><ol start="4" type="1"><li>通过使用sin和cos函数来生成位置向量，这种方式可以向模型描述各个单词之间的顺序关系，并且能够在维度空间上均匀分布位置向量。可以假设'<START>'的位置编号为1，"Tom"的位置编号为2，"chase"的位置编号为3，"Jerry"的位置编号为4，"<END>"的位置编号为5。那么它们位置向量可能是：</li></ol><p>        <span class="math inline">\(p_1 =[sin(\frac{1}{10000^{2*\frac{1}{4}}}),cos(\frac{1}{10000^{2*\frac{1}{4}}}),sin(\frac{2}{10000^{2*\frac{1}{4}}}),cos(\frac{2}{10000^{2*\frac{1}{4}}}) ]\)</span></p><p>        <span class="math inline">\(p_2 =[sin(\frac{2}{10000^{2*\frac{1}{4}}}),cos(\frac{2}{10000^{2*\frac{1}{4}}}),sin(\frac{3}{10000^{2*\frac{1}{4}}}),cos(\frac{3}{10000^{2*\frac{1}{4}}}) ]\)</span></p><p>        <span class="math inline">\(p_3 =[sin(\frac{3}{10000^{2*\frac{1}{4}}}),cos(\frac{3}{10000^{2*\frac{1}{4}}}),sin(\frac{4}{10000^{2*\frac{1}{4}}}),cos(\frac{4}{10000^{2*\frac{1}{4}}}) ]\)</span></p><p>        <span class="math inline">\(p_4 =[sin(\frac{4}{10000^{2*\frac{1}{4}}}),cos(\frac{4}{10000^{2*\frac{1}{4}}}),sin(\frac{5}{10000^{2*\frac{1}{4}}}),cos(\frac{5}{10000^{2*\frac{1}{4}}}) ]\)</span></p><p>        <span class="math inline">\(p_5 =[sin(\frac{5}{10000^{2*\frac{1}{4}}}),cos(\frac{5}{10000^{2*\frac{1}{4}}}),sin(\frac{6}{10000^{2*\frac{1}{4}}}),cos(\frac{6}{10000^{2*\frac{1}{4}}}) ]\)</span></p><ol start="5" type="1"><li>最后，输入到Transformer中的序列就是由词向量和位置向量相加得到的，例如，“Tomchase Jerry”的输入序列可能是:</li></ol><p>        [<span class="math inline">\(v_ {&lt;START&gt;}\)</span> +<span class="math inline">\(p_1\)</span> , <spanclass="math inline">\(v_{Tom}\)</span> + <spanclass="math inline">\(p_2\)</span>, <spanclass="math inline">\(v_{chase}\)</span> + <spanclass="math inline">\(p_3\)</span>, <spanclass="math inline">\(v_{Jerry}\)</span> + <spanclass="math inline">\(p_4\)</span>, <span class="math inline">\(v_{&lt;END&gt;}\)</span> + <span class="math inline">\(p_5\)</span>]</p><p>下面分别介绍过程中相关的知识点：</p><h3 id="分词tokenization">分词(Tokenization)</h3><blockquote><p>什么是分词</p></blockquote><p>Transformer模型的输入通常是序列数据，如文本、语音等。这些数据在输入之前需要进行预处理，其中一个重要的步骤就是分词。<strong>分词是获取词向量之前的一个必要步骤。</strong></p><p>分词是NLP的一个重要概念，<strong>表示将文本(text)切分成符号(token)的过程。</strong>token可以是以下三种类型：</p><ol type="1"><li><p>单词 (word) —— 例如，短语“dogs likecats”由三个词标记组成：“dogs”、“like”和“cats”。</p></li><li><p>字符 (character) —— 例如，短语“yourfish”由九个字符标记组成。<strong>（请注意，空格算作标记之一）</strong></p></li><li><p>子词 (subword) ——其中单个词可以是单个标记或多个标记。子词由词根、前缀或后缀组成。例如，使用子词作为标记的语言模型可能会将单词“dogs”视为两个标记（词根“dog”和复数后缀“s”）。相同的语言模型可能会将单个词“更高”视为两个子词（词根“high”和后缀“er”）。</p></li></ol><blockquote><p>分词的作用？</p></blockquote><p>通过分词，模型可以将文本分成若干个单独的词汇单元，可以减少翻译系统需要处理的信息量，提高翻译效率和准确性，同时更好地维护语言的语法结构。</p><h3 id="嵌入embedding">嵌入(Embedding)</h3><p>NLP中，<strong>使用分词后的词向量作为模型输入</strong>是常见的做法，而词向量是一种特定类型的嵌入。</p><blockquote><p>什么是嵌入？</p></blockquote><p>嵌入是NLP中使用的一种技术，以机器学习模型能够理解的数字格式表示单词、短语甚至整个句子。</p><blockquote><p>嵌入的作用？</p></blockquote><p>嵌入的目标是<strong>通过机器学习模型能够理解的方式捕捉词的含义和上下文，并将词语的语义信息转化为数字，使得它们可以被计算机理解。</strong>从另一种角度来看，嵌入也可以被视为一种降维形式，将高维的单词表示转化为低维的词向量。</p><p>举个例子，此前对于单词或句子，很容易想到类似 one-hot的编码向量表示方法，一个单词或句子在向量中仅被表示为一个非零元素，其他元素都是零，从而将单词或句子映射为高维和稀疏的向量。</p><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/onehot.png" width = "80%" /><center><code>图13: 图解one hot的编码方式</code></center><p><br/><br/>这种表示方法确实可以避免线性不可分的问题，但由于大多数元素为零，因此计算效率会低下。此外，使用one-hot编码表示的词向量不能很好地表示单词之间的语义关系，因此机器学习模型难以学习到单词的含义。因此，嵌入技术是更有效的选择。</p><p>与其相反，嵌入是一种以较低维度的密集格式表示单词或句子的方法，更适合机器学习模型。<strong>通过将单词或句子映射到一个较低的维度空间，嵌入可以捕捉到单词或句子的含义和相互关系，同时舍弃不太重要的信息。</strong></p><p><strong>常见的词向量编码方式是word2vec。</strong>相比于one-hot编码只允许我们将单词作为单个不同的条目来解释，word2vec允许我们寻找每个单词和其他单词的关系，从而创建更好的特征表示。</p><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/word2.png" width = "80%" /><center><code>图14: 图解word2vec的编码方式</code></center><p><br/><br/>word2vec模型使用神经网络来学习一个词的向量表示。它将每个词映射到一个高维向量，其中语义相似的词在向量空间中是紧密相连的。例如，在word2vec模型中，"banana"这个词的向量表示可能是[-0.2,0.5, 0.1, 0.3,...]，代表这个词的含义和上下文，(一个词向量往往是300~1000维，向量中的每个元素代表这个词的意义或上下文的一个维度，<strong>嵌入向量的维数通常比 one-hot 编码向量的维数低得多</strong>)。单个数字的含义本身不可解释，只有在与其他词或句子的向量相关时才有意义</p><blockquote><p>输入的embedding是否需要经过训练</p></blockquote><p>将单词<spanclass="math inline">\(x\)</span>的embedding输入encoder，有两种常见的选择：</p><ol type="1"><li><p>使用Pre-trained的<strong>embeddings并固化</strong>，这种情况下embedding取自一个预先训练好的模型，在训练过程中不更新。实际就是一个LookupTable（查找表）。<strong>这是bert（一种特殊的Transformer模型，专门用于自然语言处理任务）选择的做法。</strong></p></li><li><p>对其进行随机初始化（当然也可以选择Pre-trained的结果），但<strong>设为Trainable</strong>。这样在training过程中不断地对embeddings进行改进。即End2End（端到端）训练方法，意味着模型从头到尾都被训练，所有的参数，包括嵌入都在训练过程中被更新。<strong>这也是Transformer选择的做法。</strong></p></li><li><p>有些情况下，在Encoder的输入层之前还会使用一个词汇表（vocabulary），并且对于OOV（out-of-vocabulary）的单词使用一个特殊的embedding，例如UNK（unknown）或PAD（padding）。</p></li></ol><h3 id="位置编码positional-encoding">位置编码(Positional Encoding)</h3><blockquote><p>为什么需要知道每个单词的位置，并且添加位置编码呢？</p></blockquote><p>首先，咱们知道，一句话中同一个词如果的出现位置不同，意思可能发生翻天覆地的变化，就比如：我欠他100W和他欠我100W。这两句话的意思一个地狱一个天堂。可见获取词语出现在句子中的位置信息是一件很重要的事情。</p><p>而Transformer没有用RNN也没有卷积，它使用的注意力机制(主要是由于selfattention)，不能获取词语位置信息，就算打乱一句话中词语的位置，每个词还是能与其他词之间计算attention值。所以为了让模型能利用序列的顺序，必须输入序列中词的位置，所以Transformer采用的方法是给每一个词向量，包括包括'&lt;START&gt;'和'&lt;END&gt;'都需要添加位置编码。</p><blockquote><p>怎么得到positional encoding呢？</p></blockquote><p>Transformer使用的是正余弦位置编码。位置编码通过使用不同频率的正弦、余弦函数生成，然后和对应位置的词向量相加，位置向量维度必须和词向量的维度一致。过程如上图，PE（positionalencoding）计算公式如下：</p><p><span class="math display">\[PE_{(pos,2i)} =sin(\frac{pos}{10000^{\frac{2 i}{d_{model}}}})\]</span> <spanclass="math display">\[PE_{(pos,2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\]</span></p><p>解释一下上面的公式：</p><ul><li><p><spanclass="math inline">\(pos\)</span>表示单词在句子中的绝对位置，<spanclass="math inline">\(pos=0, 1, 2, \dots\)</span>，例如：Jerry在"Tomchase Jerry"中的pos=2；</p></li><li><p><spanclass="math inline">\(d_{model}\)</span>表示词向量的维度，一般<spanclass="math inline">\(d_{model}\)</span>=512；2i和2i+1表示奇偶性，i表示词向量中的第几维，例如这里<spanclass="math inline">\(d_{model}\)</span>=512，故<spanclass="math inline">\(i=0, 1, 2, \dots, 255\)</span>。</p></li></ul><p>至于上面两个公式是怎么得来的，其实不重要，很有可能是作者根据经验自己造的，而且公式也不唯一，后续Google在Bert中的采用类似词向量的方法通过训练PE，说明<strong>这种求位置向量的方法还是存在一定问题滴</strong>。</p><blockquote><p>为什么是将positional encoding与词向量相加，而不是拼接呢？</p></blockquote><p>事实上，拼接或者相加都可以，只是词向量本身的维度（512维）就已经蛮大了。再拼接一个512维的位置向量（变成1024维）这样训练起来会相对慢一些，影响学习效率。两者既然效果差不多，那当然是选择学习难度较小的相加了。</p><p>这段代码实现了Transformer模型中的位置编码，主要用于确定输入序列中每个单词的位置信息问题。</p><ul><li><strong>d_model</strong>: 定义词向量的维度。</li><li><strong>dropout</strong>:一种正则化方式，随机让部分网络参数为0，以防过拟合。</li><li><strong>max_len</strong>: 输入句子的最大长度。</li></ul><p>在初始化中，首先使用nn.Dropout类创建了一个dropout层。然后根据论文中的公式，预先计算出位置编码，并将其存储在pe变量中。</p><p>在forward函数中，将输入x加上pe变量中对应位置的位置编码，最后进行dropout操作（一种正则化方式，在训练时会随机将部分网络参数设置为0，从而防止过拟合）。</p><p>需要注意的是，这里的<strong>位置编码是在计算时预先计算好了并存储下来，而不是在运行时动态计算，这样可以减少计算量</strong>。</p><h2 id="encoder">Encoder</h2><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(2).png" width = "40%" /><center><code>图15: Transformer模型的编码器部分</code></center><p><br/><br/>在论文中，有6层编码器，即<code>“Nx”的N=6</code>。Transformer的编码器的每一层(Encoderlayer)是由4个sub-layer（子层）组成的：</p><p>第一个子层是多头注意力机制 (multi-head self-attentionmechanism)，它通过计算输入序列中各个位置的关系，生成关于该位置的输入的新的表示。第二个子层是残差连接 (residualconnection)，它将第一个子层的输出与原始输入相加，并进行归一化以维护其统计特性。第三个子层是全连接前馈层（feed-forwardlayer），它通过一个多层神经网络对第二个子层的输出进行非线性变换，从而生成新的表示。第四个子层是残差连接 (residualconnection)，它将第三个子层的输出与原始输入相加，并进行归一化以维护其统计特性。</p><p>这段代码实现了一个transformer编码器中的一个编码层EncoderLayer。这个编码层由两部分组成，分别是自注意力机制self-attn和前馈网络feed_forward。在初始化时，通过传入size、self_attn、feed_forward和dropout参数来初始化编码层。</p><p>在前向传播过程中，首先使用自注意力机制对输入x进行处理，然后将处理后的结果经过前馈网络进一步处理，最后返回处理结果。</p><p>具体来说，使用 <span class="math inline">\(sublayer[0](x, lambda x:self.self_attn(x, x, x, mask))\)</span>调用SublayerConnection类对输入进行处理，进行自注意力机制。 之后使用<span class="math inline">\(sublayer[1](x, self.feed_forward)\)</span>将结果经过前馈网络进一步处理。</p><h3 id="multi-head-attention">Multi-Head Attention</h3><p><strong>Multi-Head Attention是SelfAttention机制的进一步细化，因此先从Self Attention讲起：</strong></p><h4 id="从self-attention讲起">从Self Attention讲起</h4><p>假设下面的句子是我们要翻译的输入句子：</p><p><code>The animal didn't cross the street because it was too tired</code></p><p>这句话中的“it”指的是什么？指的是街道还是动物？这对人来说是一个简单的问题，但对算法模型来说却不那么简单。</p><imgsrc="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png" /><center><code>图16: 对单词“it”编码时Attention大部分集中在“The animal”上</code></center><p>在该例子中，当模型处理“it”这个词时，self attention允许它把“it”和“animal”联系起来。而广泛地说，<strong>当模型处理每个单词（输入序列中的每个位置）时，自注意力允许它查看输入序列中的其他位置以寻找有助于更好地编码该单词的线索。</strong></p><p>而上篇我们详细讲解过，Attention的本质，这里我们简单描述下：<strong>Attention实际上做的就是数据库中的检索操作，本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。</strong></p><p>这个类似于搜索引擎或者推荐系统的基本原理，以谷歌Google为例:</p><ul><li>用户给定需查询的问题(Query)</li><li>Google后台有各种文章标题(Key)和文章本身(Value)</li></ul><p>通过定义并计算问题Query与文章标题Key的相关性，用户就可以找到最匹配的文章Value。当然Attention的目标不是通过Query得到Value，而是通过Query得到Value的加权和。</p><p>那么回到计算Self Attention的过程上来，这次我们以新的输入“ThinkingMachines”为例进行过程描述：</p><p><strong>单词级别-第一步：</strong></p><p>从每个编码器的输入向量（词向量+位置编码）创建三个向量：一个Query查询向量、一个Key键向量和一个Value值向量。</p><imgsrc="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png" /><br /><center><code>图17: qkv向量是通过将embedding分别乘以训练的三类权重矩阵而创建的</code></center><p><br/><br/> 比如产生"Thinking"的三个向量的过程如下：</p><ol type="1"><li>将<span class="math inline">\(X_1\)</span>乘以<spanclass="math inline">\(W^Q\)</span>权重矩阵产生<spanclass="math inline">\(q_1\)</span>，即与该词关联的Query向量。</li><li>将<span class="math inline">\(X_1\)</span>乘以<spanclass="math inline">\(W^K\)</span>权重矩阵产生<spanclass="math inline">\(k_1\)</span>，即与该词关联的Key向量。</li><li>将<span class="math inline">\(X_1\)</span>乘以<spanclass="math inline">\(W^V\)</span>权重矩阵产生<spanclass="math inline">\(v_1\)</span>，即与该词关联的Value向量。</li></ol><p>请注意，qkv向量的维度小于embedding向量。它们的维数是64，而embedding和编码器输入/输出向量的维数是512。它们不必更小，这是一种使multi-headattention（大部分）计算保持不变的架构选择。</p><p><strong>单词级别-第二步</strong></p><p>计算每个单词的分数，分数是通过Query向量与当前正在评分单词的Key向量的点积计算得出的。<strong>当我们在特定位置对单词进行编码时，分数决定了将多少注意力放在输入句子的其他部分。</strong></p><p>假设我们正在计算本例中第一个词“Thinking”的自注意力。我们需要根据这个词对输入句子的每个词进行评分:</p><imgsrc="https://jalammar.github.io/images/t/transformer_self_attention_score.png" /><br /><center><code>图18: 单词的分数是对应位置Query向量和Key向量的点积</code></center><p><strong>单词级别-第三步</strong></p><p>将分数除以8，这个数字是论文中使用的关键向量维度64的平方根。</p><blockquote><p>上式为什么要除以<spanclass="math inline">\(\sqrt{d_k}\)</span>呢？</p></blockquote><p>好问题！如果不进行除以<spanclass="math inline">\(\sqrt{d_k}\)</span>的操作，那么<spanclass="math inline">\(QK^T\)</span>的值将取决于<spanclass="math inline">\(d_k\)</span>的大小。当<spanclass="math inline">\(d_k\)</span>较大时，<spanclass="math inline">\(QK^T\)</span>中的元素将变得相对较大，可能会导致softmax函数的输出非常小，导致梯度消失的问题。当<spanclass="math inline">\(d_k\)</span>较小时，<spanclass="math inline">\(QK^T\)</span>中的元素将变得相对较小，可能会导致softmax函数的输出非常大，导致梯度爆炸的问题。为了避免这种情况，Transformer中对<spanclass="math inline">\(QK^T\)</span>的每个元素都除以<spanclass="math inline">\(\sqrt{d_k}\)</span>，以使得在不同<spanclass="math inline">\(d_k\)</span>下，<spanclass="math inline">\(QK^T\)</span>的元素大小分布更加平稳，从而更稳定地进行softmax函数的计算和反向传播。因此，除以<spanclass="math inline">\(\sqrt{d_k}\)</span>是一种既简单又有效的方法来缩放注意力分数，这也是为什么Transformer中会采用这种方式。<em>--By ChatGPT</em></p><blockquote><p>为什么是<span class="math inline">\(\sqrt{d_k}\)</span>而不是<spanclass="math inline">\(d_k\)</span>呢？</p></blockquote><p>在Transformer中，除以<spanclass="math inline">\(\sqrt{d_k}\)</span>是为了缩小内积的值的范围，避免softmax函数的梯度消失和梯度爆炸问题。而为什么不使用<spanclass="math inline">\(d_k\)</span>作为缩放系数呢？这是因为<spanclass="math inline">\(d_k\)</span>的值通常会非常大（比如说，如果使用了512维的向量，<spanclass="math inline">\(d_k\)</span>就为512），这样就会导致内积的值也变得非常大，从而使得softmax函数的梯度非常小，导致梯度消失的问题。此外，除以<spanclass="math inline">\(\sqrt{d_k}\)</span>可以使得每个维度对注意力分数的影响相等，这是因为所有维度上的内积被相同的因子<spanclass="math inline">\(\sqrt{d_k}\)</span>缩放。这种均衡性有助于确保注意力机制可以平衡地关注每个维度上的信息。因此，使用<spanclass="math inline">\(\sqrt{d_k}\)</span>作为缩放系数是一种更好的选择。<em>--ByChatGPT</em></p><p><strong>单词级别-第四步</strong></p><p>然后通过 softmax 操作传递结果。softmax对分数进行归一化处理，使它们都为正且加起来为 1。</p><p>softmax分数决定了在这个位置上，输入句子的每个单词会被投入的注意力占比。</p><imgsrc="https://jalammar.github.io/images/t/self-attention_softmax.png" /><br /><center><code>图19: 缩小维度并进行归一化处理。</code></center><p><strong>单词级别-第五步</strong></p><p>将每个Value向量乘以 softmax分数（准备将它们相加）。这里的直觉是保持我们想要关注的单词的值不变，并淹没不相关的单词（例如，通过将它们乘以像0.001这样的小数字）。然后是对加权值向量求和，这会在该位置产生自注意层的输出。</p><imgsrc="https://jalammar.github.io/images/t/self-attention-output.png" /><br /><center><code>图20: 输出第一个单词的self attention计算结果</code></center><p><br/><br/>自注意力计算到此结束。生成的向量是我们可以发送到前馈神经网络的向量。然而，在实际实现中，此计算以矩阵形式完成。</p><blockquote><p>为什么实际要用矩阵而不是神经网络呢？</p></blockquote><p>因为矩阵运算能用GPU加速，会更快，同时参数量更少，更节省空间。</p><p>既然我们已经看到了单词级别的计算过程，那么让我们来看看SelfAttention实际使用的矩阵计算：</p><p><strong>矩阵计算-第一步</strong></p><p>计算Query,Key和Value共计三个矩阵。为此，我们将嵌入打包到矩阵X中，然后将其乘以我们训练过的权重矩阵<span class="math inline">\((W^Q、W^K、W^V)\)</span>。</p><imgsrc="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png" /><center><code>图21: 计算QKV矩阵</code></center><p><strong>矩阵计算-第二步</strong></p><p>由于我们处理的是矩阵，我们可以将单词形式的第二步到第五步压缩为一个公式来计算自注意力层的输出。</p><imgsrc="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" /><br /><center><code>图22: 矩阵形式的self attention计算公式</code></center><h4 id="multi-head-attention原理">Multi-head Attention原理</h4><p>该论文通过添加一种称为Multi-headAttention的机制，进一步细化了自注意力层。主要体现在两个方面：</p><ol type="1"><li><p>它扩展了模型关注不同位置的能力。比如要翻译像“The animal didn'tcross the street because it was tootired”这样的句子，知道“it”和哪几个词有关会很有用。</p></li><li><p>它为注意力层提供了多个representation subspaces(表示子空间)。正如我们接下来将看到的，对于Multi-headAttention，我们有多组QKV权重矩阵，其中的每一个都是随机初始化的。训练之后，每个集合用于将输入（初始输入或来自较低编码器/解码器的向量）投影到不同的表示子空间中。</p></li></ol><imgsrc="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png" /><br /><center><code>图23: Multi-head Attention每个头各产生不同的 QKV 矩阵</code></center><p><br/><br/></p><imgsrc="https://jalammar.github.io/images/t/transformer_attention_heads_z.png" /><br /><center><code>图24: 通过相同的计算过程，8个attention head最终会得到8个不同的Z矩阵</code></center><p><br/><br/></p><p>这给我们带来了一些挑战。前馈层不需要8个矩阵——它需要一个矩阵（每个单词一个向量）。所以我们需要一种方法将这8个压缩成一个矩阵。因此我们连接这些矩阵，然后将它们乘以一个额外的权重矩阵<span class="math inline">\(W^O\)</span>。</p><imgsrc="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png" /><br /><center><code>图25: 将多个Z矩阵通过矩阵乘法合并成总的Z矩阵</code></center><br/><br/> <imgsrc="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" /><center><code>图26: 将每个头的输出拼接在一起并通过一个Linear层，映射成类似单头的输出</code></center><p><br/><br/> 现在我们已经谈到了attentionhead，让我们重新审视我们之前的例子，看看当我们在示例句子中对单词“it”进行编码时，不同的attentionhead集中在什么地方：</p><imgsrc="https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png" /><br /><center><code>图27: 不同attention head对it的注意力权重不同</code></center><p><br/><br/> 当我们对“it”这个词进行编码时，一个attention head最关注“theanimal”，而另一个attention head则关注“tired”。这说明不同attentionhead很可能从不同角度来理解it和其他单词的关系，比如it的指代的对象是“theanimal”，而这个对象所处的状态是“tired”。因为Attention是注意力的意思而不是表示相等的意思，那么从不同角度看待同一个事物，得到不同的答案自然也是没问题的。</p><h3 id="add-normalize">Add &amp; Normalize</h3><p>可以注意到编码器/解码器的每个子层（比如self attention,ffnn）之后都带有一个 <strong>Add &amp; Normalize</strong>。</p><imgsrc="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png" /><br /><center><code>图28: 每个编码器中都有2个Add &amp; Normalize子层</code></center><p><br/><br/></p><p>我之前没有听说过残差连接，因此看着这张图好久也没看出residual这个词体现在哪里，问了“Chat老师”才明白：</p><ul><li><p>Add表示残差连接，残差连接来源于<strong>[论文4]</strong>: <ahref="https://arxiv.org/abs/1512.03385">Kaiming He, Xiangyu Zhang,Shaoqing Ren, Jian Sun: “Deep Residual Learning for Image Recognition”,2015; [http://arxiv.org/abs/1512.03385 arXiv:1512.03385].</a></p></li><li><p>Norm表示LayerNorm，LayerNorm来源于<strong>[论文5]</strong>: <ahref="https://arxiv.org/abs/1607.06450">Jimmy Lei Ba, Jamie Ryan Kiros,Geoffrey E. Hinton: “Layer Normalization”, 2016;[http://arxiv.org/abs/1607.06450 arXiv:1607.06450].</a>。</p></li></ul><p>Encoder端和Decoder端每个子模块实际的输出为：<spanclass="math inline">\(LayerNorm(x+Sublayer(x))\)</span>，其中<spanclass="math inline">\({Sublayer}(x)\)</span>为子模块的输出。这样做有助于模型更好地捕捉长期依赖关系。</p><p>关于这一部分的更多技术细节我以问答的形式展示在下面。</p><p><strong>问题一</strong></p><blockquote><p>什么是残差连接</p></blockquote><p>残差连接是一种网络设计方法，它的作用是在每一层的输入和输出之间添加一个跨层连接（shortcutconnection），并通过标准化来处理连接处的数据。在Transformer中，每个Encoder层都由一个残差连接和两个正规化步骤组成。</p><p>残差连接的公式是 y=LayerNorm(x+Sublayer(x))</p><ul><li>Sublayer(x)可以是任意的层，比如Multi-HeadAttention或Feed-Forward层。</li><li>LayerNorm是对每个样本的所有隐藏单元进行归一化，以防止过拟合和提高模型的鲁棒性。</li><li>其实也就是Add &amp; Normolize</li></ul><p>残差连接的好处是能够有效地减轻梯度消失问题，简化网络训练过程，并帮助网络更好地捕捉长期依赖关系。</p><p><strong>问题二</strong></p><blockquote><p>为什么引入残差连接</p></blockquote><imgsrc="https://img-blog.csdnimg.cn/94d2b8dbc1ea432c9ca85f62e29cb454.png#pic_center" /><center><code>图29:  Residual learning：a building block</code></center><ul><li>X是这一层残差块的输入值</li><li>F(X)是经过第一层线性变化并激活后的输出，也称为残差</li><li>在第二层输出值激活前加入X，这条路径称作跨层连接（shortcutconnection）。</li></ul><p>《Deep Residual Learning for Image Recognition》论文中的Figure2展示了残差学习的建筑块。这个建筑块由两个卷积层组成，每个卷积层后面跟着一个ReLU激活函数。在这个建筑块的最后，有一条跨越了两个卷积层的连接，它绕过了这两个卷积层，直接将输入信息传递到输出端。这个跨层连接被称为“残差连接”。</p><p>引入残差连接的目的就是为了防止<strong>在深度神经网络训练中发生退化问题</strong>，使得更深的网络能够更好地学习特征表达。</p><p><strong>问题三</strong></p><blockquote><p>什么是退化，为什么深度神经网络会发生退化？</p></blockquote><p>退化问题通常指的是：当神经网络层数增加时，网络的性能开始变差，表现为训练误差的增加和泛化误差的增加。一种可能的原因是，在网络较深的时候，由于神经网络的非线性层的存在，梯度传播变得困难，导致网络的训练变得困难。</p><p>而残差连接通过引入跨层连接，可以使梯度在网络中更容易地流动，从而减轻梯度消失的问题，进一步提高了网络的性能。</p><p>举个例子：假如某个神经网络的最优网络层数是18层，但是我们在设计的时候并不知道到底多少层是最优解，本着层数越深越好的理念，我们设计了32层，那么32层神经网络中有14层其实是多余的，我们要想达到18层神经网络的最优效果，必须保证这多出来的14层网络必须进行恒等映射，恒等映射的意思就是说，输入什么，输出就是什么，可以理解成F(x)=x这样的函数，因为只有进行了这样的恒等映射咱们才能保证这多出来的14层神经网络不会影响我们最优的效果。</p><p>实际上，在训练过程中，网络的参数并不是直接训练得到的。相反，网络是通过反向传播算法来更新网络的参数，因此参数的更新是从后向前进行的，要想保证训练参数能够很精确的完成F(x)=x的恒等映射其实是很困难的。这个时候大神们就提出了ResNet（残差神经网络）来解决神经网络退化的问题。</p><p><strong>问题四</strong></p><blockquote><p>为什么添加了残差块能防止神经网络退化问题呢？</p></blockquote><p>咱们再来看看添加了残差块后，咱们之前说的要完成恒等映射的函数变成什么样子了。是不是就变成h(X)=F(X)+X，我们要让h(X)=X，那么是不是相当于只需要让F(X)=0就可以了，这里就巧妙了！神经网络通过训练变成0是比变成X容易很多的，因为大家都知道，咱们一般初始化神经网络的参数的时候就是设置的[0,1]之间的随机数嘛。所以经过网络变换后很容易接近于0。举个例子：</p><p><imgsrc="https://img-blog.csdnimg.cn/20200326001443472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /></p><p>假设该网络只经过线性变换，没有bias也没有激活函数。我们发现因为随机初始化权重一般偏向于0，那么经过该网络的输出值为[0.60.6]，很明显会更接近与[0 0]，而不是[21]，相比与学习h(x)=x，模型要更快到学习F(x)=0。</p><p>并且ReLU能够将负数激活为0，过滤了负数的线性变化，也能够更快的使得F(x)=0。用学习残差F(x)=0更新该冗余层的参数来代替学习h(x)=x更新冗余层的参数。通过学习残差F(x)=0来让该层网络恒等映射上一层的输入，使得有了这些冗余层的网络效果与没有多余层的浅层网络相同。这样很大程度上解决了网络的退化问题。</p><p><strong>问题五</strong></p><blockquote><p>为什么要进行Normalize呢？</p></blockquote><p>在神经网络进行训练之前，都需要对于输入数据进行Normalize归一化，目的有二：</p><ol type="1"><li><p>能够加快训练的速度。归一化能够加快训练速度的原因是可以使得数据在网络中传递时，每一层的输出值分布更加均匀，避免了在激活函数饱和区域的情况下，网络梯度消失或爆炸，从而保证了训练的速度和效果。</p></li><li><p>提高训练的稳定性。归一化能够提高训练的稳定性的原因是使得数据在网络中传递时，每一层的输出值都保持在一个相对小的范围内，避免了出现数据的偏移和方差过大的情况，从而减小了模型对于数据的敏感度，使得模型更加稳定。</p></li></ol><p>在Transformer中，归一化层通常在每个多头自注意力和前馈网络之间进行，这些归一化层被称为"LayerNormalization"。它们在一个batch内的每个样本中对每个特征维度进行独立归一化，并且使用样本的均值和方差来标准化每个特征。</p><p><strong>问题六</strong></p><blockquote><p>为什么使用Layer Normalization（LN）而不使用BatchNormalization（BN）呢？</p></blockquote><p>先看图，LN是在同一个样本中不同神经元之间进行归一化，而BN是在同一个batch中不同样本之间的同一位置的神经元之间进行归一化。</p><imgsrc="https://img-blog.csdnimg.cn/20200326202939489.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /><br /><center><code>图30: Layer Normalization和Batch Normalization可视化</code></center><p>可以看出，相比于图像领域，NLP任务中的输入是文本数据，一般使用词向量表示，每个词向量是一个高维稠密向量。在NLP任务中，同一个batch中的不同样本可能具有相似的词向量，但每个样本的长度（即词向量的数量）不同。因此，在NLP任务中，每个词向量的不同维度可能具有不同的意义，将每个维度视为不同的特征，并对不同的特征进行归一化处理更加合理。因此，LayerNormalization更适合NLP任务中的使用。而BatchNormalization更适用于图像领域的任务，其中输入是具有相同空间维度的图像。</p><p>因此，总结原因如下：</p><ol type="1"><li><p>LN更适用于序列数据：在NLP任务中，输入通常是一系列标记的序列，例如单词或字符。这种序列数据通常具有可变的长度和不同的分布。与固定大小的图像批次相比，序列的不同位置可能具有截然不同的统计属性，因此使用BN将导致不同位置的特征之间出现耦合。相比之下，LN可以更好地适应序列数据的变化。</p></li><li><p>LN避免了对小批次大小的依赖：在BN中，对于每个小批次，特征的均值和方差是在该小批次上计算的。对于小批次，可能会出现均值和方差计算上的不准确性，从而导致性能下降。相比之下，LN不涉及小批次的计算，而是仅使用样本的特征，这使得它对小批次大小的依赖更小。</p></li><li><p>LN更适用于深度网络：随着神经网络的加深，BN计算的均值和方差将越来越不可靠，这会导致性能下降。而LN则不会受到这个问题的影响，因为它在每个特征上进行归一化，而不是在整个批次上计算。</p></li><li><p>LN可以更好地适应动态计算图：在深度学习中，一些计算图是动态的，其中图的结构在运行时可以更改。BN的计算依赖于图的结构，因此在这些情况下可能会遇到困难。相比之下，LN只依赖于每个特征的值，因此在动态计算图的环境下更容易使用。</p></li></ol>因此，我们可以可视化Add &amp; Normalize操作，如下图所示： <imgsrc="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png" /><br /><center><code>图28: 可视化Add &amp; Normalize操作</code></center><h3 id="feed-forward">Feed Forward</h3><p>每一层经过self attention之后，还会有一个Feed ForwardNetwork(FFN)，这个FFN的作用就是空间变换。FFN包含了2层lineartransformation层，中间的激活函数是ReLu。</p><p><span class="math display">\[FFN(x) = \max(0, xW_1 + b_1 )W_2 +b_2\]</span></p><blockquote><p>对于每个多头自注意层的输出，最后都会和 <spanclass="math inline">\(W_O\)</span>相乘拼接成一个Z矩阵，为什么要在每个多头自注意层的输出后再增加一个2层的FFN网络？</p></blockquote><p>这个全连接层的作用是对每个多头自注意力层的输出进行非线性转换(ReLu激活函数)，以更好地表达输入序列中的信息。最终，全连接层的输出被与另一个权重矩阵<span class="math inline">\(\mathbf{W}_{O}\)</span>相乘，以得到最终的输出。</p><p>因此，全连接层不是必须的，但它可以帮助模型更好地表达输入序列中的信息。特别地，它可以捕捉一些局部特征，从而提高模型的性能。</p><h2 id="decoder">Decoder</h2><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(3).png" width = "50%" /><center><code>图30: Transformer模型的解码器部分</code></center><p>论文中Decoder也是N=6层堆叠的结构。被分为3个sub-layer，Encoder与Decoder有<strong>三大主要的不同</strong>：</p><ol type="1"><li><p>Decoder sub-layer-1使用的是“<strong>Masked</strong>” Multi-HeadedAttention机制，<strong>防止为了模型看到要预测的数据，防止泄露</strong>。</p></li><li><p>sub-layer-2是一个Encoder-Decoder Multi-head Attention。</p></li><li><p>LinearLayer和SoftmaxLayer作用于sub-layer-3的输出后面，来预测对应的word的概率。</p></li></ol><p>如果你弄懂了Encoder部分，Decoder部分也就没有那么可怕了：</p><ul><li>输入都是 embedding + positional Encoding。</li><li>Decoder也是N=6层堆叠的结构。被分为3个sub-layer，具体细节方面：<ol type="1"><li>masked multi-headattention：由于在机器翻译中，Decode的过程是一个顺序的过程，也就是当解码第k个位置时，我们只能看到第k- 1及其之前的解码结果，因此<strong>加了mask，防止模型看到要预测的数据。这点和Encoder不同</strong></li><li>Encoder-Decoder Multi-HeadAttention：和Encoder的类似，每一层Decoder都会接受Encoder最后一层输出作为key和value，而当前解码器输出作为query。然后计算输入序列和目标序列中每个位置之间的相似度，最后将所有头的结果拼接在一起得到最终的输出。</li><li>FeedForward：和Encoder一样</li></ol></li><li>最后都连接了LinearLayer和SoftmaxLayer</li></ul><imgsrc="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKtDCvJ7TGjfZPuP%2Fencoder-decoder-2.jpg?generation=1583677008527428&amp;alt=media" /><center><code>图31: Transformer每一层Decoder都会分别接受Encoder最后一层输出</code></center><p>由此可见，<strong>只有masked multi-headattention需要详细讲解，其余的在encoder处都已经掌握了。</strong></p><h3 id="masked-multi-head-attention">Masked Multi-Head-Attention</h3><p>MaskedMulti-Head-Attention则是在传统的多头注意力层的基础上，在计算过程中添加了一个遮挡（mask）机制。这个遮挡机制可以避免解码器在生成目标序列时看到未来的信息。</p><p>具体来说，在计算解码器在当前位置的输出值时，如果该位置对应的输入位置在目标序列中出现的位置在当前位置之后，那么这个输入位置就会被遮挡，不会被用来计算输出值。这样做能够避免解码器在生成目标序列时看到未来的信息，提高模型的效果。</p><p><strong>问题一</strong></p><blockquote><p>什么是mask</p></blockquote><p>mask表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer模型里面涉及两种mask，分别是padding mask和sequence mask。 其中，padding mask在所有的scaleddot-product attention 里面都需要用到，而sequencemask只有在Decoder的Self-Attention里面用到。</p><p><strong>问题二</strong></p><blockquote><p>什么是padding mask？</p></blockquote><p>因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的Attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。</p><p>具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过softmax，这些位置的概率就会接近0！而我们的padding mask实际上是一个张量，每个值都是一个Boolean，值为false的地方就是我们要进行处理的地方。</p><p><strong>问题三</strong></p><blockquote><p>什么是sequence mask</p></blockquote><p>文章前面也提到，sequencemask是为了使得Decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p><p>sequence mask的目的是防止Decoder “seeing thefuture”，就像防止考生偷看考试答案一样。这里mask是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0。下面是个10维度的下三角矩阵：</p><p><span class="math display">\[\begin{bmatrix}1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;0\\1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;0\\1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;0\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;0\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp;0\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp;0\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp;0\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp;0\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;0\\1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1\\\end{bmatrix}\]</span></p><p>对于Decoder的Self-Attention，里面使用到的scaled dot-productattention，同时需要padding mask和sequencemask作为attn_mask，具体实现就是两个mask相加作为attn_mask。</p><p>其他情况，attn_mask一律等于padding mask。</p><p>举个例子：</p><p>假设最大允许的序列长度为10，先令padding mask为</p><p>[0 0 0 0 0 0 0 0 0 0]</p><p>然后假设当前句子一共有5个单词（加一个起始标识），在输入第三个单词的时候，前面有一个开始标识和两个单词，则此刻的sequencemask为</p><p>[1 1 1 0 0 0]</p><p>然后padding mask和sequence mask相加，得</p><p>[1 1 1 0 0 0 0 0 0 0]</p><p><strong>问题四</strong></p><blockquote><p>为什么在模型训练阶段，Decoder的初始输入需要整体右移（ShiftedRight）一位？</p></blockquote><p>因为<span class="math inline">\(T-1\)</span>时刻需要预测<spanclass="math inline">\(T\)</span>时刻的输出，所以Decoder的输入需要整体后移一位</p><p>举例说明：<code>汤姆追逐杰瑞</code> →<code>Tom chase Jerry</code></p><p>位置关系： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0-“Tom”</span><br><span class="line">1-“chase”</span><br><span class="line">2-“Jerry”</span><br></pre></td></tr></table></figure> 操作：整体右移一位（Shifted Right）<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0-&lt;/s&gt;【起始符】目的是为了预测下一个Token</span><br><span class="line">1-“Tom”</span><br><span class="line">2-“chase”</span><br><span class="line">3-“Jerry”</span><br></pre></td></tr></table></figure></p><imgsrc="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKtPv2OdAZhNaaw3%2Ftransformer-process-2.gif?generation=1583677021471399&amp;alt=media" /><center><code>图32: 另一个例子说明Transformer模型的解码工作过程(省略了&lt;/s&gt;)</code></center><h2 id="output">Output</h2><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(4).png" /><center><code>图33: Decoder之后的输出部分</code></center><p><br><br>Decoder最终输出的结果是一个浮点型数据的向量，我们要如何把这个向量转为一个单词呢？这个就是Linear和softmax要做的事情了。</p><p>Linear层是一个全连接的神经网络，输出神经元个数一般等于我们的词汇表大小。Decoder输出的结果会输入到Linear层，然后再用softmax进行转换，得到的是词汇表大小的向量，向量的每个值对应的是当前Decoder是对应的这个词的概率，我们只要取概率最大的词，就是当前词语Decoder的结果了。</p><p>也就是说，Decoder的输出值首先经过一次线性变换，然后Softmax得到输出的概率分布，然后通过词典，输出概率最大的对应的单词作为我们的预测输出。</p><h1 id="transformer训练tricks">Transformer训练Tricks</h1><p>这里有两个训练小技巧，第一个是label平滑，第二个就是学习率要有个wormup过程，然后再下降。</p><p>1、Label Smoothing（regularization）</p><p>由传统的 <span class="math display">\[\begin{equation}P_i=\begin{cases}1&amp; \text{ $ i = y $ } \\0&amp; \text{ $ i \neq y $ }\end{cases}\end{equation}\]</span></p><p>变为</p><p><span class="math display">\[\begin{equation}P_i=\begin{cases}1−ϵ&amp; \text{ $ i = y $ } \\\frac{ϵ}{K−1}&amp; \text{ $ i \neq y $ }\end{cases}\end{equation}\]</span></p><p>注：<span class="math inline">\(K\)</span>表示多分类的类别总数，<spanclass="math inline">\(\epsilon\)</span>是一个较小的超参数。</p><p>2、<strong>[论文6]</strong>: <ahref="https://arxiv.org/abs/1812.10464">Mikel Artetxe, Holger Schwenk:“Massively Multilingual Sentence Embeddings for Zero-Shot Cross-LingualTransfer and Beyond”, 2018; [http://arxiv.org/abs/1812.10464arXiv:1812.10464]. DOI: [https://dx.doi.org/10.1162/tacl_a_0028810.1162/tacl_a_00288].</a></p><p>Noam learning rateschedule是一种在训练深度学习模型时调整学习率的方法。这种方法是由GoogleAI团队的NoamShazeer在2018年提出的。它的基本思想是，随着模型的训练进程，学习率应该逐渐降低。具体来说，学习率是根据训练步数的对数来调整的。这个方法可以帮助模型在训练初期快速收敛，并在训练后期更稳定地优化。</p><p>学习率不按照Noam Learning RateSchedule，可能就得不到一个好的Transformer。</p><p><spanclass="math display">\[lr=d_{model}^{−0.5}⋅min(step_{num}^{−0.5}, step_{num}\cdotwarmup\_steps^{−1.5})\]</span></p><p>公式表示学习率随着训练步数的增加而逐渐降低，在训练的前<spanclass="math inline">\(warmup_steps\)</span>步中学习率是线性增长的,之后学习率是指数下降的。如图所示：</p><imgsrc="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKtJ7Yme6Ll5rgo9%2Flr-worm-up.jpg?generation=1583677009478369&amp;alt=media" /><center><code>图34: Noam learning rate schedule学习率随着训练步数的增加先上升后下降</code></center><h1 id="transformer特点">Transformer特点</h1><h2 id="优点">优点</h2><ol type="1"><li><p>每层计算复杂度比RNN低，但是在序列长度很长时，Transformer的计算复杂度仍然很高，尤其是Self-Attention计算的复杂度是<spanclass="math inline">\(O(n^2)\)</span>，这使得Transformer在处理超长序列时仍然存在挑战。</p></li><li><p>并行计算的能力确实是Transformer的优点之一，它可以在GPU等硬件加速下实现高效计算。</p></li><li><p>从计算一个序列长度为n的信息要经过的路径长度来看,CNN需要增加卷积层数来扩大视野，RNN需要从1到n逐个进行计算，而Self-attention只需要一步矩阵计算就可以。Self-Attention可以比RNN<strong>更好地解决长时依赖问题</strong>。当然如果计算量太大，比如序列长度N大于序列维度D这种情况，也可以用窗口限制Self-Attention的计算数量。</p></li><li><p>Transformer确实比较容易进行可视化和解释，特别是在语言建模和文本生成等任务上，可以通过可视化Attention权重来观察模型的输出结果。</p></li></ol><h2 id="缺点">缺点</h2><p>在原文中没有提到缺点，是后来在UniversalTransformers中指出的，主要是两点：</p><ol type="1"><li>在原始的Transformer中，无法处理具有变长输出的序列转换任务，这是由于在每个时间步骤输出只依赖于其之前的输入序列和位置编码。因此，Transformer不能像RNN那样处理任意长度的序列输出。这被称为"输出长度限制"问题。对于复制字符串这样的任务，RNN可以轻松处理，因为RNN可以按照输入序列中的顺序逐个生成输出序列。但是对于Transformer，由于输出长度受到限制，因此复制字符串这样的任务需要特殊处理才能实现。另一个相关问题是在推理时，可能会遇到超出训练时所见过的序列长度的输入序列。由于Transformer使用的是固定的位置编码，它在训练时只能处理长度为<span class="math inline">\(n\)</span>的序列，而在推理时只能处理长度小于等于 <spanclass="math inline">\(n\)</span>的序列。如果遇到超出这个长度的输入序列，Transformer无法准确地处理这些输入，因为它从未见过这些位置的位置编码。这是Transformer的"序列长度限制"问题。</li></ol><p>解决这些问题的方法是对Transformer进行扩展，例如在输出端使用动态解码器，允许变长输出；或使用可学习的位置编码来处理变长输入序列。此外，一些变体的Transformer，例如UniversalTransformer，能够在推理时动态地生成其自身的位置编码，从而能够处理任意长度的序列。</p><ol start="2" type="1"><li><p>理论上：transformers不是computationallyuniversal(图灵完备)，而RNN图灵完备，这种非RNN式的模型是非图灵完备的，<strong>无法单独完成NLP中推理、决策等计算问题</strong>（包括使用transformer的bert模型等等）。</p></li><li><p>在一些需要考虑序列顺序的任务上，如机器翻译和语音识别，Transformer仍然需要一些额外的手段来考虑顺序问题，例如在Transformer中引入位置编码。</p></li></ol><h1 id="参考链接">参考链接</h1><h2 id="整体代码实现">整体代码实现</h2><ul><li><p>哈佛大学NLP组的notebook，很详细文字和代码描述，用pytorch实现</p><p><ahref="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p></li><li><p>Google的TensorFlow官方的，用tf keras实现</p><p><ahref="https://www.tensorflow.org/tutorials/text/transformer">https://www.tensorflow.org/tutorials/text/transformer</a></p></li></ul><h2 id="参考网站">参考网站</h2><ol type="1"><li><p>Self-Attention和Transformer</p><p><ahref="https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#%E8%AF%8D%E5%90%91%E9%87%8FEmbedding%E8%BE%93%E5%85%A5">https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#%E8%AF%8D%E5%90%91%E9%87%8FEmbedding%E8%BE%93%E5%85%A5</a></p></li><li><p>史上最小白之Transformer详解:</p><p><ahref="https://blog.csdn.net/Tink1995/article/details/105080033">https://blog.csdn.net/Tink1995/article/details/105080033</a></p></li><li><p>史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！:</p><p><ahref="https://zhuanlan.zhihu.com/p/148656446">https://zhuanlan.zhihu.com/p/148656446</a></p></li><li><p>The Illustrated Transformer:</p><p><ahref="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p></li><li><p>A Brief Overview of Recurrent Neural Networks (RNN):</p><p><ahref="https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/">https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/</a></p></li><li><p>Practical PyTorch: Translation with a Sequence to SequenceNetwork and Attention:</p><p><ahref="https://notebook.community/spro/practical-pytorch/seq2seq-translation/seq2seq-translation-batched">https://notebook.community/spro/practical-pytorch/seq2seq-translation/seq2seq-translation-batched</a></p></li><li><p>也来谈谈RNN的梯度消失/爆炸问题:</p><p><ahref="https://kexue.fm/archives/7888">https://kexue.fm/archives/7888</a></p></li><li><p>Transformer 架构逐层功能介绍和详细解释:</p><p><ahref="https://avoid.overfit.cn/post/a895d880dab245609c177db7598446d4">https://avoid.overfit.cn/post/a895d880dab245609c177db7598446d4</a></p></li><li><p>Pytorch中 nn.Transformer的使用详解与Transformer的黑盒讲解:</p><p><ahref="https://blog.51cto.com/u_11466419/5530949">https://blog.51cto.com/u_11466419/5530949</a></p></li><li><p>【深度学习】Attention is All You Need : Transformer模型:</p><p><ahref="https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/">https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/</a></p></li><li><p>Bert前篇：手把手带你详解Transformer原理:</p><p><ahref="https://zhuanlan.zhihu.com/p/364659780">https://zhuanlan.zhihu.com/p/364659780</a></p></li><li><p>ChatGPT3：</p><p><ahref="https://chat.openai.com/chat">https://chat.openai.com/chat</a></p></li><li><p>一文搞懂one-hot和embedding：</p><p><ahref="https://blog.csdn.net/Alex_81D/article/details/114287498">https://blog.csdn.net/Alex_81D/article/details/114287498</a></p></li><li><p>残差网络(Residual Network)：</p><p><ahref="https://www.cnblogs.com/gczr/p/10127723.html">https://www.cnblogs.com/gczr/p/10127723.html</a></p></li><li><p>【经典精读】万字长文解读Transformer模型和Attention机制</p><p><ahref="https://zhuanlan.zhihu.com/p/104393915?utm_source=ZHShareTargetIDMore">https://zhuanlan.zhihu.com/p/104393915?utm_source=ZHShareTargetIDMore</a></p></li></ol><h2 id="参考文献">参考文献</h2><ul><li><p><strong>[论文1]</strong>: <ahref="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">Hochreiter,Sepp, and Jürgen Schmidhuber. "Long short-term memory." Neuralcomputation 9.8 (1997): 1735-1780.</a></p></li><li><p><strong>[论文2]</strong>: <ahref="https://arxiv.org/abs/1409.0473">Dzmitry Bahdanau, Kyunghyun Cho,Yoshua Bengio: “Neural Machine Translation by Jointly Learning to Alignand Translate”, 2014; [http://arxiv.org/abs/1409.0473arXiv:1409.0473].</a></p></li><li><p><strong>[论文3]</strong>: <ahref="https://arxiv.org/abs/1706.03762">Ashish Vaswani, Noam Shazeer,Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, Illia Polosukhin: “Attention Is All You Need”, 2017;[http://arxiv.org/abs/1706.03762 arXiv:1706.03762].</a></p></li><li><p><strong>[论文4]</strong>: <ahref="https://arxiv.org/abs/1512.03385">Kaiming He, Xiangyu Zhang,Shaoqing Ren, Jian Sun: “Deep Residual Learning for Image Recognition”,2015; [http://arxiv.org/abs/1512.03385 arXiv:1512.03385].</a></p></li><li><p><strong>[论文5]</strong>: <ahref="https://arxiv.org/abs/1607.06450">Jimmy Lei Ba, Jamie Ryan Kiros,Geoffrey E. Hinton: “Layer Normalization”, 2016;[http://arxiv.org/abs/1607.06450 arXiv:1607.06450].</a></p></li><li><p><strong>[论文6]</strong>: <ahref="https://arxiv.org/abs/1812.10464">Mikel Artetxe, Holger Schwenk:“Massively Multilingual Sentence Embeddings for Zero-Shot Cross-LingualTransfer and Beyond”, 2018; [http://arxiv.org/abs/1812.10464arXiv:1812.10464]. DOI: [https://dx.doi.org/10.1162/tacl_a_0028810.1162/tacl_a_00288].</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;导航&quot;&gt;导航&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;这篇文章是关于Transformer的原理详细解读。&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;本博客的 Transformer 系列文章共计四篇，导航如下：&lt;/p&gt;
&lt;/blockquote&gt;</summary>
      
    
    
    
    <category term="笔记, 教程" scheme="https://le-ying.github.io/categories/%E7%AC%94%E8%AE%B0-%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="科研" scheme="https://le-ying.github.io/tags/%E7%A7%91%E7%A0%94/"/>
    
    <category term="transformer" scheme="https://le-ying.github.io/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>CS231n Assignment 1 逐行解析</title>
    <link href="https://le-ying.github.io/2022/12/20/CS231n-Assignment-1/"/>
    <id>https://le-ying.github.io/2022/12/20/CS231n-Assignment-1/</id>
    <published>2022-12-20T14:49:13.000Z</published>
    <updated>2023-02-05T01:17:55.035Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cs231n-assignment-1">CS231n Assignment 1</h1><p>作业1分为五个部分：KNN、SVM、Softmax classifier、2层神经网络、HigherLevel Representations: Image Features.</p><p>建议作业完成顺序：</p><ol type="1"><li>k近邻分类：knn.ipynb &amp; k_nearest_neighbor.py</li><li>svm线性分类：svm.ipynb &amp; linear_svm.py &amp;linear_classifier.py</li><li>softmax线性分类：softmax.ipynb &amp; softmax.py</li><li>两层神经网络：two_layer_net.ipynb &amp; neural_net.py</li></ol><h1 id="k-nearest-neighbor-knn">k-Nearest Neighbor (kNN)</h1><p>在knn.ipynb中，调用了k_nearest_neighbor.py文件。</p><p>k近邻分类算法步骤如下介绍：</p><ol type="1"><li>记住所有训练图像</li><li>计算测试图像与所有训练图像的距离（常用L2距离）</li><li>选择与测试图像距离最小的k张训练图像</li><li>计算这k张图像所对应的类别出现的次数，选择出现次数最多的类别记为预测类别</li></ol><h2 id="k_nearest_neighbor.py">k_nearest_neighbor.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">KNearestNeighbor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; a kNN classifier with L2 distance &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#定义一个k近邻分类器的类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="训练">训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Train the classifier. For k-nearest neighbors this is just</span></span><br><span class="line"><span class="string">    memorizing the training data.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_train, D) containing the training data</span></span><br><span class="line"><span class="string">      consisting of num_train samples each of dimension D.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing the training labels, where</span></span><br><span class="line"><span class="string">         y[i] is the label for X[i].</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#self.X_train 是训练数据，维度是 (N,D)，训练集有N个样本，每个样本特征是D维</span></span><br><span class="line">    <span class="comment">#self.y_train 是标签，维度是（N,）,即N个训练样本对应的标签</span></span><br><span class="line">    self.X_train = X</span><br><span class="line">    self.y_train = y</span><br></pre></td></tr></table></figure><h3id="预测计算测试图像和所有训练图像的l2距离">预测：计算测试图像和所有训练图像的L2距离</h3><p>预测时首先需要计算测试样本与所有训练样本的距离,然后根据距离判断样本的类别。</p><p>计算距离需要我们实现三种方法，分别为需要双重循环，单循环，不需要循环。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X, k=<span class="number">1</span>, num_loops=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Predict labels for test data using this classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_test, D) containing test data consisting</span></span><br><span class="line"><span class="string">         of num_test samples each of dimension D.</span></span><br><span class="line"><span class="string">    - k: The number of nearest neighbors that vote for the predicted labels.</span></span><br><span class="line"><span class="string">    - num_loops: Determines which implementation to use to compute distances</span></span><br><span class="line"><span class="string">      between training points and testing points.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">      test data, where y[i] is the predicted label for the test point X[i].</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> num_loops == <span class="number">0</span>:</span><br><span class="line">        dists = self.compute_distances_no_loops(X)</span><br><span class="line">    <span class="keyword">elif</span> num_loops == <span class="number">1</span>:</span><br><span class="line">        dists = self.compute_distances_one_loop(X)</span><br><span class="line">    <span class="keyword">elif</span> num_loops == <span class="number">2</span>:</span><br><span class="line">        dists = self.compute_distances_two_loops(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid value %d for num_loops&quot;</span> % num_loops)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.predict_labels(dists, k=k)</span><br></pre></td></tr></table></figure><hr /><h4 id="双重循环实现">双重循环实现</h4><p>第i个测试样本与第j个训练样本的距离<spanclass="math inline">\(dist[i,j]\)</span>等于用第i个测试图像的特征向量减去第j个训练图像的特征向量的值</p><p><spanclass="math inline">\(dist[i,j]=\sqrt{\sum_{d}(x_{test[i,d]}-x_{train[j,d]})^2}\)</span></p><p><img src="https://s1.ax1x.com/2023/02/04/pSyTBNj.png" /></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_distances_two_loops</span>(<span class="params">self, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using a nested loop over both the training data and the</span></span><br><span class="line"><span class="string">    test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_test, D) containing test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">      is the Euclidean distance between the ith test point and the jth training</span></span><br><span class="line"><span class="string">      point.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_test):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_train):</span><br><span class="line">            <span class="comment">#####################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                             #</span></span><br><span class="line">            <span class="comment"># Compute the l2 distance between the ith test point and the jth    #</span></span><br><span class="line">            <span class="comment"># training point, and store the result in dists[i, j]. You should   #</span></span><br><span class="line">            <span class="comment"># not use a loop over dimension, nor use np.linalg.norm().          #</span></span><br><span class="line">            <span class="comment">#####################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            dists[i][j]=np.sqrt(np.<span class="built_in">sum</span>(np.square(X[i,:]-self.X_train[j,:])))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure><hr /><h4 id="单循环实现">单循环实现</h4><p>利用numpy的broadcast机制，可以直接计算第i张测试图像与所有训练样本的距离</p><p><img src="https://s1.ax1x.com/2023/02/04/pSyT0EQ.png" /></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_distances_one_loop</span>(<span class="params">self, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using a single loop over the test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_test):</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                               #</span></span><br><span class="line">        <span class="comment"># Compute the l2 distance between the ith test point and all training #</span></span><br><span class="line">        <span class="comment"># points, and store the result in dists[i, :].                        #</span></span><br><span class="line">        <span class="comment"># Do not use np.linalg.norm().                                        #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        dists[i,:]=np.sqrt(np.<span class="built_in">sum</span>(np.square(X[i,:]-self.X_train),axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure><hr /><h4 id="无循环实现">无循环实现</h4><p>两个矩阵不能直接相减，不用循环计算距离，考虑距离公式，同时需保证最后得到的dists.shape满足(num_test,num_train)=(500,5000)</p><p><span class="math inline">\((a-b)^2=a^2+b^2-2ab\)</span></p><p><img src="https://s1.ax1x.com/2023/02/04/pSyTsCn.png" /></p><p>注意点：</p><ol type="1"><li>np.square，返回原列表每个元素的平方值，shape不变。</li><li>np.sum(,axis=1)按列相加（横向），我之前在这个地方没想通。以np.sum(np.square(self.X_train),axis = 1)为例：</li></ol><ul><li>np.shape(self.X_train)=(5000,3072)</li><li>np.shape(np.square(self.X_train))=(5000,3072)</li><li>np.shape(np.sum(np.square(self.X_train)))=(5000,)这表示的是一个含有5000个元素的一维数组，并不是(5000,1)具有5000行的二维list。因此可以被boardcasting,(1,5000)-&gt;(500,5000)</li></ul><ol start="3" type="1"><li>transpose针对二维及以上list有效。以np.transpose([np.sum(np.square(X),axis = 1)]))为例，结合第2点可知:</li></ol><ul><li>得到的np.sum(np.square(X), axis =1)]))是一个一维数组，shape为(500,)</li><li>因此加上[]变成(1,500)</li><li>之后转置得到(500,1),因此可以被boardcasting,(500,1)-&gt;(500,5000)</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_distances_no_loops</span>(<span class="params">self, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using no explicit loops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">    <span class="comment"># Compute the l2 distance between all test points and all training      #</span></span><br><span class="line">    <span class="comment"># points without using any explicit loops, and store the result in      #</span></span><br><span class="line">    <span class="comment"># dists.                                                                #</span></span><br><span class="line">    <span class="comment">#                                                                       #</span></span><br><span class="line">    <span class="comment"># You should implement this function using only basic array operations; #</span></span><br><span class="line">    <span class="comment"># in particular you should not use functions from scipy,                #</span></span><br><span class="line">    <span class="comment"># nor use np.linalg.norm().                                             #</span></span><br><span class="line">    <span class="comment">#                                                                       #</span></span><br><span class="line">    <span class="comment"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span></span><br><span class="line">    <span class="comment">#       and two broadcast sums.                                         #</span></span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    dists = np.sqrt(-<span class="number">2</span>*np.dot(X, self.X_train.T) + np.<span class="built_in">sum</span>(np.square(self.X_train), axis = <span class="number">1</span>) + np.transpose([np.<span class="built_in">sum</span>(np.square(X), axis = <span class="number">1</span>)]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure><h3id="预测根据k个最邻近距离中的多数确定标签">预测：根据K个最邻近距离中的多数确定标签</h3><ul><li>np.argsort() 返回一个数组排好序后各元素对应的原来的位置序号</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#One dimensional array:</span></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">np.argsort(x)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#Two-dimensional array:</span></span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">0</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">x</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">ind = np.argsort(x, axis=<span class="number">0</span>)  <span class="comment"># sorts along first axis (down)</span></span><br><span class="line">ind</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">np.take_along_axis(x, ind, axis=<span class="number">0</span>)  <span class="comment"># same as np.sort(x, axis=0)</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure><ul><li>np.bincount() 计算非负整数数组中每个值的出现次数。</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#[0,1,2,3,4]</span></span><br><span class="line">np.bincount(np.arange(<span class="number">5</span>))</span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">np.bincount(np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">7</span>]))</span><br><span class="line">array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><hr /><p><strong>需要实现的两个功能</strong></p><ol type="1"><li><p>选择与测试图像最相似（距离最小）的k张训练图像np.argsort(dists[i])函数是将dist中的i行元素从小到大排列，并得到对应的index。然后再取前k个索引（也就是得到距离最近的k张图像的索引）</p></li><li><p>计算这k张图像所对应的类别出现的次数，选择出现次数最多的类别</p></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_labels</span>(<span class="params">self, dists, k=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Given a matrix of distances between test points and training points,</span></span><br><span class="line"><span class="string">    predict a label for each test point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">      gives the distance betwen the ith test point and the jth training point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">      test data, where y[i] is the predicted label for the test point X[i].</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_test = dists.shape[<span class="number">0</span>]</span><br><span class="line">    y_pred = np.zeros(num_test)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_test):</span><br><span class="line">        <span class="comment"># A list of length k storing the labels of the k nearest neighbors to</span></span><br><span class="line">        <span class="comment"># the ith test point.</span></span><br><span class="line">        closest_y = []</span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">        <span class="comment"># Use the distance matrix to find the k nearest neighbors of the ith    #</span></span><br><span class="line">        <span class="comment"># testing point, and use self.y_train to find the labels of these       #</span></span><br><span class="line">        <span class="comment"># neighbors. Store these labels in closest_y.                           #</span></span><br><span class="line">        <span class="comment"># Hint: Look up the function numpy.argsort.                             #</span></span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        closest_y = self.y_train[np.argsort(dists[i])[:k]]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">        <span class="comment"># Now that you have found the labels of the k nearest neighbors, you    #</span></span><br><span class="line">        <span class="comment"># need to find the most common label in the list closest_y of labels.   #</span></span><br><span class="line">        <span class="comment"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span></span><br><span class="line">        <span class="comment"># label.                                                                #</span></span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        y_pred[i] = np.argmax(np.bincount(closest_y))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="knn.ipynb">knn.ipynb</h2><p>讨论knn中k的取值问题</p><ul><li>np.array_split() 将一个数组拆分为多个子数组，可以大小不等。</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">8.0</span>)</span><br><span class="line">np.array_split(x, <span class="number">3</span>)</span><br><span class="line">[array([<span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>]), array([<span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>]), array([<span class="number">6.</span>,  <span class="number">7.</span>])]</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">9</span>)</span><br><span class="line">np.array_split(x, <span class="number">4</span>)</span><br><span class="line">[array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]), array([<span class="number">3</span>, <span class="number">4</span>]), array([<span class="number">5</span>, <span class="number">6</span>]), array([<span class="number">7</span>, <span class="number">8</span>])]</span><br></pre></td></tr></table></figure><ul><li>dictionary.setdefault(keyname, value)返回具有指定键的项目的值。</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">car = &#123;</span><br><span class="line">  <span class="string">&quot;brand&quot;</span>: <span class="string">&quot;Ford&quot;</span>,</span><br><span class="line">  <span class="string">&quot;model&quot;</span>: <span class="string">&quot;Mustang&quot;</span>,</span><br><span class="line">  <span class="string">&quot;year&quot;</span>: <span class="number">1964</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">x = car.setdefault(<span class="string">&quot;color&quot;</span>, <span class="string">&quot;White&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>hstack() 按列顺序(横向)把数组给堆叠起来</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b=[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="built_in">print</span>(np.hstack((a,b)))</span><br><span class="line"><span class="comment">#[1 2 3 4 5 6 ]</span></span><br><span class="line"></span><br><span class="line">a=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">b=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">c=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">d=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line"><span class="built_in">print</span>(np.hstack((a,b,c,d)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[1 1 1 1]</span></span><br><span class="line"><span class="string"> [2 2 2 2]</span></span><br><span class="line"><span class="string"> [3 3 3 3]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><hr /><p>knn需要计算这k张图像所对应的类别出现的次数，选择出现次数最多的类别，那么问题来了，k应该取几效果会比较好呢？</p><p>这需要做两个任务：</p><ol type="1"><li>划分训练集</li><li>交叉验证</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_folds = <span class="number">5</span></span><br><span class="line">k_choices = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">X_train_folds = []</span><br><span class="line">y_train_folds = []</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Split up the training data into folds. After splitting, X_train_folds and    #</span></span><br><span class="line"><span class="comment"># y_train_folds should each be lists of length num_folds, where                #</span></span><br><span class="line"><span class="comment"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span></span><br><span class="line"><span class="comment"># Hint: Look up the numpy array_split function.                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">X_train_folds = np.array_split(X_train,num_folds)</span><br><span class="line">y_train_folds = np.array_split(y_train,num_folds)</span><br><span class="line"></span><br><span class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A dictionary holding the accuracies for different values of k that we find</span></span><br><span class="line"><span class="comment"># when running cross-validation. After running cross-validation,</span></span><br><span class="line"><span class="comment"># k_to_accuracies[k] should be a list of length num_folds giving the different</span></span><br><span class="line"><span class="comment"># accuracy values that we found when using that value of k.</span></span><br><span class="line">k_to_accuracies = &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Perform k-fold cross validation to find the best value of k. For each        #</span></span><br><span class="line"><span class="comment"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span></span><br><span class="line"><span class="comment"># where in each case you use all but one of the folds as training data and the #</span></span><br><span class="line"><span class="comment"># last fold as a validation set. Store the accuracies for all fold and all     #</span></span><br><span class="line"><span class="comment"># values of k in the k_to_accuracies dictionary.                               #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">    k_to_accuracies.setdefault(k, [])</span><br><span class="line">    <span class="comment">#print(k_to_accuracies)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_folds):</span><br><span class="line">    classifier = KNearestNeighbor()</span><br><span class="line">    X_val_train = np.vstack(X_train_folds[<span class="number">0</span>:i] + X_train_folds[i+<span class="number">1</span>:])</span><br><span class="line">    y_val_train = np.hstack(y_train_folds[<span class="number">0</span>:i] + y_train_folds[i+<span class="number">1</span>:])</span><br><span class="line">    <span class="comment">#print(X_val_train, y_val_train)</span></span><br><span class="line">    classifier.train(X_val_train, y_val_train)</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">        y_val_pred = classifier.predict(X_train_folds[i], k)</span><br><span class="line">        num_correct = np.<span class="built_in">sum</span>(y_val_pred == y_train_folds[i])</span><br><span class="line">        accuracy = <span class="built_in">float</span>(num_correct) / <span class="built_in">len</span>(y_val_pred)</span><br><span class="line">        k_to_accuracies[k] += [accuracy]</span><br><span class="line">        <span class="comment">#print(k,k_to_accuracies[k])</span></span><br><span class="line">    <span class="comment">#print(k_to_accuracies)</span></span><br><span class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the computed accuracies</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">sorted</span>(k_to_accuracies):</span><br><span class="line">    <span class="keyword">for</span> accuracy <span class="keyword">in</span> k_to_accuracies[k]:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;k = %d, accuracy = %f&#x27;</span> % (k, accuracy))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#OUTPUT</span></span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.263000</span></span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.257000</span> </span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.264000</span></span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.278000</span></span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.266000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.239000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.249000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.240000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.266000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.254000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.248000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.266000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.280000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.292000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.280000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.262000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.282000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.273000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.290000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.273000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.265000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.296000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.276000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.284000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.280000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.260000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.295000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.279000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.283000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.280000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.252000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.289000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.278000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.282000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.274000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.270000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.279000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.279000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.282000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.285000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.271000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.288000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.278000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.269000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.266000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.256000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.270000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.263000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.256000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.263000</span></span><br></pre></td></tr></table></figure><p>通过这行代码可以选出最好的k值</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_k = k_choices[accuracies_mean.argmax()]</span><br></pre></td></tr></table></figure><hr /><h3 id="inline-question-1"><strong>Inline Question 1</strong></h3><p>Notice the structured patterns in the distance matrix, where somerows or columns are visibly brighter. (Note that with the default colorscheme black indicates low distances while white indicates highdistances.)</p><ul><li>What in the data is the cause behind the distinctly brightrows?</li><li>What causes the columns?</li></ul><p><span class="math inline">\(\color{blue}{\textit YourAnswer:}\)</span> <em>fill this in.</em></p><ol type="1"><li><p>The test image is far different from all the trainimage.</p></li><li><p>The train image is unsimilar to all the test image.</p></li></ol><hr /><h3 id="inline-question-2"><strong>Inline Question 2</strong></h3><p>We can also use other distance metrics such as L1 distance. For pixelvalues <span class="math inline">\(p_{ij}^{k}\)</span> at location <spanclass="math inline">\((i,j)\)</span> of some image <spanclass="math inline">\(I_k\)</span>,</p><p>the mean <span class="math inline">\(\mu\)</span> across all pixelsover all images is <spanclass="math display">\[\mu=\frac{1}{nhw}\sum_{k=1}^n\sum_{i=1}^{h}\sum_{j=1}^{w}p_{ij}^{(k)}\]</span>And the pixel-wise mean <span class="math inline">\(\mu_{ij}\)</span>across all images is <spanclass="math display">\[\mu_{ij}=\frac{1}{n}\sum_{k=1}^np_{ij}^{(k)}.\]</span>The general standard deviation <spanclass="math inline">\(\sigma\)</span> and pixel-wise standard deviation<span class="math inline">\(\sigma_{ij}\)</span> is definedsimilarly.</p><p>Which of the following preprocessing steps will not change theperformance of a Nearest Neighbor classifier that uses L1 distance?Select all that apply.</p><ol type="1"><li><p>Subtracting the mean <span class="math inline">\(\mu\)</span>(<spanclass="math inline">\(\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu\)</span>.)</p></li><li><p>Subtracting the per pixel mean <spanclass="math inline">\(\mu_{ij}\)</span> (<spanclass="math inline">\(\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu_{ij}\)</span>.)</p></li><li><p>Subtracting the mean <span class="math inline">\(\mu\)</span> anddividing by the standard deviation <spanclass="math inline">\(\sigma\)</span>.</p></li><li><p>Subtracting the pixel-wise mean <spanclass="math inline">\(\mu_{ij}\)</span> and dividing by the pixel-wisestandard deviation <spanclass="math inline">\(\sigma_{ij}\)</span>.</p></li><li><p>Rotating the coordinate axes of the data.</p></li></ol><p><span class="math inline">\(\color{blue}{\textit YourAnswer:}\)</span></p><p>1, 3 will not change the L1 Distance.</p><p><span class="math inline">\(\color{blue}{\textit YourExplanation:}\)</span></p><p><img src="https://s1.ax1x.com/2023/02/04/pSyTRDU.jpg" /></p><p><img src="https://s1.ax1x.com/2023/02/04/pSyTcvV.jpg" /></p><p><img src="https://s1.ax1x.com/2023/02/04/pSyT2uT.jpg" /></p><hr /><h3 id="inline-question-3"><strong>Inline Question 3</strong></h3><p>Which of the following statements about <spanclass="math inline">\(k\)</span>-Nearest Neighbor (<spanclass="math inline">\(k\)</span>-NN) are true in a classificationsetting, and for all <span class="math inline">\(k\)</span>? Select allthat apply. 1. The decision boundary of the k-NN classifier is linear.2. The training error of a 1-NN will always be lower than or equal tothat of 5-NN. 3. The test error of a 1-NN will always be lower than thatof a 5-NN. 4. The time needed to classify a test example with the k-NNclassifier grows with the size of the training set. 5. None of theabove.</p><p><span class="math inline">\(\color{blue}{\textit YourAnswer:}\)</span></p><p>2, 4.</p><p><span class="math inline">\(\color{blue}{\textit YourExplanation:}\)</span></p><ol type="1"><li><p>False. It depends on the given categories of data, if you give acategory with a circle boundary to its neighborhood, it isnon-linear.</p></li><li><p>True. In fact the training error of a 1-NN is always 0, and5-NN's lower bound is 0. It is because the nearest neighbor of test datais always going to be itself in 1-NN.</p></li><li><p>False. The value of k is thus data-dependent, that is why we needto perform cross validation to determine the best k for your intendedapplication and dataset.</p></li><li><p>True. At test, KNN needs to make a full pass through the entiredata set and sort points by distance. The time needed thus grows withthe size of the data.</p></li></ol><hr /><h2 id="knn部分参考链接">KNN部分参考链接:</h2><ol type="1"><li><p>cs231n官网: <ahref="https://cs231n.github.io/">https://cs231n.github.io/</a></p></li><li><p>cs231n作业，assignment1-knn详解（注重算法与代码的结合）: <ahref="https://blog.csdn.net/qq_24906797/article/details/89245722">https://blog.csdn.net/qq_24906797/article/details/89245722</a></p></li><li><p>cs231n assignment1 knn: <ahref="https://blog.csdn.net/SpicyCoder/article/details/94992552">https://blog.csdn.net/SpicyCoder/article/details/94992552</a></p></li><li><p>【本课程配套的代码作业讲解见置顶评论】斯坦福CS231N计算机视觉作业讲解：<ahref="https://www.bilibili.com/video/BV1t4411U78z/?spm_id_from=333.337.search-card.all.click&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a">https://www.bilibili.com/video/BV1t4411U78z/?spm_id_from=333.337.search-card.all.click&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a</a></p></li><li><p>CS231N作业详解零基础版： <ahref="https://www.bilibili.com/video/BV19z411b7u9/?p=6&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a">https://www.bilibili.com/video/BV19z411b7u9/?p=6&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a</a></p></li></ol><h1 id="support-vector-machine-svm">Support Vector Machine (SVM)</h1><p>在svm.ipynb中，调用了linear_svm.py和linear_classifier.py两个文件。</p><p>为方便理解，先介绍SVM的引入基于的几个概念：</p><ol type="1"><li><p>我们要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：</p><ul><li><p>一个是评分函数（scorefunction），它是原始图像数据到类别分值的映射。</p></li><li><p>另一个是损失函数（lossfunction），它是用来量化预测分类标签的得分与真实标签之间一致性的。</p></li></ul><p>该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。从图像像素值到所属类别的评分函数（scorefunction）</p></li><li><p>该我们现在定义评分函数为<span class="math inline">\(f:R^D \toR^K\)</span>，该函数是原始图像像素到分类分值的映射。在<strong>线性分类器</strong>中，一个线性映射：<spanclass="math inline">\(f(x_i,W,b)=Wx_i+b\)</span>。在函数中，数据<spanclass="math inline">\((x_i,y_i)\)</span>是给定的，不能修改。但是我们可以调整权重矩阵<spanclass="math inline">\(W\)</span>这个参数，使得评分函数的结果与训练数据集中图像的真实类别一致，即评分函数在正确的分类的位置应当得到最高的评分（score）。</p></li><li><p>我们将使用损失函数（****Loss Function）（有时也叫代价函数****CostFunction或目标函数****Objective）来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。多类支持向量机（SVM）损失函数是其中一种。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值Delta$ $。</p></li></ol><h2 id="svm简介">SVM简介</h2><h3 id="公式说明">公式说明</h3><p>SVM算法由两个部分组成：数据损失（dataloss），即所有样例的的平均损失L_i，以及正则化损失（regularizationloss）。完整公式如下：</p><p><imgsrc="https://camo.githubusercontent.com/19467d143a4397b56c0aaf83d66de6ba9a4615c801e5ed3f2e1272b80748e789/687474703a2f2f7a686968752e636f6d2f6571756174696f6e3f7465783d4c253344253543646973706c61797374796c652b253543756e64657262726163652537422b25354366726163253742312537442537424e25374425354373756d5f692b4c5f692537445f253742646174612b2535432b2b6c6f7373253744253242253543756e64657262726163652537422535436c616d6264612b52253238572532392537445f253742726567756c6172697a6174696f6e2b2535432b6c6f7373253744" /></p><p>将其展开完整公式是：</p><p><span class="math inline">\(L=\frac{1}{N}\sum_i\sum_{j \not=y_i}[\max(0,f(x_i;W)j-f(x_i;W){y_i}+\Delta)]+\lambda \sum_k \sum_lW^2_{k,l}\)</span></p><p>其中参数意义如下： * X(N,D),N是训练集的数据量。 *W(D,C),C代表图片分类的数量。 * y(N,) * i 是迭代第N个训练集数据 * j是第C个图片分类 * <span class="math inline">\(\lambda\)</span>正则化惩罚</p><p>正则化惩罚添加到了损失函数里面，并用超参数<spanclass="math inline">\(\lambda\)</span>来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。</p><p>引入正则化惩罚还带来很多良好的性质，其中最好的性质就是对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响。</p><h3 id="注意点">注意点</h3><p>超参数在绝大多数情况下设为<spanclass="math inline">\(\Delta\)</span>=1.0都是安全的。超参数<spanclass="math inline">\(\Delta\)</span>和<spanclass="math inline">\(\lambda\)</span>看起来是两个不同的超参数，但实际上他们一起控制同一个权衡：即损失函数中的数据损失和正则化损失之间的权衡。</p><h2 id="linear_svm.py">linear_svm.py</h2><p>损失函数公式： <span class="math inline">\(L=\frac{1}{N}\sum_i\sum_{j\not= y_i}[\max(0,f(x_i;W)j-f(x_i;W){y_i}+\Delta)]+\lambda \sum_k \sum_lW^2_{k,l}\)</span></p><p>我们想通过一个方法来得到损失函数L的最小值，这里考虑使用计算W的梯度来不停的对L进行优化，这里想的就是初始化一个W，然后计算W的梯度，接着不停的迭代W，直到收敛或者达到迭代次数。那问题就变成如何求L对于W的梯度了。</p><h3 id="循环求解">循环求解</h3><p>后面的正则项，就是<span class="math inline">\(\lambda \sum_k \sum_lW^2_{k,l}\)</span>，求导即为 <span class="math display">\[\frac{dL}{dw}(正则项)=2*\lambda*W\]</span></p><p>主要是求前面数据损失函数的梯度。那么，我们先把L给拆分一下，这样可以去掉一个求和符号<span class="math display">\[L_i=\sum_{j \not=y_i}\max(0,x_iw_j-x_iw_{y_i}+\Delta)\]</span></p><p>（1）考虑<span class="math inline">\(j \not= y_i\)</span></p><p><span class="math display">\[\begin{aligned}\frac{dL_i}{dw_j}=1(x_iw_j-x_iw_{y_i}+\Delta&gt;0) *    \begin{array}    {|c|}    x_{i1} \\    x_{i2}\\    \vdots&amp;\\    x_{iD}    \end{array}\end{aligned}\]</span> 所以得到 <span class="math display">\[\frac{dL_i}{dw_j}=1(x_iw_j-x_iw_{y_i}+\Delta&gt;0) *{x_i}\]</span></p><p>（2）考虑<span class="math inline">\(j = y_i\)</span>，则满足</p><ul><li><spanclass="math display">\[\frac{d(x_iw_j)}{dw_{y_i}}=0\]</span></li><li><spanclass="math display">\[\frac{d(-x_iw_{y_i})}{dw_{y_i}}=-x_i\]</span></li></ul><p>因此代入以下公式 <span class="math display">\[\begin{aligned}\frac{dL_i}{dw_{y_i}}=-\sum_{j \not= y_i}1(x_iw_j-x_iw_{y_i}+\Delta&gt;0) *    \begin{array}    {|c|}    x_{i1} \\    x_{i2}\\    \vdots&amp;\\    x_{iD}    \end{array}\end{aligned}\]</span></p><p>最终可以得到</p><p><span class="math display">\[\frac{dL_i}{dw_{y_i}}=-\sum_{j \not= y_i}1(x_iw_j-x_iw_{y_i}+\Delta&gt;0) * x_i\]</span></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">svm_loss_naive</span>(<span class="params">W, X, y, reg</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Structured SVM loss function, naive implementation (with loops).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dW = np.zeros(W.shape)  <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the loss and the gradient</span></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        correct_class_score = scores[y[i]]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            margin = scores[j] - correct_class_score + <span class="number">1</span>  <span class="comment"># note delta = 1</span></span><br><span class="line">            <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</span><br><span class="line">                loss += margin</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Right now the loss is a sum over all training examples, but we want it</span></span><br><span class="line">    <span class="comment"># to be an average instead so we divide by num_train.</span></span><br><span class="line">    loss /= num_train</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add regularization to the loss.</span></span><br><span class="line">    loss += reg * np.<span class="built_in">sum</span>(W * W)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Compute the gradient of the loss function and store it dW.                #</span></span><br><span class="line">    <span class="comment"># Rather that first computing the loss and then computing the derivative,   #</span></span><br><span class="line">    <span class="comment"># it may be simpler to compute the derivative at the same time that the     #</span></span><br><span class="line">    <span class="comment"># loss is being computed. As a result you may need to modify some of the    #</span></span><br><span class="line">    <span class="comment"># code above to compute the gradient.                                       #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_train):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            margin = scores[j] - correct_class_score + <span class="number">1</span>  <span class="comment">#note delta = 1 </span></span><br><span class="line">            <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</span><br><span class="line">                dW[:,j]+=X[i,:]</span><br><span class="line">                dW[:,y[i]]+=-X[i,:]</span><br><span class="line">    </span><br><span class="line">    dW /= num_train</span><br><span class="line">    dW += <span class="number">2</span>*reg*dW</span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><h3 id="向量化实现">向量化实现</h3><p>第一个部分，损失函数。公式和前面基本一致，数据损失函数部分</p><p>分类正确即对应 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scores[<span class="built_in">range</span>(N),y]=<span class="number">0</span> </span><br></pre></td></tr></table></figure></p><p>分类错误即对应</p><p><span class="math display">\[L_i=\sum_{j \not=y_i}\max(0,x_iw_j-x_iw_{y_i}+\Delta)\]</span></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scores = np.dot(X,W) - scores[<span class="built_in">range</span>(N),[y]].T + <span class="number">1</span></span><br><span class="line">np.maximum(<span class="number">0</span>,scores)</span><br></pre></td></tr></table></figure><p>因为X(N,D),N是训练集的数据量。W(D,C),C代表图片分类的数量。所以一开始保证维数一致:<spanclass="math display">\[scores = X * W\]</span></p><ul><li><p>np.dot(X,W) shape: (N,C)</p></li><li><p>scores shape: (N,C)</p><ul><li>scores[range(N),[y]].T shape: (N,C) -&gt; (1,N) -&gt; (N,1)</li><li>表示选取每个图片的正确分类，给它们评分函数的相应位置置0，说明损失为0，而其他位置则需要按照SVM的公式计算损失并且和0比较大小。</li></ul></li></ul><p>后面的正则项损失函数部分，</p><p><span class="math display">\[\frac{dL}{dw}(正则项)=2*\lambda*W\]</span></p><p>二维数组的np.sum, shape是( , ) 也就是一个数值。</p><p>第二个部分，梯度求解。用到了链式法则： <span class="math display">\[\frac{dL}{dw}=\frac{dL}{dS}*\frac{dS}{dw}\]</span></p><p><img src="https://s1.ax1x.com/2023/02/04/pSyTWbF.png" /></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">svm_loss_vectorized</span>(<span class="params">W, X, y, reg</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Structured SVM loss function, vectorized implementation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as svm_loss_naive.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros(W.shape)  <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the structured SVM loss, storing the    #</span></span><br><span class="line">    <span class="comment"># result in loss.                                                           #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    N=<span class="built_in">len</span>(y)</span><br><span class="line">    scores = np.dot(X,W)</span><br><span class="line">    scores -= scores[<span class="built_in">range</span>(N),[y]].T</span><br><span class="line">    scores += <span class="number">1</span></span><br><span class="line">    scores[<span class="built_in">range</span>(N),y]=<span class="number">0</span></span><br><span class="line">    margin = np.maximum(<span class="number">0</span>,scores)</span><br><span class="line">    loss = np.<span class="built_in">sum</span>(margin) / N + reg * np.<span class="built_in">sum</span>(np.square(W)) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the gradient for the structured SVM     #</span></span><br><span class="line">    <span class="comment"># loss, storing the result in dW.                                           #</span></span><br><span class="line">    <span class="comment">#                                                                           #</span></span><br><span class="line">    <span class="comment"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span></span><br><span class="line">    <span class="comment"># to reuse some of the intermediate values that you used to compute the     #</span></span><br><span class="line">    <span class="comment"># loss.                                                                     #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    ds = np.zeros_like(margin)</span><br><span class="line">    ds[margin&gt;<span class="number">0</span>]=<span class="number">1</span></span><br><span class="line">    ds[<span class="built_in">range</span>(N),y]-=np.<span class="built_in">sum</span>(ds,axis=<span class="number">1</span>)</span><br><span class="line">    ds /= N</span><br><span class="line">    dW = X.T.dot(ds)</span><br><span class="line">    dW += <span class="number">2</span> * reg * W</span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><h2 id="linear_classifier.py">linear_classifier.py</h2><h3 id="训练-1">训练</h3><ul><li>np.random.choice() 从给定的一维数组生成随机样本。</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#从大小为 3 的 np.arange(5) 生成均匀随机样本：</span></span><br><span class="line">np.random.choice(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">array([<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>]) <span class="comment"># random</span></span><br><span class="line"><span class="comment">#This is equivalent to np.random.randint(0,5,3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#从大小为 3 的 np.arange(5) 生成一个非均匀随机样本：</span></span><br><span class="line">np.random.choice(<span class="number">5</span>, <span class="number">3</span>, p=[<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0</span>])</span><br><span class="line">array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>]) <span class="comment"># random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#从大小为 3 的 np.arange(5) 生成一个统一的随机样本，无需替换，说明没有重复取值：</span></span><br><span class="line">np.random.choice(<span class="number">5</span>, <span class="number">3</span>, replace=<span class="literal">False</span>)</span><br><span class="line">array([<span class="number">3</span>,<span class="number">1</span>,<span class="number">0</span>]) <span class="comment"># random</span></span><br><span class="line"><span class="comment">#This is equivalent to np.random.permutation(np.arange(5))[:3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#从大小为 3 的 np.arange(5) 生成非均匀随机样本，无需替换：</span></span><br><span class="line">np.random.choice(<span class="number">5</span>, <span class="number">3</span>, replace=<span class="literal">False</span>, p=[<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0</span>])</span><br><span class="line">array([<span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>]) <span class="comment"># random</span></span><br></pre></td></tr></table></figure><p>实现train函数，作用是从每一个 iteration 中选出 batch_size个训练样本投入到 SVM 中，然后再计算一次 Loss函数进行梯度下降，避免计算太频繁导致时间消耗过大。有两部分需要补全，第一个是随机选择数据，第二个是梯度下降，实现都比较简单</p><p>梯度下降公式： <imgsrc="https://math.jianshu.com/math?formula=%5Ctheta%20%3D%5Ctheta%20-%5Ceta%20%5Ccdot%20%5Ctriangledown%20_%7B%5Ctheta%20%7DJ%5Cleft%20(%20%5Ctheta%20%5Cright%20)" /></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LinearClassifier</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.W = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        X,</span></span><br><span class="line"><span class="params">        y,</span></span><br><span class="line"><span class="params">        learning_rate=<span class="number">1e-3</span>,</span></span><br><span class="line"><span class="params">        reg=<span class="number">1e-5</span>,</span></span><br><span class="line"><span class="params">        num_iters=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">        batch_size=<span class="number">200</span>,</span></span><br><span class="line"><span class="params">        verbose=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Train this linear classifier using stochastic gradient descent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (N, D) containing training data; there are N</span></span><br><span class="line"><span class="string">          training samples each of dimension D.</span></span><br><span class="line"><span class="string">        - y: A numpy array of shape (N,) containing training labels; y[i] = c</span></span><br><span class="line"><span class="string">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span></span><br><span class="line"><span class="string">        - learning_rate: (float) learning rate for optimization.</span></span><br><span class="line"><span class="string">        - reg: (float) regularization strength.</span></span><br><span class="line"><span class="string">        - num_iters: (integer) number of steps to take when optimizing</span></span><br><span class="line"><span class="string">        - batch_size: (integer) number of training examples to use at each step.</span></span><br><span class="line"><span class="string">        - verbose: (boolean) If true, print progress during optimization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Outputs:</span></span><br><span class="line"><span class="string">        A list containing the value of the loss function at each training iteration.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_train, dim = X.shape</span><br><span class="line">        num_classes = (</span><br><span class="line">            np.<span class="built_in">max</span>(y) + <span class="number">1</span></span><br><span class="line">        )  <span class="comment"># assume y takes values 0...K-1 where K is number of classes</span></span><br><span class="line">        <span class="keyword">if</span> self.W <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># lazily initialize W</span></span><br><span class="line">            self.W = <span class="number">0.001</span> * np.random.randn(dim, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run stochastic gradient descent to optimize W</span></span><br><span class="line">        loss_history = []</span><br><span class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">            X_batch = <span class="literal">None</span></span><br><span class="line">            y_batch = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Sample batch_size elements from the training data and their           #</span></span><br><span class="line">            <span class="comment"># corresponding labels to use in this round of gradient descent.        #</span></span><br><span class="line">            <span class="comment"># Store the data in X_batch and their corresponding labels in           #</span></span><br><span class="line">            <span class="comment"># y_batch; after sampling X_batch should have shape (batch_size, dim)   #</span></span><br><span class="line">            <span class="comment"># and y_batch should have shape (batch_size,)                           #</span></span><br><span class="line">            <span class="comment">#                                                                       #</span></span><br><span class="line">            <span class="comment"># Hint: Use np.random.choice to generate indices. Sampling with         #</span></span><br><span class="line">            <span class="comment"># replacement is faster than sampling without replacement.              #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            bindex = np.random.choice(num_train,batch_size)</span><br><span class="line">            X_batch = X[bindex]</span><br><span class="line">            y_batch = y[bindex]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># evaluate loss and gradient</span></span><br><span class="line">            loss, grad = self.loss(X_batch, y_batch, reg)</span><br><span class="line">            loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># perform parameter update</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Update the weights using the gradient and the learning rate.          #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            self.W = self.W - learning_rate * grad</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;iteration %d / %d: loss %f&quot;</span> % (it, num_iters, loss))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss_history</span><br></pre></td></tr></table></figure><h3 id="预测">预测</h3><p>调参环节，从不同的 learning rate 与 regularization strengths中选出使验证集正确率最高的组合。对每一种组合都训一遍SVM，然后计算一次正确率。不过在 learning rate较大的两个情况训练时，发生了计算溢出的情况。题面中说这是正常现象，正确率接近39%​就算成功。</p><ul><li>np.mean(y_train == y_train_pred)解释：<ul><li>mean是求平均值的意思</li><li>y_train ==y_train_pred意思就是判断训练的值和预测的值是否相同，相等返回1</li><li>将相等的全部加起来/总训练数，就是训练集的准确率了，mean这里就是统计相等的做除法算出准确率的作用</li><li>所以 np.mean(y_train == y_train_pred)就是算训练集准确率的意思</li></ul></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Use the validation set to tune hyperparameters (regularization strength and</span></span><br><span class="line"><span class="comment"># learning rate). You should experiment with different ranges for the learning</span></span><br><span class="line"><span class="comment"># rates and regularization strengths; if you are careful you should be able to</span></span><br><span class="line"><span class="comment"># get a classification accuracy of about 0.39 (&gt; 0.385) on the validation set.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: you may see runtime/overflow warnings during hyper-parameter search. </span></span><br><span class="line"><span class="comment"># This may be caused by extreme values, and is not a bug.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># results is dictionary mapping tuples of the form</span></span><br><span class="line"><span class="comment"># (learning_rate, regularization_strength) to tuples of the form</span></span><br><span class="line"><span class="comment"># (training_accuracy, validation_accuracy). The accuracy is simply the fraction</span></span><br><span class="line"><span class="comment"># of data points that are correctly classified.</span></span><br><span class="line">results = &#123;&#125;</span><br><span class="line">best_val = -<span class="number">1</span>   <span class="comment"># The highest validation accuracy that we have seen so far.</span></span><br><span class="line">best_svm = <span class="literal">None</span> <span class="comment"># The LinearSVM object that achieved the highest validation rate.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Write code that chooses the best hyperparameters by tuning on the validation #</span></span><br><span class="line"><span class="comment"># set. For each combination of hyperparameters, train a linear SVM on the      #</span></span><br><span class="line"><span class="comment"># training set, compute its accuracy on the training and validation sets, and  #</span></span><br><span class="line"><span class="comment"># store these numbers in the results dictionary. In addition, store the best   #</span></span><br><span class="line"><span class="comment"># validation accuracy in best_val and the LinearSVM object that achieves this  #</span></span><br><span class="line"><span class="comment"># accuracy in best_svm.                                                        #</span></span><br><span class="line"><span class="comment">#                                                                              #</span></span><br><span class="line"><span class="comment"># Hint: You should use a small value for num_iters as you develop your         #</span></span><br><span class="line"><span class="comment"># validation code so that the SVMs don&#x27;t take much time to train; once you are #</span></span><br><span class="line"><span class="comment"># confident that your validation code works, you should rerun the validation   #</span></span><br><span class="line"><span class="comment"># code with a larger value for num_iters.                                      #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Provided as a reference. You may or may not want to change these hyperparameters</span></span><br><span class="line">learning_rates = [<span class="number">1e-7</span>, <span class="number">5e-5</span>]</span><br><span class="line">regularization_strengths = [<span class="number">2.5e4</span>, <span class="number">5e4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"><span class="keyword">for</span> rs <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">  <span class="keyword">for</span> lr <span class="keyword">in</span> learning_rates:</span><br><span class="line">    svm = LinearSVM()</span><br><span class="line">    loss_hist = svm.train(X_train,y_train,lr,rs,num_iters=<span class="number">1500</span>)</span><br><span class="line">    y_train_pred = svm.predict(X_train)</span><br><span class="line">    train_accuracy = np.mean(y_train == y_train_pred)</span><br><span class="line">    y_val_pred = svm.predict(X_val)</span><br><span class="line">    val_accuracy = np.mean(y_val == y_val_pred)</span><br><span class="line">    <span class="keyword">if</span> val_accuracy &gt; best_val:</span><br><span class="line">      best_val = val_accuracy</span><br><span class="line">      best_svm = svm</span><br><span class="line">    results[(lr,rs)] = train_accuracy,val_accuracy</span><br><span class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Print out results.</span></span><br><span class="line"><span class="keyword">for</span> lr, reg <span class="keyword">in</span> <span class="built_in">sorted</span>(results):</span><br><span class="line">    train_accuracy, val_accuracy = results[(lr, reg)]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;lr %e reg %e train accuracy: %f val accuracy: %f&#x27;</span> % (</span><br><span class="line">                lr, reg, train_accuracy, val_accuracy))</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;best validation accuracy achieved during cross-validation: %f&#x27;</span> % best_val)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#output result</span></span><br><span class="line">    lr <span class="number">1.000000e-07</span> reg <span class="number">2.500000e+04</span> train accuracy: <span class="number">0.371204</span> val accuracy: <span class="number">0.387000</span></span><br><span class="line">    lr <span class="number">1.000000e-07</span> reg <span class="number">5.000000e+04</span> train accuracy: <span class="number">0.351857</span> val accuracy: <span class="number">0.356000</span></span><br><span class="line">    lr <span class="number">5.000000e-05</span> reg <span class="number">2.500000e+04</span> train accuracy: <span class="number">0.054959</span> val accuracy: <span class="number">0.068000</span></span><br><span class="line">    lr <span class="number">5.000000e-05</span> reg <span class="number">5.000000e+04</span> train accuracy: <span class="number">0.100265</span> val accuracy: <span class="number">0.087000</span></span><br><span class="line">    best validation accuracy achieved during cross-validation: <span class="number">0.387000</span></span><br></pre></td></tr></table></figure><h3 id="inline-question-1-1"><strong>Inline Question 1</strong></h3><p>It is possible that once in a while a dimension in the gradcheck willnot match exactly. What could such a discrepancy be caused by? Is it areason for concern? What is a simple example in one dimension where agradient check could fail? How would change the margin affect of thefrequency of this happening? <em>Hint: the SVM loss function is notstrictly speaking differentiable</em></p><p><span class="math inline">\(\color{blue}{\textit YourAnswer:}\)</span></p><p>It is possible that the numerical gradient does not match the actualgradient, because the max function is non-linear, continuous at 0 butnot derivable, so the numerical gradient is inaccurate at thissituation.</p><h3 id="inline-question-2-1"><strong>Inline question 2</strong></h3><p>Describe what your visualized SVM weights look like, and offer abrief explanation for why they look the way they do.</p><p><span class="math inline">\(\color{blue}{\textit YourAnswer:}\)</span></p><p>Each class of weighted visual image shows roughly the shape of theobjects in that class as well as the background colour. When an imagehas a shape or background colour similar to that class, there is a highprobability that it will be classified as such.</p><h2 id="svm部分参考链接">SVM部分参考链接:</h2><ol type="1"><li><p>cs231n官网: <ahref="https://cs231n.github.io/">https://cs231n.github.io/</a></p></li><li><p>深度学习课程 CS231n Assignment1 SVM部分: <ahref="http://marvolo.top/archives/17202">http://marvolo.top/archives/17202</a></p></li><li><p>CS231-Multi-calss SVM的求导: <ahref="https://www.cnblogs.com/chenyusheng0803/p/10018306.html">https://www.cnblogs.com/chenyusheng0803/p/10018306.html</a></p></li><li><p>机器学习算法：梯度下降法——原理篇 <ahref="https://www.jianshu.com/p/424b7b70df7b">https://www.jianshu.com/p/424b7b70df7b</a></p></li><li><p>CS231N作业详解零基础版： <ahref="https://www.bilibili.com/video/BV19z411b7u9/?p=9&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a">https://www.bilibili.com/video/BV19z411b7u9/?p=9&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a</a></p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;cs231n-assignment-1&quot;&gt;CS231n Assignment 1&lt;/h1&gt;
&lt;p&gt;作业1分为五个部分：KNN、SVM、Softmax classifier、2层神经网络、Higher
Level Representations: Image F</summary>
      
    
    
    
    <category term="笔记" scheme="https://le-ying.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="介绍" scheme="https://le-ying.github.io/tags/%E4%BB%8B%E7%BB%8D/"/>
    
    <category term="深度学习" scheme="https://le-ying.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="cs231n" scheme="https://le-ying.github.io/tags/cs231n/"/>
    
    <category term="作业" scheme="https://le-ying.github.io/tags/%E4%BD%9C%E4%B8%9A/"/>
    
  </entry>
  
  <entry>
    <title>Transformer 零基础解析教程，从Encoder-Decoder架构说起（1/4）</title>
    <link href="https://le-ying.github.io/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/"/>
    <id>https://le-ying.github.io/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/</id>
    <published>2022-12-14T01:42:13.000Z</published>
    <updated>2023-02-14T11:50:42.057Z</updated>
    
    <content type="html"><![CDATA[<h1 id="transformer教程系列介绍">Transformer教程系列介绍</h1><p>大模型的发展正在逐渐从单一模态数据输入向多模态数据输入演进，文本、语音、图像、视频等多模态的联合训练学习，不同模态之间形成有效互补，这将有助于提升模型的效果和泛化能力，为迈向通用人工智能奠定更加坚实的一步。而提到多模态的算法模型，就不得不提到大名鼎鼎的Transformer。</p><p>2017年, <strong>Google研究团队</strong>发表了论文 <strong>《AttentionIs All YouNeed》</strong>，Transformer横空出世，并很快成为<strong>自然语言处理(NLP)</strong>领域的标杆模型，如机器翻译及后续的BERT、GPT系列等NLP大模型。Transformer也被应用到<strong>计算机视觉(CV)</strong>领域，用于图像分类、物体检测、图像生成和视频处理等任务，代表性如DERT、ViT等。此外，Transforme也被应用到了<strong>语音领域(ASR)</strong>，用于语音识别、语音合成、语音增强和音乐生成等任务。此外，NLP、视觉和语音构成的<strong>多模态</strong>场景，也是近年来Transformer应用的热点方向，例如视觉问答、视觉常识推理、语音到文本翻译和文本到图像生成等。</p><p><strong>本文主要讲解Transformer之前的两个技术基础：Encoder-Decoder框架和 Attention机制。</strong>已经掌握Encoder-Decoder和Attention基础的读者可以直接阅读后续篇章。</p><blockquote><p>本博客的 Transformer 系列文章共计四篇，导航如下：</p></blockquote><ul><li><p><ahref="https://serika-onoe.github.io/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/">Transformer零基础解析教程，从Encoder-Decoder架构说起（1/4）<strong>本篇</strong></a></p></li><li><p><ahref="https://serika-onoe.github.io/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/">Transformer零基础解析教程，剥洋葱般层层剖析内在原理（2/4）</a></p></li><li><p><ahref="https://serika-onoe.github.io/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/">Transformer零基础解析教程，牛刀小试Pytorch简易版攻略（3/4）</a></p></li><li><p><ahref="https://serika-onoe.github.io/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/">Transformer零基础解析教程，完整版代码最终挑战（4/4）</a></p></li></ul><h1 id="前言">前言</h1><blockquote><p>We propose a new simple network architecture, the<strong>Transformer</strong>, based solely on <strong>attentionmechanisms</strong>, dispensing with recurrence and convolutionsentirely.</p><p><em>Reference: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I. (2017). </em><br><em>Attention is all you need.</em></p></blockquote><p>结合论文原文的初始定义，我们可以简单划分 Encoder-Decoder, Attention,Transformer 三者的关系：</p><table><colgroup><col style="width: 50%" /><col style="width: 50%" /></colgroup><thead><tr class="header"><th style="text-align: center;">概念</th><th style="text-align: center;">说明</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Encoder-Decoder</td><td style="text-align: center;">解决序列-序列问题的框架</td></tr><tr class="even"><td style="text-align: center;">Attention</td><td style="text-align: center;">改善Encoder-Decoder输出结果的机制</td></tr><tr class="odd"><td style="text-align: center;">Transformer</td><tdstyle="text-align: center;">用于解决序列-序列问题<br>使用了Encoder-Decoder框架和Attention机制</td></tr></tbody></table><h1 id="encoder-decoder框架">Encoder-Decoder框架</h1><h2 id="简介">简介</h2><p>对于自然语言处理(NLP)领域的典型问题，都可以简化为处理一个句子对&lt;Source,Target&gt;，比如：</p><ul><li>文本摘要，Source是一篇文章，Target是概括性的几句描述语句</li><li>文本翻译，Source是待翻译的句子，Target是翻译后的句子</li><li>问答系统，Source是一段提问，Target是一段回答</li></ul><p>我们的目标是给定输入句子<code>Source</code>，通过Encoder-Decoder框架，最后生成目标句子<code>Target</code>。Source和Target分别由各自的单词序列构成。这其中的Encoder-Decoder是一种用于处理序列-序列问题的框架，编码器(<code>Encoder</code>)输入一个序列并输出一个编码，解码器(<code>Decoder</code>)使用这个编码来生成一个输出序列。</p><p>Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。区别在于，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。用序列对&lt;Source,Target&gt;举例来说：</p><ul><li>语音识别，Source是一段语音流，Target是对应的文本信息</li><li>图像描述，Source是一幅图片，Target是图片内容的描述语。</li></ul><h2 id="原理">原理</h2><p>举个更具体的例子，我们以翻译给定的输入句子<spanclass="math inline">\(X\)</span>为例，通过Encoder-Decoder框架，最后生成目标句子<spanclass="math inline">\(Y\)</span>。其中<spanclass="math inline">\(X\)</span>和<spanclass="math inline">\(Y\)</span>分别由单词序列构成</p><ul><li>原始句子 <span class="math inline">\(X = (x_1, x_2, \cdots,x_m)\)</span></li><li>翻译的目标句子 <span class="math inline">\(Y = (y_1, y_2, \cdots,y_n)\)</span></li></ul><p>Encoder任务就是对输入句子<spanclass="math inline">\(X\)</span>进行编码，将输入句子通过非线性变换转化为中间语义表示<spanclass="math inline">\(C\)</span>：</p><p><span class="math display">\[C = F(x_1, x_2, \cdots, x_m)\]</span></p><p>Decoder任务就是根据句子<spanclass="math inline">\(X\)</span>的中间语义表示<spanclass="math inline">\(C\)</span>和之前已经生成的历史信息<spanclass="math inline">\(y_1, y_2, \cdots,y_{i-1}\)</span>来生成i时刻要生成的单词<spanclass="math inline">\(y_i\)</span>。</p><p><span class="math display">\[y_i = G(C, y_1, y_2, \cdots, y_{i-1})\]</span></p><p>每个<spanclass="math inline">\(y_i\)</span>都依次这么产生，最终看起来就是整个系统根据输入句子<spanclass="math inline">\(X\)</span>生成了目标句子<spanclass="math inline">\(Y\)</span>。</p><p>Encoder-Decoder是通用的计算框架，Encoder,Decoder具体用什么模型，都可以自己选择。<code>(因此这可以是创新点)</code></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/encoder-decoder.jpg"title="Encoder-Decoder架构图" /></p><center><code>图1: Encoder-Decoder架构图</code></center><h2 id="经典decoder形式及其问题">经典Decoder形式及其问题</h2>其中经典的Decoder有两种形式，对应两篇论文：<br /><strong>[论文1]</strong>: <ahref="https://arxiv.org/abs/1406.1078">Kyunghyun Cho, Bart vanMerrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, Yoshua Bengio: “Learning Phrase Representations using RNNEncoder-Decoder for Statistical Machine Translation”, 2014;[http://arxiv.org/abs/1406.1078 arXiv:1406.1078].</a><br /><imgsrc="https://img-blog.csdnimg.cn/20200322154916564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /><br /><center><code>图2: 输入序列的语义编码C均等影响输出序列</code></center><p><br> 论文1中指出，Ecoder、Decoder均使用了RNN，因为语义编码<spanclass="math inline">\({C}\)</span>包含了整个输入序列的信息，所以在计算每一时刻的输出<spanclass="math inline">\(y_t\)</span>时，都应该输入语义编码<spanclass="math inline">\({C}\)</span>，也就是在解码的每一步都引入输入信息<spanclass="math inline">\({C}\)</span>。下面用公式表达：</p><ul><li>Decoder中<span class="math inline">\(t\)</span>时刻的内部状态的<spanclass="math inline">\(h_{&lt;t&gt;}\)</span>为:</li></ul><p><spanclass="math display">\[h_{&lt;t&gt;}=f(h_{t-1},y_{t-1},C)\]</span></p><ul><li><span class="math inline">\(t\)</span>时刻的输出概率为</li></ul><p><spanclass="math display">\[p(y_t|y_{t-1},y_{t-2},\cdots,y_{1},C)=g(h_{&lt;t&gt;},y_{t−1},C)\]</span></p><p><span class="math inline">\(h_t\)</span>为当前<spanclass="math inline">\(t\)</span>时刻的隐藏层的值，<spanclass="math inline">\(y_{t-1}\)</span>为上一时刻的预测输出。</p><strong>[论文2]</strong>: <a href="https://arxiv.org/abs/1409.3215">IlyaSutskever, Oriol Vinyals, Quoc V. Le: “Sequence to Sequence Learningwith Neural Networks”, 2014; [http://arxiv.org/abs/1409.3215arXiv:1409.3215].</a><br /><img src="https://img-blog.csdnimg.cn/20200322162508369.png" /><br /><center><code>图3: 只在Decoder的初始输入引入语义编码C</code></center><p><br/><br/>论文2的方式相对简单，只在Decoder的初始输入引入语义编码<spanclass="math inline">\({C}\)</span>，将语义编码<spanclass="math inline">\({C}\)</span>作为隐藏层状态值<spanclass="math inline">\(h_{&lt;0&gt;}\)</span>的初始值，公式如下：</p><p><span class="math display">\[h_{&lt;0&gt;}=C\]</span></p><p><spanclass="math display">\[h_{&lt;t&gt;}=f(h_{&lt;t-1&gt;},y_{t-1})\]</span></p><p><spanclass="math display">\[p(y_t)=g(h_{&lt;t&gt;},y_{t−1})\]</span></p><p>这两种的Encoder-Decoder结构至少存在以下几个问题：</p><ol type="1"><li><p>如果按照论文1解码，意味着输入序列<spanclass="math inline">\(X\)</span>中任意单词对生成某个目标单词<spanclass="math inline">\(y_i\)</span>来说影响力都是相同的（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，所以这种情况下有时逆序输入得到的准确率可能更高）</p></li><li><p>如果按照论文2解码，意味着输入序列的所有信息压缩在了一个语义编码<spanclass="math inline">\(C\)</span>中。如果输入序列过短，导致<spanclass="math inline">\(C\)</span>包含的信息不足，那么生成的输出序列可能不够准确。如果输入序列很长，那么Decoder在生成输出序列时可能会丢失重要信息，还可能出现梯度消失问题。</p></li></ol><h2 id="问题的简单例子">问题的简单例子</h2><p>相比Attention模型，原始的Encoder-Decoder模型可以看作是注意力不集中的模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下：</p><p><span class="math display">\[y_i = G(C, y_1, y_2, \cdots, y_{i-1})\]</span><br />代入具体值，即为<br /><span class="math display">\[y_1 = G(C)\]</span><br /><span class="math display">\[y_2 = G(C, y_1)\]</span><br /><span class="math display">\[y_3 = G(C, y_1, y_2)\]</span></p><p>其中<spanclass="math inline">\(f\)</span>是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码<spanclass="math inline">\(C\)</span>都是一样的，没有任何区别。</p><p>拿机器翻译来解释这个模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：<code>Tom chase Jerry</code>，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。</p><p>在翻译“杰瑞”这个中文单词的时候，模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但该模型无法体现这一点。</p><p>既然只引入一个<spanclass="math inline">\(C\)</span>不行，那咱们就用多个<spanclass="math inline">\(C\)</span>，让不同的语义编码<spanclass="math inline">\(C_i\)</span>对应生成目标句子的不同单词。这个时候我们就引入了改善Encoder-Decoder输出结果的<code>Attention</code>机制。</p><h1 id="attention机制">Attention机制</h1><h2 id="简介-1">简介</h2><p>总结一下前文：在文本翻译的Encoder-Decoder模型中，编码器会输出一个向量序列，而解码器则需要逐步生成目标语言的单词。其中的每一步，解码器都需要根据之前生成的单词和编码器的输出来预测下一个单词，而对于编码器的输出，解码器缺少关注的重点。为了让解码器能够更好地使用编码器的输出，我们可以使用Attention 机制。</p><p>Attention机制可以让解码器在每一步时重点关注编码器输出的某些部分，这样解码器就可以更准确地生成目标语言的单词。同时，编码器也可以使用Attention机制。在编码器的每一层中，它会使用自注意力机制来注意输入序列的不同部分。</p><h2 id="原理-1">原理</h2><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" /><center><code>图4: 引入了Attention机制的Encoder-Decoder框架</code></center><p><br/><br/>上图就是引入了Attention机制的Encoder-Decoder框架，我们一眼就能看出它不再只有一个单一的语义编码<spanclass="math inline">\(C\)</span>，而是有多个<spanclass="math inline">\(C_1,C_2,C_3\)</span>这样的编码。当我们在预测<spanclass="math inline">\(Y_1\)</span>时，可能<spanclass="math inline">\(Y_1\)</span>的注意力是放在<spanclass="math inline">\(C_1\)</span>上，那咱们就用<spanclass="math inline">\(C_1\)</span>作为语义编码，当预测<spanclass="math inline">\(Y_2\)</span>时，<spanclass="math inline">\(Y_2\)</span>的注意力集中在<spanclass="math inline">\(C_2\)</span>上，那咱们就用<spanclass="math inline">\(C_2\)</span>作为语义编码，以此类推...</p><h3id="问题一关于注意力应该如何分配">问题一：关于注意力应该如何分配</h3><blockquote><p>如何判断每次在做解码的时候，注意力应该放在输入序列的哪个位置呢？即怎么计算出<spanclass="math inline">\(C1，C2，...Cn\)</span>呢？</p></blockquote><p>我们可以假设，输入序列的每个单词对于翻译“杰瑞”的<strong>注意力应该不一样</strong>，如翻译“杰瑞”时：<span class="math display">\[(Tom, 0.3), \;  (Chase, 0.2), \;  (Jerry, 0.5)\]</span>每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标单词是会有帮助的。因为针对输入的每个单词，输出的不同位置都引入了新的信息，并且放大了对应输入部分的注意力概率。</p><p>而这样做的前提在于，目标句子中的每个单词都应该学会分配对应的源句子中每个单词的注意力概率信息。也就是说，在生成每个单词<spanclass="math inline">\(Y_i\)</span>的时候，源句子中对应的相关单词概率会高于无关的单词。</p><blockquote><p>个人理解：Attention的意思就是让模型学会在输出下一个单词时，对输入序列与之相关的单词投入更多的注意力，从而实现了将相同的中间语义表示<spanclass="math inline">\(C\)</span>替换成了根据当前生成单词而不断变化的<spanclass="math inline">\(C_i\)</span>。（<strong>这是Attention机制的关键</strong>）</p></blockquote><p>具体的实现过程举例如下：</p><p>请观察下目标句子Target中每个单词的生成过程，和原模型相比，<spanclass="math inline">\(C\)</span>变成了<spanclass="math inline">\(C_i\)</span>：</p><p><span class="math display">\[y_i = G(C_i, y_1, y_2, \cdots, y_{i-1})\]</span><br />代入具体值，即为<br /><span class="math display">\[y_1 = G(C_1)\]</span><br /><span class="math display">\[y_2 = G(C_2, y_1)\]</span><br /><span class="math display">\[y_3 = G(C_3, y_1, y_2)\]</span></p><p>那么对于刚才的例子<code>“汤姆追逐杰瑞”</code>来说，<spanclass="math inline">\(C_i\)</span>可以表示为：</p><p><imgsrc="https://img-blog.csdnimg.cn/20200322173132643.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /></p><ul><li><spanclass="math inline">\(f2\)</span>函数代表Encoder对输入英文单词的某种变换函数，如果Encoder是用的RNN模型的话，这个<spanclass="math inline">\(f2\)</span>函数的结果往往是某个时刻输入<spanclass="math inline">\(x_i\)</span>后隐藏层节点的状态值</li><li><spanclass="math inline">\(g\)</span>函数代表Encoder根据单词的中间表示，合成整个句子中间语义表示的变换函数，一般的做法中，<spanclass="math inline">\(g\)</span>函数就是对构成元素加权求和，即下列公式：<span class="math display">\[C_i = ∑_{j=1}^{Lx}a_{ij}h_j\]</span><br /></li><li><ul><li><p><spanclass="math inline">\(L_x\)</span>代表输入句子Source的长度</p></li><li><p><spanclass="math inline">\(a_{ij}\)</span>代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数</p></li><li><p><spanclass="math inline">\(h_j\)</span>是Source输入句子中第j个单词的语义编码。</p><p><strong>假设<spanclass="math inline">\(C_i\)</span>指的就是上面例子所说的“<spanclass="math inline">\(C_{汤姆}\)</span>”</strong></p></li><li><p><span class="math inline">\(L_x=3\)</span></p></li><li><p><span class="math inline">\(h_1=f2(&#39;Tom&#39;),h_2=f2(&#39;Chase&#39;), h_3=f2(&#39;Jerry&#39;)\)</span></p></li><li><p><span class="math inline">\(a_{11}=0.6, a_{12}=0.2,a_{13}=0.2\)</span></p></li></ul>由此可见，我们通过一个加权求和函数，计算出<spanclass="math inline">\(C_i\)</span>，也就是不同解码位置对应的注意力概率分配，在这个例子中这个函数记作<spanclass="math inline">\(g\)</span>函数，如下图：</li></ul><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention2.jpg" /><center><code>图5: g函数过程可视化</code></center><h3id="问题二关于具体注意力概率的计算">问题二：关于具体注意力概率的计算</h3><blockquote><p>生成目标句子某个单词的时候，我们如何知道这个单词的注意力概率计算值各是多少呢？</p><p>比如在生成“汤姆”这个中文翻译词的时候，模型生成了“汤姆”对应的输入句子Source中各个单词的概率分布：<spanclass="math display">\[(Tom,0.6)(Chase,0.2) (Jerry,0.2)\]</span>这个分布是如何得到的呢？</p></blockquote><p>为了便于说明，我们假设对初始的Encoder-Decoder框架进行细化，Encoder,Decoder都采用RNN模型，这是比较常见的一种模型配置，架构图如下。</p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention3.jpg" /><center><code>图6: 初始Encoder-Decoder常用框架</code></center><p><br/><br/></p><p>对于采用RNN的Decoder来说，在时刻<spanclass="math inline">\(i\)</span>，如果要生成<spanclass="math inline">\(y_i\)</span>单词，我们是可以知道Target在生成<spanclass="math inline">\(y_i\)</span>之前的时刻 <spanclass="math inline">\(i-1\)</span> 时，隐层节点 <spanclass="math inline">\(i-1\)</span> 时刻的输出值<spanclass="math inline">\(H_{i-1}\)</span>的，而我们的目的是要计算生成<spanclass="math inline">\(y_i\)</span>时输入句子中的单词“Tom”、“Chase”、“Jerry”对<spanclass="math inline">\(y_i\)</span>来说的注意力分配概率分布，那么可以用Target输出句子<span class="math inline">\(i-1\)</span> 时刻的隐层节点状态<spanclass="math inline">\(H_{i-1}\)</span>去和输入句子Source中每个单词对应的RNN隐层节点状态<spanclass="math inline">\(h_j\)</span>进行对比，即<strong>通过函数<spanclass="math inline">\(F(h_j,H_{i-1})\)</span>来获得目标单词<spanclass="math inline">\(y_i\)</span>和每个输入单词对应的对齐可能性</strong>，（这个<spanclass="math inline">\(F\)</span>函数在不同论文里可能会采取不同的方法），<strong>然后函数<spanclass="math inline">\(F\)</span>的输出经过Softmax进行归一化，就得到了符合概率分布取值区间的注意力分配概率分布数值</strong>。比如<code>“汤姆追逐杰瑞”</code>的例子可以表示如下</p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention4.jpg" /><center><code>图7: “汤姆追逐杰瑞”的例子中“Tom”的注意力分配系数计算过程</code></center><p><br/><br/> 注意一下，这里咱们使用的是<strong>SoftAttention</strong>，所有的数据都会被注意到，并计算出相应的注意力权值，不会设置筛选条件。还有一种<strong>HardAttention</strong>，会在生成注意力权重后筛选掉一部分不符合条件的注意力，让它的注意力权值为0，即可以理解为不再注意这些不符合条件的部分。</p><h2 id="本质">本质</h2><p>如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p><p><strong>[论文3]</strong>: <ahref="https://arxiv.org/abs/1706.03762">Ashish Vaswani, Noam Shazeer,Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, Illia Polosukhin: “Attention Is All You Need”, 2017;[http://arxiv.org/abs/1706.03762 arXiv:1706.03762].</a>这篇论文讨论了使用Query, Key和Value向量计算Attention的方法。</p><blockquote><p>Q, K, V向量分别代表什么，为什么要使用这三个向量？</p></blockquote><p>Attention简单来说就是一些值的加权平均值，就像“汤姆追逐杰瑞”例子中提到的：</p><p><span class="math display">\[C_i = ∑_{j=1}^{Lx}a_{ij}h_j\]</span></p><ul><li><spanclass="math inline">\(L_x\)</span>代表输入句子Source的长度</li><li><spanclass="math inline">\(a_{ij}\)</span>代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数</li><li><spanclass="math inline">\(h_j\)</span>是Source输入句子中第j个单词的语义编码。</li></ul><p>Transformer的关键思想是Attention机制。相比于循环网络（如 RNN/LSTM）必须一个接一个地查看每个单词（因此存在长期记忆问题），Transformer不仅考虑来自一个方向的输入，而且从整体上可以同时看到输入中的所有标记，以确定每个单词应该“注意”哪些内容。<strong>也就是说，我们期待给定一个输入Source之后，输出能同时看到所有的输入信息，然后每个输出的位置根据不同权重选择自己的注意点做输出。</strong></p><p>这个类似于搜索引擎或者推荐系统的基本原理，以谷歌Google为例:</p><ul><li>用户给定需查询的问题(Query)</li><li>Google后台有各种文章标题(Key)和文章本身(Value)</li></ul><p>通过定义并计算问题Query与文章标题Key的相关性，用户就可以找到最匹配的文章Value。当然Attention的目标不是通过Query得到Value，而是通过Query得到Value的加权和。因此我们可以认为，<strong>Attention实际上做的就是数据库中的检索操作</strong><em>(Attention is actually working as a retrieval operation in adatabase. )</em></p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention_bz1.jpg" /><center><code>图8: Attention机制的本质思想</code></center><p><br/><br/>所以我们可以这样来看待Attention机制：我们的输入、输出记为句子对&lt;Source,Target&gt;。将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以<strong>本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。</strong>即可以将其本质思想改写为如下公式：</p><p><img src="https://img-blog.csdnimg.cn/20200322195829296.png" /></p><ul><li><span class="math inline">\(L_x=||Source||\)</span>代表Source的长度</li></ul><p>Key和Value具有强关联性，所以它们也可以是同一个东西，比如Google搜索理论上可以直接搜索到文章本身，而不用列出文章的标题列表。再比如“汤姆追逐杰瑞”的翻译例子里，Key和Value指向的都是同一个东西：输入句子中每个单词对应的语义编码<spanclass="math inline">\(C_i\)</span>。</p><p>当然，从概念上把Attention理解为从大量信息中有选择地筛选出少量重要信息，并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p><p>刚才提到“Attention实际上做的就是数据库中的检索操作”，因此也可以用寻址类比检索过程：我们将Attention机制看作一种软寻址(SoftAddressing):Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个<spanclass="math inline">\(Key=Query\)</span>的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，是因为不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。</p><p>我忽然想到可以用日常的例子来类比，比如解密型的密室逃脱游戏，玩家需要像侦探一样根据提示，在不同的地方找到一个个线索或道具，每个线索都可能和最终的逃脱方法有关，（当然越后面阶段拿到的线索或者道具有可能权重越高），最终合理运用推理逃出生天。</p><h2 id="计算过程">计算过程</h2><p>Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：</p><ol type="1"><li>根据Query和Key计算权重系数<ul><li>根据Query和Key计算两者的相似性或者相关性</li><li>对相似性的原始分值进行归一化处理，得到</li></ul></li><li>根据权重系数对Value进行加权求和。</li></ol><p>我们也可以把上述的第一个过程细化为两个阶段。因此三个阶段分别为</p><ul><li>阶段1：Query与每一个Key计算相似性得到相似性评分<spanclass="math inline">\(s1, s2, s3, \cdots, s_i\)</span></li><li>阶段2：将<span class="math inline">\(s1, s2, s3, \cdots,s_i\)</span>评分进行softmax转换成[0,1]之间的概率分布</li><li>阶段3：将<span class="math inline">\([a1, a2, a3, \cdots,a_n]\)</span>作为权值矩阵对Value进行加权求和，得到最后的Attention值</li></ul><p>如下图所示，</p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention_bz2.jpg" /><center><code>图9: 图解Attention机制的计算过程</code></center><p><br/><br/></p><h3 id="阶段1">阶段1</h3><p>在阶段1，可以引入不同的函数和计算机制，根据Query和某个<spanclass="math inline">\(Key_i\)</span>，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值：</p><p><img src="https://img-blog.csdnimg.cn/20200322200849586.png" /></p><h3 id="阶段2">阶段2</h3><p>阶段1产生的分值根据具体产生的方法不同，其结果数值取值范围也不一样，因此阶段2引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><p><img src="https://img-blog.csdnimg.cn/20200322205913297.png" /></p><h3 id="阶段3">阶段3</h3><p>阶段2的计算结果<span class="math inline">\(a_i\)</span>即为<spanclass="math inline">\(Value_i\)</span>对应的权重系数，然后阶段3进行加权求和即可得到Attention数值：</p><p><img src="https://img-blog.csdnimg.cn/20200322210027426.png" /></p><p>通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p><h2 id="优缺点">优缺点</h2><p>Attention是一种常用的解决长序列处理问题的技术，不过它真的无懈可击吗？</p><p><strong>优点：</strong></p><ol type="1"><li><p><strong>可并行计算：</strong>Attention机制解决了RNN不能并行计算的问题。这里需要说明一下，咱们在训练Attention机制的seq2seq模型的时候，decoder并不是说预测出了一个词，然后再把这个词作为下一个输入，而是有监督训练：咱们已经有了target的数据，所以是可以并行输入的，可以并行计算decoder的每一个输出，但是实际做预测的时候，是没有target数据的，这个时候就需要基于上一个时间节点的预测值来当做下一个时间节点decoder的输入。所以节省的是训练的时间。</p></li><li><p><strong>改善长序列处理能力：</strong>Attention机制可以帮助模型在处理长序列时，选择关注其中重要的部分，从而提高模型的准确性。</p></li><li><p><strong>模型权重可视化：</strong>Attention机制通过计算权重系数，能够得到每个输入序列中元素对输出结果的重要性，将权重系数可视化比以往方法有助于我们理解模型学到了什么。</p></li></ol><p><strong>缺点：</strong></p><p>1.<strong>Encoder部分未实现并行运算：</strong>Encoder部分依旧采用的是RNN，LSTM这些按照顺序编码的模型，不够完美。</p><p>2.<strong>计算复杂度高：</strong>Attention机制通常需要对整个输入序列进行遍历，计算量大，尤其是在处理长序列时。</p><h3 id="改进self-attention">改进：Self Attention</h3><p>为了改进上面两个缺点，更加完美的Self-Attention出现了。</p><p>在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素和Source中的所有元素之间。而SelfAttention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为<spanclass="math inline">\(Target=Source\)</span>这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，相当于是<spanclass="math inline">\(Query=Key=Value\)</span>，计算过程与attention一样，所以这里不再赘述其计算过程细节。</p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/self_attention.jpg" /><center><code>图9: self-attention可以帮助模型理解代词的含义</code></center><p><br/><br/>上图是self-attention的一个例子。我们想知道这句话中的its，在这句话里its指代的是什么，与哪一些单词相关，那么就可以将its作为Query，然后将这一句话作为Key和Value来计算attention值，找到与这句话中its最相关的单词。通过self-attention我们发现its在这句话中与之最相关的是Law和application，通过我们分析语句意思也十分吻合。</p><p>如此引入SelfAttention后，会更容易捕获句子中长距离的依赖关系，因为如果是RNN或者LSTM，需要依次序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。但是SelfAttention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短。</p><p>除此外，Self Attention还做了其他的改进，主要如下：</p><ol type="1"><li><p>将输入序列中每个元素和其他元素之间的关系考虑进来，从而更好的理解语境。</p></li><li><p>通过添加多头机制来提高模型的泛化能力，从而增加了Encoder计算的并行性。</p></li><li><p>通过添加位置编码来确保序列元素之间的相对顺序对于结果的影响。</p></li></ol><p>关于这部分内容，我会在下篇文章详细讲解SelfAttention的工作原理，以及它在Transformer中Encoder的应用。</p><h1 id="结束语">结束语</h1><p>在这篇文章中，我们已经了解了 Encoder-Decoder 和 Attention的概念，这些概念在自然语言处理中非常有用。不过，我们希望这些模型不仅仅可以用于处理自然语言，还可以用在更多的领域，比如我的最终目的是使用Transformer来预测时序数据，因此Transformer的技术原理将会是我在下一篇文章的重点。且看它如何在Encoder-decoder 和 Attention 的基础上开疆拓土，让我们拭目以待！</p><h1 id="参考链接">参考链接</h1><h2 id="参考网站">参考网站</h2><ol type="1"><li><p>Tensorflow的官方教程：Neural machine translation with aTransformer and Keras:</p><p><ahref="https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb">https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb</a></p></li><li><p>史上最小白之Attention详解:</p><p><ahref="https://blog.csdn.net/Tink1995/article/details/105012972">https://blog.csdn.net/Tink1995/article/details/105012972</a></p></li><li><p>史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！:</p><p><ahref="https://zhuanlan.zhihu.com/p/148656446">https://zhuanlan.zhihu.com/p/148656446</a></p></li><li><p>Encoder-Decoder Models for Natural Language Processing</p><p><ahref="https://www.baeldung.com/cs/nlp-encoder-decoder-models">https://www.baeldung.com/cs/nlp-encoder-decoder-models</a></p></li><li><p>ChatGPT3：</p><p><ahref="https://chat.openai.com/chat">https://chat.openai.com/chat</a></p></li><li><p>自然语言处理中的Attention Model：是什么以及为什么[一]：</p><p><ahref="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247484196&amp;idx=1&amp;sn=efa6b79d24b138ff79f33ec3426c79e2&amp;chksm=ebb43bf0dcc3b2e6e69ff3b7d7cabf28744e276ee08ccfcfc1dc2c3f0bf52a122c9bd77ff319&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247484196&amp;idx=1&amp;sn=efa6b79d24b138ff79f33ec3426c79e2&amp;chksm=ebb43bf0dcc3b2e6e69ff3b7d7cabf28744e276ee08ccfcfc1dc2c3f0bf52a122c9bd77ff319&amp;scene=21#wechat_redirect</a></p></li><li><p>Query, Key and Value in Attention mechanism</p><p><ahref="https://lih-verma.medium.com/query-key-and-value-in-attention-mechanism-3c3c6a2d4085">https://lih-verma.medium.com/query-key-and-value-in-attention-mechanism-3c3c6a2d4085</a></p></li><li><p>如何理解 Transformer 中的 Query、Key 与 Value</p><p><ahref="https://blog.csdn.net/yangyehuisw/article/details/116207892">https://blog.csdn.net/yangyehuisw/article/details/116207892</a></p></li><li><p>深度学习中的注意力机制(2017版)</p><p><ahref="https://blog.csdn.net/malefactor/article/details/78767781">https://blog.csdn.net/malefactor/article/details/78767781</a></p></li><li><p>Transformer打开了通用人工智能的一扇门？</p><p><ahref="https://maimai.cn/article/detail?fid=1736575591&amp;efid=G3Qsw_4kvsfAXWUIsSeeig">https://maimai.cn/article/detail?fid=1736575591&amp;efid=G3Qsw_4kvsfAXWUIsSeeig</a></p></li></ol><h2 id="参考文献">参考文献</h2><p><strong>[论文1]</strong>: <ahref="https://arxiv.org/abs/1406.1078">Kyunghyun Cho, Bart vanMerrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, Yoshua Bengio: “Learning Phrase Representations using RNNEncoder-Decoder for Statistical Machine Translation”, 2014;[http://arxiv.org/abs/1406.1078 arXiv:1406.1078].</a></p><p><strong>[论文2]</strong>: <ahref="https://arxiv.org/abs/1409.3215">Ilya Sutskever, Oriol Vinyals,Quoc V. Le: “Sequence to Sequence Learning with Neural Networks”, 2014;[http://arxiv.org/abs/1409.3215 arXiv:1409.3215].</a></p><p><strong>[论文3]</strong>: <ahref="https://arxiv.org/abs/1706.03762">Ashish Vaswani, Noam Shazeer,Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, LukaszKaiser, Illia Polosukhin: “Attention Is All You Need”, 2017;[http://arxiv.org/abs/1706.03762 arXiv:1706.03762].</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;transformer教程系列介绍&quot;&gt;Transformer教程系列介绍&lt;/h1&gt;
&lt;p&gt;大模型的发展正在逐渐从单一模态数据输入向多模态数据输入演进，文本、语音、图像、视频等多模态的联合训练学习，不同模态之间形成有效互补，这将有助于提升模型的效果和泛化能力，为迈</summary>
      
    
    
    
    <category term="笔记, 教程" scheme="https://le-ying.github.io/categories/%E7%AC%94%E8%AE%B0-%E6%95%99%E7%A8%8B/"/>
    
    
    <category term="科研" scheme="https://le-ying.github.io/tags/%E7%A7%91%E7%A0%94/"/>
    
    <category term="transformer" scheme="https://le-ying.github.io/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫爬取国家统计局数据</title>
    <link href="https://le-ying.github.io/2021/01/13/Python%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E5%9B%BD%E5%AE%B6%E7%BB%9F%E8%AE%A1%E5%B1%80%E6%95%B0%E6%8D%AE/"/>
    <id>https://le-ying.github.io/2021/01/13/Python%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E5%9B%BD%E5%AE%B6%E7%BB%9F%E8%AE%A1%E5%B1%80%E6%95%B0%E6%8D%AE/</id>
    <published>2021-01-12T16:12:47.000Z</published>
    <updated>2022-12-12T14:35:22.494Z</updated>
    
    <content type="html"><![CDATA[<h1 id="python-爬虫爬取国家统计局数据">Python爬虫爬取国家统计局数据</h1><p><strong>本次实验以爬取“<ahref="https://data.stats.gov.cn/index.htm">国家统计局</a>”首页中的【上海市城乡居民收支基本情况】为例，国家统计局其他页面的爬取方法大同小异</strong></p><h2 id="爬虫基本流程">1.爬虫基本流程</h2><ol type="1"><li>发起请求：通过http/https库向目标站点发起请求，即发送一个request，请求可以包含额外的headers等信息，等待服务器响应</li><li>获取相应内容：如果服务器能正常响应，会得到一个response，response的内容便是所要获取的页面内容，类型可能有HTML，json字符串，二进制数据（如图片视频）等类型</li><li>解析内容：得到的内容可能是HTML，可以用正则表达式，网页解析库进行解析，可能是json，可以直接转为json对象，可能是二进制数据，可以做保存或者进一步的处理<strong>（本次实验得到的解析内容是json）</strong></li><li>保存数据：可以存为文本，也可以保存至数据库，或者特定格式的文件</li></ol><h2 id="打开网页并分析">2.打开网页并分析</h2><p>国家统计局的网站很奇怪，明明是https却会告警不安全，首次打开界面如下（本人使用的是谷歌浏览器）</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/1.jpg" /></p><p>点击“高级”-“继续前往”，方可进入首页</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/2.jpg" /></p><p>选择“季度数据”-“分省季度数据”</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/3.jpg" /></p><p>选择“人民生活”-“城乡收支情况”</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/4.jpg" /></p><p>地区修改为“上海市”</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/5.jpg" /></p><p>按下F12，进入浏览器调试模式</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/6.jpg" /></p><p>刷新重新获取网页信息，找到easyquery.htm?m=QueryData&amp;dbc...的文件。可以先选中"XHR"过滤条件，缩小查找范围</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/7.jpg" /></p><p>怎么确认这个文件就包含有我们要找的数据呢？点击“response”板块，向右拖动滑块可以看到表格数据可以一一对应（但数据并没有连续出现）</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/8.jpg" /></p><p><strong>注意：这里的data和strdata看上去一样，但实际格式不一样，data是int或double格式，strdata是str格式，这个表格有一些空数据行，字符串格式方便做判断，字符串转数字使用eval()即可</strong></p><h2 id="完整代码及解析">3.完整代码及解析</h2><p><strong>注：缺少的库可以在命令行使用pip命令安装，如缺少requests库，可以在命令行输入命令</strong></p><p><code>pip install requests</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib3 </span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用urllib3.disable_warnings()在关闭SSL认证（verify=False）情况下</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将requests请求禁用安全请求警告</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests    <span class="comment"># 使用Requests发送网络请求</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time      <span class="comment"># 用来获取时间戳(计算当前时间，用于网页验证)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json      <span class="comment"># 处理json文件</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  <span class="comment"># 处理数组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd  <span class="comment"># np.array()转换成pd.DataFrame格式，再使用to_excel()写入excel表格</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取毫秒级时间戳，用于网页验证</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTime</span>():</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">int</span>(<span class="built_in">round</span>(time.time() * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理，获取json列表中层层包裹的strdata元素（数据）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getList</span>(<span class="params">length</span>):</span><br><span class="line"></span><br><span class="line">  <span class="type">List</span>=[]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line"></span><br><span class="line">​    temp = js[<span class="string">&#x27;returndata&#x27;</span>][<span class="string">&#x27;datanodes&#x27;</span>][i][<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;strdata&#x27;</span>]</span><br><span class="line"></span><br><span class="line">​    <span class="comment"># 城乡居民收支列表中，原网站有同比增长数据为空，若直接使用eval()会报错，需要先判断</span></span><br><span class="line"></span><br><span class="line">​    <span class="keyword">if</span>(<span class="built_in">len</span>(temp)!=<span class="number">0</span>):</span><br><span class="line"></span><br><span class="line">​      <span class="comment"># eval()数字转字符串</span></span><br><span class="line"></span><br><span class="line">​      <span class="type">List</span>.append(<span class="built_in">eval</span>(temp))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="type">List</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 请求目标网址(链接?前面的东西)</span></span><br><span class="line"></span><br><span class="line">  url=<span class="string">&#x27;https://data.stats.gov.cn/easyquery.htm&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 请求头，User-Agent: 用来证明你是浏览器，满足一定格式即可，不一定和自己的浏览器一样</span></span><br><span class="line"></span><br><span class="line">  headers=&#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0(Windows;U;Windows NT6.1;en-US;rv:1.9.1.6) Geko/20091201 Firefox/3.5.6&#x27;</span>&#125;<span class="comment">#浏览器代理</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 构造参数键值对，具体数值从网页结构参数中获取</span></span><br><span class="line"></span><br><span class="line">  key=&#123;&#125;</span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;m&#x27;</span>]=<span class="string">&#x27;QueryData&#x27;</span></span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;dbcode&#x27;</span>]=<span class="string">&#x27;fsjd&#x27;</span></span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;rowcode&#x27;</span>]=<span class="string">&#x27;zb&#x27;</span></span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;colcode&#x27;</span>]=<span class="string">&#x27;sj&#x27;</span></span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;wds&#x27;</span>]=<span class="string">&#x27;[&#123;&quot;wdcode&quot;:&quot;reg&quot;,&quot;valuecode&quot;:&quot;310000&quot;&#125;]&#x27;</span></span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;k1&#x27;</span>]=<span class="built_in">str</span>(getTime()) </span><br><span class="line"></span><br><span class="line">  <span class="comment"># &quot;wdcode&quot;:&quot;reg&quot; 地区栏</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 上海 310000 </span></span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;dfwds&#x27;</span>]=<span class="string">&#x27;[&#123;&quot;wdcode&quot;:&quot;zb&quot;,&quot;valuecode&quot;:&quot;A0300&quot;&#125;,&#123;&quot;wdcode&quot;:&quot;sj&quot;,&quot;valuecode&quot;:&quot;LAST6&quot;&#125;]&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># &quot;wdcode&quot;:&quot;zb&quot; 选取左侧哪个条目,&quot;wdcode&quot;:&quot;sj&quot;选项框中选取&quot;最近6季度&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 禁用安全请求警告</span></span><br><span class="line"></span><br><span class="line">  requests.packages.urllib3.disable_warnings()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 发出请求，使用post方法，这里使用前面自定义的头部和参数</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># ！！！verify=False，国家统计局20年下半年改用https协议,若不加该代码无法通过SSL验证</span></span><br><span class="line"></span><br><span class="line">  r = requests.post(url, headers=headers, params=key,verify=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 使用json库中loads函数，将r.text字符串解析成dict字典格式存储于js中</span></span><br><span class="line"></span><br><span class="line">  js = json.loads(r.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 得到所需数据的一维数组，利用np.array().reshape()整理为二维数组</span></span><br><span class="line"></span><br><span class="line">  length=<span class="built_in">len</span>(js[<span class="string">&#x27;returndata&#x27;</span>][<span class="string">&#x27;datanodes&#x27;</span>])</span><br><span class="line"></span><br><span class="line">  res=getList(length)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 总数据划分成6行的格式</span></span><br><span class="line"></span><br><span class="line">  array=np.array(res).reshape(<span class="built_in">len</span>(res)//<span class="number">6</span>,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># np.array()转换成pd.DataFrame格式，后续可使用to_excel()直接写入excel表格</span></span><br><span class="line"></span><br><span class="line">  df_shanghai=pd.DataFrame(array)</span><br><span class="line"></span><br><span class="line">  df_shanghai.columns=[<span class="string">&#x27;2020年第三季度&#x27;</span>,<span class="string">&#x27;2020年第二季度&#x27;</span>,<span class="string">&#x27;2020年第一季度&#x27;</span>,<span class="string">&#x27;2019年第四季度&#x27;</span>,</span><br><span class="line"></span><br><span class="line">​        <span class="string">&#x27;2019年第三季度&#x27;</span>,<span class="string">&#x27;2019年第二季度&#x27;</span>]</span><br><span class="line"></span><br><span class="line">  df_shanghai.index=[<span class="string">&#x27;居民人均可支配收入累计值(元)&#x27;</span>,</span><br><span class="line"></span><br><span class="line">​       <span class="string">&#x27;城镇居民人均可支配收入累计值(元)&#x27;</span>,</span><br><span class="line"></span><br><span class="line">​       <span class="string">&#x27;农村居民人均可支配收入累计值(元)&#x27;</span>,</span><br><span class="line"></span><br><span class="line">​       <span class="string">&#x27;居民人均消费支出累计值(元)&#x27;</span>,</span><br><span class="line"></span><br><span class="line">​        <span class="string">&#x27;城镇居民人均消费支出累计值(元)&#x27;</span>,</span><br><span class="line"></span><br><span class="line">​       <span class="string">&#x27;农村居民人均消费支出累计值(元)&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(df_shanghai)</span><br></pre></td></tr></table></figure><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/9.jpg" /></p><h2 id="部分代码说明">4.部分代码说明</h2><h3 id="数据提取">数据提取</h3><p>得到表格中的数据需要先分析提取到的js文件，打印内容如下：</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/10.jpg" /></p><p>将五层列表层层剥开，得到需要的strdata</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/11.jpg" /></p><h3 id="请求网站">请求网站</h3><p>请求目标网址(''?''前面的东西)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">url=<span class="string">&#x27;https://data.stats.gov.cn/easyquery.htm&#x27;</span></span><br></pre></td></tr></table></figure><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/12.jpg" /></p><p>请求头，User-Agent:用来证明你是浏览器，满足一定格式即可，不一定要和自己的浏览器一样</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">headers=&#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0(Windows;U;Windows NT6.1;en-US;rv:1.9.1.6) Geko/20091201 Firefox/3.5.6&#x27;</span>&#125;<span class="comment">#浏览器代理</span></span><br></pre></td></tr></table></figure><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/13.jpg" /></p><p>构造参数键值对，下列参数会以 &amp; 连接，放在链接的''?''后面</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">key=&#123;&#125;</span><br><span class="line">key[<span class="string">&#x27;m&#x27;</span>]=<span class="string">&#x27;QueryData&#x27;</span></span><br><span class="line">key[<span class="string">&#x27;dbcode&#x27;</span>]=<span class="string">&#x27;fsjd&#x27;</span></span><br><span class="line">key[<span class="string">&#x27;rowcode&#x27;</span>]=<span class="string">&#x27;zb&#x27;</span></span><br><span class="line">key[<span class="string">&#x27;colcode&#x27;</span>]=<span class="string">&#x27;sj&#x27;</span></span><br><span class="line">key[<span class="string">&#x27;wds&#x27;</span>]=<span class="string">&#x27;[&#123;&quot;wdcode&quot;:&quot;reg&quot;,&quot;valuecode&quot;:&quot;310000&quot;&#125;]&#x27;</span></span><br><span class="line">key[<span class="string">&#x27;k1&#x27;</span>]=<span class="built_in">str</span>(getTime())  </span><br><span class="line">key[<span class="string">&#x27;dfwds&#x27;</span>]=<span class="string">&#x27;[&#123;&quot;wdcode&quot;:&quot;zb&quot;,&quot;valuecode&quot;:&quot;A0300&quot;&#125;,&#123;&quot;wdcode&quot;:&quot;sj&quot;,&quot;valuecode&quot;:&quot;LAST6&quot;&#125;]&#x27;</span></span><br></pre></td></tr></table></figure><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/14.jpg" /></p><p>部分参数可以从下图所示位置查看到，有些不显示的为默认，如果需要显示相同页面，需选取选项框中的相应选项</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/15.jpg" /></p><h2 id="数据保存到excel表格">5.数据保存到excel表格</h2><p>爬虫爬到的数据现以panda.dataframe格式存储，可以利用to_excel()函数，直接保存在excel表格中</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># write对象为该Excel工作簿，使用该方法保存多个工作表</span></span><br><span class="line">    write = pd.ExcelWriter(<span class="string">&#x27;F:/Ivory_Tower/norm/分省季度数据_城乡居民收支.xls&#x27;</span>) <span class="comment">#该路径自己设置即可，没有该文件的话会自行创建一个，存在的话写入会覆盖原内容</span></span><br><span class="line">    df_shanghai.to_excel(write,sheet_name=<span class="string">&#x27;上海&#x27;</span>)</span><br><span class="line">    <span class="comment">#如果爬多个省份的数据，可以写入多个工作表，且必须要加上save()保存</span></span><br><span class="line">    write.save()</span><br></pre></td></tr></table></figure><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/16.jpg" /></p><h2 id="表格优化可选">6.表格优化（可选）</h2><p>可以借助python代码，优化表格格式，如上图所示的结果不尽人意，至少还需要自动调整列宽。</p><p>这里本人采用xlwings库，需要先在命令行下载相应的库</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install xlwings</span><br><span class="line">pip install pywin32</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用xlwings库，利用python编辑整理Excel表格</span></span><br><span class="line"><span class="keyword">import</span> xlwings <span class="keyword">as</span> xw</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app=xw.App(visible=<span class="literal">False</span>,add_book=<span class="literal">False</span>) <span class="comment">#过程不可见，不添加新工作表</span></span><br><span class="line">    wb=app.books.<span class="built_in">open</span>(<span class="string">r&#x27;F:/Ivory_Tower/norm/分省季度数据_城乡居民收支.xls&#x27;</span>)</span><br><span class="line">    <span class="comment"># wb就是新建的工作簿(workbook)</span></span><br><span class="line">    <span class="comment"># 对8个工作表，分别进行操作</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>): </span><br><span class="line">        rng=wb.sheets[i].<span class="built_in">range</span>(<span class="string">&#x27;A1:H20&#x27;</span>)      <span class="comment"># 选中这些单元格</span></span><br><span class="line">        rng.api.HorizontalAlignment = -<span class="number">4108</span>   <span class="comment"># 文字水平方向居中</span></span><br><span class="line">        rng.autofit()                         <span class="comment"># 自动调整行高列宽</span></span><br><span class="line">    wb.save()</span><br><span class="line">    wb.close()</span><br><span class="line">    app.quit()</span><br></pre></td></tr></table></figure><p>运行代码，即可得到以下效果（后续多爬了其他一些省份，在key处修改相应参数即可）</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/17.jpg" /></p><h2 id="参考资料">7.参考资料</h2><p>史上超详细python爬取国家统计局数据：https://blog.csdn.net/qq_41988893/article/details/103017854</p><p>如果报其他各种各样莫名其妙的错，可以评论或私信询问哦~</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;python-爬虫爬取国家统计局数据&quot;&gt;Python
爬虫爬取国家统计局数据&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;本次实验以爬取“&lt;a
href=&quot;https://data.stats.gov.cn/index.htm&quot;&gt;国家统计局&lt;/a&gt;”首页中的【上海市城乡</summary>
      
    
    
    
    <category term="项目详情" scheme="https://le-ying.github.io/categories/%E9%A1%B9%E7%9B%AE%E8%AF%A6%E6%83%85/"/>
    
    
    <category term="Python" scheme="https://le-ying.github.io/tags/Python/"/>
    
    <category term="爬虫" scheme="https://le-ying.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>游戏内抽卡机制总结</title>
    <link href="https://le-ying.github.io/2020/09/06/%E6%B8%B8%E6%88%8F%E5%86%85%E6%8A%BD%E5%8D%A1%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93/"/>
    <id>https://le-ying.github.io/2020/09/06/%E6%B8%B8%E6%88%8F%E5%86%85%E6%8A%BD%E5%8D%A1%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93/</id>
    <published>2020-09-06T03:00:00.000Z</published>
    <updated>2022-12-11T03:55:10.746Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面">写在前面</h2><p>卡牌类游戏很好地搭载了二次元的核心元素——“角色”，抽卡活动对库存货币消耗和新增货币均有很大的正面作用，且参与了抽卡活动的玩家也占了充值用户相当高的比例。抽卡活动确实是当前非常重要的付费点</p><p>但本人深知“玄不改非，氪能改命”，而因为是轻度玩家并不想充钱变强，所以卡牌游戏玩的并不多，阴阳师、明日方舟、剑与远征等都略有接触，而每个游戏都有着属于自己的抽卡概率。</p><p>像阴阳师的SSR大体在1%左右，而剑与远征的概率在4.8%左右，但为了用户体验，最重要的是不能让时间玩家拉开RMB玩家的差距，不能违反大R碾压小R的原则，所以所谓随机基本都是伪随机。</p><p>纯干货，为了不影响阅读就不放图片了，下面介绍一些常见的抽卡机制。</p><h3 id="保底机制">①保底机制</h3><p>这是最简单，也最普遍的一种机制，如《王者荣耀》，购买次数到达361次时，荣耀水晶产出概率为100%。《剑与远征》两个保底机制，30抽必出紫卡，在同卡池内累计抽30次即可获得出一张紫卡英雄不论是单抽还是连抽，只要数量达到即必出紫卡。</p><p>还有一个保底机制就是10连抽必出一个稀有或者精英级别的英雄，和30抽不一样的地方在于只能是适用于十连抽而不能适用于十次单抽。</p><p>保底机制保证了玩家的最终体验</p><h3 id="玄学抽奖法">②玄学抽奖法</h3><p>在一些抽卡游戏里是用一定作用的，可能由于游戏开发者在写抽卡的程序时，有时候会引用其他数据，然后增加一定算法，来决定抽到哪一张卡，这就是玩家玄学的由来。</p><p>如果引用的数据是当前系统时间，那么有可能出现“凌晨某个时间点抽卡中奖率高，或者每小时的前十分钟中奖率高”</p><p>虽说结果都在你抽卡的那一刻，在服务器就决定好，这就与抽卡画出什么图案、使用哪种方法无关，但游戏厂商还是乐意留下一个玩家主导的过程，让玩家相信是抽卡过程影响抽卡结果，对抽卡这一过程充满仪式感。</p><h3id="概率递增不知道业内是不是叫水位">③概率递增（不知道业内是不是叫水位）</h3><p>概率递增法，是指抽卡时，抽卡次数越多，爆率越高的抽卡方法。如果在还没累积到这个数值前已经抽到，那么就将概率归零。</p><p>可以让玩家的游戏体验保持在一个比较均衡的位置。</p><h3 id="奖池划分">④奖池划分</h3><p>这种抽卡方法比较复杂，在一些频繁出新卡的游戏里比较多。</p><p>当玩家抽取时候，会先判定玩家进入哪个奖池（R,SR,SSR），然后再判定玩家在这个奖池里抽到哪一张卡。如果官方加入一张新卡，会单出一个奖池，暗中去掉一张旧卡，玩家不会太过关注旧卡的出卡率，也乐意多抽出新卡。</p><h3 id="剧本抽卡">⑤剧本抽卡</h3><p>《空当接龙》所有的牌组都已经写好，每次开始游戏，就从牌组剧本中挑选一个。</p><p>《斗地主》游戏官方会特意编写出多连对，多飞机，多炸弹的牌组，随机发牌很可能出现散牌。</p><h3 id="氪金区分抽卡">⑥氪金区分抽卡</h3><p>原来的游戏是充值多少送一次抽奖，且一般都能得到非常珍贵的游戏道具。现在会暗中增加一个数据栏，计算玩家充值的数量，划分等级调整概率，来提高氪金玩家的游戏体验。</p><p>如果某种货币既可以从游戏内肝到，也可以选择充值得到，那么官方可以暗中设定一个状态栏，将活动肝到的和充值得到的区别开，每次抽奖，都会识别这次抽奖所使用的钻石是哪种类型的钻石。如果使用的过程中两种同时使用，可能默认都是充值得到的，这时概率会比用肝到的大。</p><h3 id="与抽卡促销的其他玩法">⑦与抽卡促销的其他玩法</h3><p>常见的表现方式是进入游戏，就给玩家一笔足够首抽的钱，引导玩家进行抽奖然后获得珍贵道具。</p><p>或者抽奖时，系统突然提醒你：你获得了一个购买稀有道具的机会，并附带增加时间限制。</p><p>还有一种方法是根据新玩家的道具需求，调整不同物品的爆率。比如收集套装正缺那一个部件，很可能抽奖的时候就爆出来。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;写在前面&quot;&gt;写在前面&lt;/h2&gt;
&lt;p&gt;卡牌类游戏很好地搭载了二次元的核心元素——“角色”，抽卡活动对库存货币消耗和新增货币均有很大的正面作用，且参与了抽卡活动的玩家也占了充值用户相当高的比例。抽卡活动确实是当前非常重要的付费点&lt;/p&gt;
&lt;p&gt;但本人深知“玄不改</summary>
      
    
    
    
    <category term="游戏" scheme="https://le-ying.github.io/categories/%E6%B8%B8%E6%88%8F/"/>
    
    
    <category term="二次元" scheme="https://le-ying.github.io/tags/%E4%BA%8C%E6%AC%A1%E5%85%83/"/>
    
    <category term="抽卡" scheme="https://le-ying.github.io/tags/%E6%8A%BD%E5%8D%A1/"/>
    
    <category term="游戏" scheme="https://le-ying.github.io/tags/%E6%B8%B8%E6%88%8F/"/>
    
  </entry>
  
  <entry>
    <title>我的第一篇博客</title>
    <link href="https://le-ying.github.io/2020/02/11/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <id>https://le-ying.github.io/2020/02/11/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</id>
    <published>2020-02-11T04:14:00.000Z</published>
    <updated>2023-03-19T07:48:55.592Z</updated>
    
    <content type="html"><![CDATA[<h1 id="为什么要写博客">为什么要写博客</h1><p>记录博客是酝酿很久的想法，相信多数人作出这一决定也都经历了较长时间的拖延症hh。不过除了习惯性偷懒，也有出于对自身技术水平不自信的考量，毕竟大牛是极少数，多数人不过是在平均线上徘徊。</p><p>不过大脑做决定并不是纯粹理性的，反而主要凭感性。立下了靶子，定下来方向，理性思维才会积极地把行为合理化。</p><p>对为什么突发奇想开始记录博客，个人总结了如下动机：</p><ul><li><p>无论课内外，本人都已养成了动笔前先查阅大量资料的习惯。心里对那些具有开源精神的大牛们、前辈们充满敬意和感激。很多时候，一个简洁清晰的结论、一行高度概括的代码，单靠自己的探索往往要事倍功半，甚至还可能因为其在知识盲区(UnknownUnknown)而作不必要的苦恼，被前辈们留下的博客文章中不经意地一语道破，这样的瞬间简直不要太多。</p></li><li><p>从一个纯小白进化到现在一个在很多领域都有些入门经验的....小白来说，也很希望把当时掉进去的坑补上，最起码在前面做个警示，新人在环境搭建阶段没必要走弯路，把重心放在解决需求的程序调试阶段，实现更高的自我提升效率。</p></li><li><p>俗话说得好：“好记性不如烂笔头。”之前看过一本讲如何高效记笔记的书，但纸面的笔记也常常无法翻阅。加上现在经常用手机浏览很多碎片化的知识点，得不到有效的整理，博客的存在比起私人笔记，也有种民主监督的意味在里头，避免个人认知偏差和局限。</p></li><li><p>还有《暗时间》，让我受益匪浅，学习的时候我也会经常想象如何把知识向一个小白讲解，而博客也相当于把这个过程实例化，可视化。</p></li></ul><h1 id="博客记录什么">博客记录什么</h1><p>大学期间，课内学的很多是原理层面的东西，课外兴趣广泛，为避免犯蜻蜓点水般浅尝辄止的毛病，我总结所学以下几个方面的知识技术，抽空进行记录：</p><ul><li>编程语言类：C、Python、JAVA等</li><li>软件安装类: Android Studio，WordPress等</li><li>音频编辑类：pr，ps，au等等</li></ul><p>博客更新频率尽量保持在一周一两次，在此先作个纪念，日后若需要再加更改。</p><p>有诗云:“青山一道同云雨，明月何曾是两乡。”</p><p>愿与诸君共勉。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;为什么要写博客&quot;&gt;为什么要写博客&lt;/h1&gt;
&lt;p&gt;记录博客是酝酿很久的想法，相信多数人作出这一决定也都经历了较长时间的拖延症hh。不过除了习惯性偷懒，也有出于对自身技术水平不自信的考量，毕竟大牛是极少数，多数人不过是在平均线上徘徊。&lt;/p&gt;
&lt;p&gt;不过大脑做决</summary>
      
    
    
    
    <category term="总结" scheme="https://le-ying.github.io/categories/%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="介绍" scheme="https://le-ying.github.io/tags/%E4%BB%8B%E7%BB%8D/"/>
    
  </entry>
  
</feed>
