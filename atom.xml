<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Kung&#39;s Blog</title>
  
  <subtitle>欢迎来到我的博客!</subtitle>
  <link href="https://serika-onoe.github.io/atom.xml" rel="self"/>
  
  <link href="https://serika-onoe.github.io/"/>
  <updated>2023-01-31T02:16:40.916Z</updated>
  <id>https://serika-onoe.github.io/</id>
  
  <author>
    <name>龚泽颖</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Transformer 的 Pytorch 复现简易版教程</title>
    <link href="https://serika-onoe.github.io/2023/01/11/Transformer%20%E7%9A%84%20PyTorch%20%E5%AE%9E%E7%8E%B0/"/>
    <id>https://serika-onoe.github.io/2023/01/11/Transformer%20%E7%9A%84%20PyTorch%20%E5%AE%9E%E7%8E%B0/</id>
    <published>2023-01-11T09:00:10.000Z</published>
    <updated>2023-01-31T02:16:40.916Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍一下如何使用 PyTorch 复现Transformer，实现简单的机器翻译任务。Transformer的原理部分可以参考我的这篇文章:<ahref="https://serika-onoe.github.io/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/">Transformer解析，适合没有NLP基础的小白入门（下）</a>。</p><p><strong>该教程代码初始来源于JeffJung，我阅读了一些博客和视频后做了大量的注释和修改，更加方便阅读和复现。</strong></p><p>代码中为了加快可读性和运行速度，并没有用到大型的数据集，而是手动输入了两对中文→英语的句子，还有每个字的索引也是手动硬编码上去的，主要是为了降低代码执行速度和阅读难度，哪怕用普通的笔记本CPU也能在1分钟以内完成，从而方便读者<strong>把重点放到模型实现的部分！</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ======================================</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">code by Tae Hwan Jung(Jeff Jung) @graykode, Derek Miller @dmmiller612, modify by shwei</span></span><br><span class="line"><span class="string">Reference: https://github.com/jadore801120/attention-is-all-you-need-pytorch</span></span><br><span class="line"><span class="string">           https://github.com/JayParks/transformer</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># ====================================================================================================</span></span><br></pre></td></tr></table></figure><h1 id="数据预处理">数据预处理</h1><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> Data</span><br><span class="line"></span><br><span class="line">device = <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line"><span class="comment"># device = &#x27;cuda&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># transformer epochs</span></span><br><span class="line">epochs = <span class="number">100</span></span><br><span class="line"><span class="comment"># epochs = 1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里我没有用什么大型的数据集，而是手动输入了两对中文→英语的句子</span></span><br><span class="line"><span class="comment"># 还有每个字的索引也是我手动硬编码上去的，主要是为了降低代码阅读难度</span></span><br><span class="line"><span class="comment"># S: Symbol that shows starting of decoding input</span></span><br><span class="line"><span class="comment"># E: Symbol that shows starting of decoding output</span></span><br><span class="line"><span class="comment"># P: Symbol that will fill in blank sequence if current batch data size is shorter than time steps</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="comment"># 中文和英语的单词个数不要求相同</span></span><br><span class="line">    <span class="comment"># enc_input                dec_input           dec_output</span></span><br><span class="line">    [<span class="string">&#x27;我 有 一 个 女 朋 友&#x27;</span>, <span class="string">&#x27;S i have a girl friend . &#x27;</span>, <span class="string">&#x27;i have a girl friend . E&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;我 有 零 个 好 朋 友&#x27;</span>, <span class="string">&#x27;S i have zero good friend .&#x27;</span>, <span class="string">&#x27;i have zero good friend . E&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 中文和英语的单词要分开建立词库</span></span><br><span class="line"><span class="comment"># Padding Should be Zero</span></span><br><span class="line">src_vocab = &#123;</span><br><span class="line">            <span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;我&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;有&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;一&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;个&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;好&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;朋&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;友&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;零&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;女&#x27;</span>: <span class="number">9</span>&#125;</span><br><span class="line">src_idx2word = &#123;</span><br><span class="line">            i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(src_vocab)&#125;</span><br><span class="line">src_vocab_size = <span class="built_in">len</span>(src_vocab)</span><br><span class="line"></span><br><span class="line">tgt_vocab = &#123;</span><br><span class="line">            <span class="string">&#x27;P&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;i&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;have&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;a&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;good&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;friend&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;zero&#x27;</span>: <span class="number">6</span>, <span class="string">&#x27;girl&#x27;</span>: <span class="number">7</span>, <span class="string">&#x27;S&#x27;</span>: <span class="number">8</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">9</span>, <span class="string">&#x27;.&#x27;</span>: <span class="number">10</span>&#125;</span><br><span class="line">idx2word = &#123;</span><br><span class="line">            i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(tgt_vocab)&#125;</span><br><span class="line">tgt_vocab_size = <span class="built_in">len</span>(tgt_vocab)</span><br><span class="line"></span><br><span class="line">src_len = <span class="number">8</span>  <span class="comment"># （源句子的长度）enc_input max sequence length</span></span><br><span class="line">tgt_len = <span class="number">7</span>  <span class="comment"># dec_input(=dec_output) max sequence length</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Transformer Parameters</span></span><br><span class="line">d_model = <span class="number">512</span>  <span class="comment"># Embedding Size（token embedding和position编码的维度）</span></span><br><span class="line">d_ff = <span class="number">2048</span>  <span class="comment"># FeedForward dimension (两次线性层中的隐藏层 512-&gt;2048-&gt;512，线性层是用来做特征提取的），当然最后会再接一个projection层</span></span><br><span class="line">d_k = d_v = <span class="number">64</span>  <span class="comment"># dimension of K(=Q), V（Q和K的维度需要相同，这里为了方便让K=V）</span></span><br><span class="line">n_layers = <span class="number">6</span>  <span class="comment"># number of Encoder of Decoder Layer（Block的个数）</span></span><br><span class="line">n_heads = <span class="number">8</span>  <span class="comment"># number of heads in Multi-Head Attention（有几套头）</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ==============================================================================================</span></span><br><span class="line"><span class="comment"># 数据构建</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentences</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;把单词序列转换为数字序列&quot;&quot;&quot;</span></span><br><span class="line">    enc_inputs, dec_inputs, dec_outputs = [], [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(sentences)):</span><br><span class="line">        enc_input = [[src_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">0</span>].split()]]  <span class="comment"># [[1, 2, 3, 4, 0], [1, 2, 3, 5, 0]]</span></span><br><span class="line">        dec_input = [[tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">1</span>].split()]]  <span class="comment"># [[6, 1, 2, 3, 4, 8], [6, 1, 2, 3, 5, 8]]</span></span><br><span class="line">        dec_output = [[tgt_vocab[n] <span class="keyword">for</span> n <span class="keyword">in</span> sentences[i][<span class="number">2</span>].split()]]  <span class="comment"># [[1, 2, 3, 4, 8, 7], [1, 2, 3, 5, 8, 7]]</span></span><br><span class="line"></span><br><span class="line">        enc_inputs.extend(enc_input)</span><br><span class="line">        dec_inputs.extend(dec_input)</span><br><span class="line">        dec_outputs.extend(dec_output)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataSet</span>(Data.Dataset):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义DataLoader&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, enc_inputs, dec_inputs, dec_outputs</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyDataSet, self).__init__()</span><br><span class="line">        self.enc_inputs = enc_inputs</span><br><span class="line">        self.dec_inputs = dec_inputs</span><br><span class="line">        self.dec_outputs = dec_outputs</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.enc_inputs.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 这行代码创建了一个PyTorch数据加载器，用于在训练机器学习模型时加载数据。DataLoader是PyTorch的核心库torch.utils.data中的函数。它的作用是将</span></span><br><span class="line"><span class="comment"># 数据集（此处为MyDataSet类的实例，传入的enc_inputs、dec_inputs和dec_outputs作为数据）拆分成小批次以提高加载效率。</span></span><br><span class="line"><span class="comment"># 具体参数说明：</span></span><br><span class="line"><span class="comment"># batch_size: 2，每批加载的样本数。</span></span><br><span class="line"><span class="comment"># shuffle: True，是否打乱数据顺序。</span></span><br></pre></td></tr></table></figure><p>上面都比较简单，下面开始涉及到模型就比较复杂了，因此我会将模型拆分成以下几个部分进行讲解</p><ul><li>Positional Encoding</li><li>PadMask（序列本身固定长度，不够长的序列需要填充（pad），也就是'P'）</li><li>Subsequence Mask（Decoder input 不能看到未来时刻单词信息，因此需要mask）</li><li>ScaledDotProductAttention</li><li>Multi-Head Attention</li><li>FeedForward Layer</li><li>Encoder Layer</li><li>Encoder</li><li>Decoder Layer</li><li>Decoder</li><li>Transformer</li></ul><p>关于代码中的注释，如果值为 <code>src_len</code> 或者<code>tgt_len</code> 的，我一定会写清楚，但是有些函数或者类，Encoder 和Decoder 都有可能调用，因此就不能确定究竟是 <code>src_len</code> 还是<code>tgt_len</code>，对于不确定的，我会记作 <code>seq_len</code></p><h1 id="模型构建">模型构建</h1><h2 id="positional-encoding">Positional Encoding</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout=<span class="number">0.1</span>, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, self).__init__()</span><br><span class="line">        self.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        在dropout函数中，参数p是指丢弃元素的概率。它决定了输入张量的元素在丢弃操作中被设置为零的比率。</span></span><br><span class="line"><span class="string">        例如，如果p=0.1，那么10%的元素将在丢弃操作中被设置为零。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        self.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        通过调用self.register_buffer(&#x27;pe&#x27;, pe)，我们将这个张量注册为模型的一个可学习参数，</span></span><br><span class="line"><span class="string">        这意味着这个张量在模型训练过程中不需要更新其梯度，即不需要在损失函数的计算中考虑这个张量的梯度。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        x: [seq_len, batch_size, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x = x + self.pe[:x.size(<span class="number">0</span>), :]</span><br><span class="line">        <span class="keyword">return</span> self.dropout(x)</span><br></pre></td></tr></table></figure><h2 id="pad-mask">Pad Mask</h2><p>由于在 Encoder 和 Decoder 中都需要进行 mask操作，因此就无法确定这个函数的参数中 <code>seq_len</code> 的值，如果是在Encoder 中调用的，<code>seq_len</code> 就等于<code>src_len</code>；如果是在 Decoder 中调用的，<code>seq_len</code>就有可能等于 <code>src_len</code>，也有可能等于<code>tgt_len</code>（因为 Decoder 有两次 mask）</p><p><strong>padmask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量。</strong></p><p>这个函数最核心的一句代码是<code>seq_k.data.eq(0)</code>，这句的作用是返回一个大小和<code>seq_k</code> 一样的 tensor，只不过里面的值只有 True 和 False。如果<code>seq_k</code> 某个位置的值等于 0，那么对应位置就是 True，否则即为False。举个例子，输入为<code>seq_data = [1, 2, 3, 4, 0]</code>，<code>seq_data.data.eq(0)</code>就会返回 <code>[False, False, False, False, True]</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span><br><span class="line">    <span class="comment"># pad mask的作用：在对value向量加权平均的时候，可以让pad对应的alpha_ij=0，这样注意力就不会考虑到pad向量</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;这里的q,k表示的是两个序列（跟注意力机制的q,k没有关系），例如encoder_inputs (x1,x2,..xm)和encoder_inputs (x1,x2..xm)</span></span><br><span class="line"><span class="string">    encoder和decoder都可能调用这个函数，所以seq_len视情况而定</span></span><br><span class="line"><span class="string">    seq_q: [batch_size, seq_len]</span></span><br><span class="line"><span class="string">    seq_k: [batch_size, seq_len]</span></span><br><span class="line"><span class="string">    seq_len could be src_len or it could be tgt_len</span></span><br><span class="line"><span class="string">    seq_len in seq_q and seq_len in seq_k maybe not equal</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    batch_size, len_q = seq_q.size()  <span class="comment"># 这个seq_q只是用来expand维度的</span></span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    <span class="comment"># eq(zero) is PAD token</span></span><br><span class="line">    <span class="comment"># 例如:seq_k = [[1,2,3,4,0], [1,2,3,5,0]]</span></span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># [batch_size, 1, len_k], True is masked</span></span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask.expand(batch_size, len_q, len_k)  <span class="comment"># [batch_size, len_q, len_k] 构成一个立方体(batch_size个这样的矩阵)</span></span><br></pre></td></tr></table></figure><h2 id="subsequence-mask">Subsequence Mask</h2><p><strong>Subsequence Mask 只有 Decoder会用到，主要作用是屏蔽未来时刻单词的信息。</strong></p><p>这段代码实现了获得一个注意力子序列掩码。它使用Numpy函数np.triu生成一个上三角矩阵，并用np.ones初始化这个矩阵。attn_shape变量储存了这个矩阵的形状，它是一个三维数组，分别是batch_size、tgt_len、tgt_len。然后，np.triu将这个矩阵初始化为上三角形，并通过参数k=1使对角线上的元素为0。最后，将这个矩阵转换为PyTorchtensor，并返回该张量。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequence_mask</span>(<span class="params">seq</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;建议打印出来看看是什么样的输出（一目了然）</span></span><br><span class="line"><span class="string">    seq: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]</span><br><span class="line">    <span class="comment"># attn_shape: [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">    subsequence_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>)  <span class="comment"># 生成一个上三角矩阵</span></span><br><span class="line">    subsequence_mask = torch.from_numpy(subsequence_mask).byte()</span><br><span class="line">    <span class="keyword">return</span> subsequence_mask  <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br></pre></td></tr></table></figure><h2 id="scaleddotproductattention">ScaledDotProductAttention</h2><p><strong>"Scaled Dot-Product Attention"是一种用于自注意力机制的注意力机制方法，通过将输入矩阵Q、K和V与自身转置进行点积运算，得到关于每一个词的注意力分数。</strong></p><p>使用Q、K、V三个变量作为输入，经过一系列矩阵运算后得到context和attn，其中context是计算出的注意力张量，attn是对应的注意力稀疏矩阵，并返回这两个结果。</p><p>具体地，在forward函数中，使用Q、K做矩阵乘法得到scores矩阵，scores中的每一个元素都是对应Q中词与K中词的相似程度。</p><p>下一步，通过使用mask矩阵对scores中的元素进行赋值，将与mask矩阵中值为1的元素相对应的scores元素赋值为-1e9，使其不被softmax计算。</p><p>最后，使用softmax对scores最后一维（也就是v）做软归一化，得到注意力稀疏矩阵attn。最后，使用attn矩阵对V做矩阵乘法，得到context矩阵，其中每一行对应一个词的向量表示。</p><p>（matmul函数是矩阵乘法，它返回两个矩阵的点积，即将两个矩阵对应元素相乘并相加。它对应的矩阵乘法操作是：C= A * B，其中C是乘积矩阵，A是左矩阵，B是右矩阵。）</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, self).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line"><span class="string">        K: [batch_size, n_heads, len_k, d_k]</span></span><br><span class="line"><span class="string">        V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line"><span class="string">        attn_mask: [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line"><span class="string">        说明：在encoder-decoder的Attention层中len_q(q1,..qt)和len_k(k1,...km)可能不同</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(d_k)  <span class="comment"># scores : [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        <span class="comment"># mask矩阵填充scores（用-1e9填充scores中与attn_mask中值为1位置相对应的元素）</span></span><br><span class="line">        scores.masked_fill_(attn_mask, -<span class="number">1e9</span>)  <span class="comment"># Fills elements of self tensor with value where mask is True.</span></span><br><span class="line"></span><br><span class="line">        attn = nn.Softmax(dim=-<span class="number">1</span>)(scores)  <span class="comment"># 对最后一个维度(v)做softmax</span></span><br><span class="line">        <span class="comment"># scores : [batch_size, n_heads, len_q, len_k] * V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line">        context = torch.matmul(attn, V)  <span class="comment"># context: [batch_size, n_heads, len_q, d_v]</span></span><br><span class="line">        <span class="comment"># context：[[z1,z2,...],[...]]向量, attn注意力稀疏矩阵（用于可视化的）</span></span><br><span class="line">        <span class="keyword">return</span> context, attn</span><br></pre></td></tr></table></figure><h2 id="multiheadattention">MultiHeadAttention</h2><p>完整代码中一定会有三处地方调用<code>MultiHeadAttention()</code>，Encoder Layer 调用一次，传入的<code>input_Q</code>、<code>input_K</code>、<code>input_V</code>全部都是 <code>enc_inputs</code>；Decoder Layer中两次调用，第一次传入的全是 <code>dec_inputs</code>，第二次传入的分别是<code>dec_outputs</code>，<code>enc_outputs</code>，<code>enc_outputs</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;这个Attention类可以实现:</span></span><br><span class="line"><span class="string">    Encoder的Self-Attention</span></span><br><span class="line"><span class="string">    Decoder的Masked Self-Attention</span></span><br><span class="line"><span class="string">    Encoder-Decoder的Attention</span></span><br><span class="line"><span class="string">    输入：seq_len x d_model</span></span><br><span class="line"><span class="string">    输出：seq_len x d_model</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)  <span class="comment"># q,k必须维度相同，不然无法做点积</span></span><br><span class="line">        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 这个全连接层可以保证多头attention的输出仍然是seq_len x d_model</span></span><br><span class="line">        self.fc = nn.Linear(n_heads * d_v, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        nn.Linear 函数是 PyTorch 模型中的一种全连接层 (fully connected layer) 的实现。</span></span><br><span class="line"><span class="string">        它的作用是对输入数据进行线性变换，即 y = Wx + b，其中 W 是线性变换的系数矩阵，b 是偏移量，x 是输入数据。</span></span><br><span class="line"><span class="string">        torch.nn.Linear(in_features, # 输入的神经元个数</span></span><br><span class="line"><span class="string">           out_features, # 输出神经元个数</span></span><br><span class="line"><span class="string">           bias=True # 是否包含偏置</span></span><br><span class="line"><span class="string">           )</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_Q, input_K, input_V, attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        input_Q: [batch_size, len_q, d_model]</span></span><br><span class="line"><span class="string">        input_K: [batch_size, len_k, d_model]</span></span><br><span class="line"><span class="string">        input_V: [batch_size, len_v(=len_k), d_model]</span></span><br><span class="line"><span class="string">        attn_mask: [batch_size, seq_len, seq_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        residual, batch_size = input_Q, input_Q.size(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 下面的多头的参数矩阵是放在一起做线性变换的，然后再拆成多个头，这是工程实现的技巧</span></span><br><span class="line">        <span class="comment"># B: batch_size, S:seq_len, D: dim</span></span><br><span class="line">        <span class="comment"># (B, S, D) -proj-&gt; (B, S, D_new) -split-&gt; (B, S, Head, W) -trans-&gt; (B, Head, S, W)</span></span><br><span class="line">        <span class="comment">#           线性变换               拆成多头</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Q: [batch_size, n_heads, len_q, d_k]</span></span><br><span class="line">        Q = self.W_Q(input_Q).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># K: [batch_size, n_heads, len_k, d_k] # K和V的长度一定相同，维度可以不同</span></span><br><span class="line">        K = self.W_K(input_K).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># V: [batch_size, n_heads, len_v(=len_k), d_v]</span></span><br><span class="line">        V = self.W_V(input_V).view(batch_size, -<span class="number">1</span>, n_heads, d_v).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 因为是多头，所以mask矩阵要扩充成4维的</span></span><br><span class="line">        <span class="comment"># attn_mask: [batch_size, seq_len, seq_len] -&gt; [batch_size, n_heads, seq_len, seq_len]</span></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, n_heads, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># context: [batch_size, n_heads, len_q, d_v], attn: [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)</span><br><span class="line">        <span class="comment"># 下面将不同头的输出向量拼接在一起</span></span><br><span class="line">        <span class="comment"># context: [batch_size, n_heads, len_q, d_v] -&gt; [batch_size, len_q, n_heads * d_v]</span></span><br><span class="line">        context = context.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(batch_size, -<span class="number">1</span>, n_heads * d_v)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这个全连接层可以保证多头attention的输出仍然是seq_len x d_model</span></span><br><span class="line">        output = self.fc(context)  <span class="comment"># [batch_size, len_q, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_model).to(device)(output + residual), attn</span><br></pre></td></tr></table></figure><h2 id="feedforward-layer">FeedForward Layer</h2><p><strong>这段代码非常简单，就是做两次线性变换，残差连接后再跟一个Layer Norm。用于实现Transformer模型中的前馈网络。</strong></p><p>该网络由两个全连接层（nn.Linear）和一个 ReLU激活函数（nn.ReLU）组成。第一个全连接层将输入从 d_model 维度转换到 d_ff维度，第二个全连接层将输入从 d_ff 维度转换回 d_model 维度。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PoswiseFeedForwardNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(PoswiseFeedForwardNet, self).__init__()</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(d_model, d_ff, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(d_ff, d_model, bias=<span class="literal">False</span>)</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        inputs: [batch_size, seq_len, d_model]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        residual = inputs</span><br><span class="line">        output = self.fc(inputs)</span><br><span class="line">        <span class="keyword">return</span> nn.LayerNorm(d_model).to(device)(output + residual)  <span class="comment"># [batch_size, seq_len, d_model]</span></span><br></pre></td></tr></table></figure><h2 id="encoder-layer-encoder">Encoder Layer &amp; Encoder</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()</span><br><span class="line">        self.enc_self_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, enc_self_attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len, d_model]</span></span><br><span class="line"><span class="string">        enc_self_attn_mask: [batch_size, src_len, src_len]  mask矩阵(pad mask or sequence mask)</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model], attn: [batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">        <span class="comment"># 第一个enc_inputs * W_Q = Q</span></span><br><span class="line">        <span class="comment"># 第二个enc_inputs * W_K = K</span></span><br><span class="line">        <span class="comment"># 第三个enc_inputs * W_V = V</span></span><br><span class="line">        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs,</span><br><span class="line">                                               enc_self_attn_mask)  <span class="comment"># enc_inputs to same Q,K,V（未线性变换前）</span></span><br><span class="line">        enc_outputs = self.pos_ffn(enc_outputs)</span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs, attn</span><br></pre></td></tr></table></figure><p><code>nn.ModuleList()</code> 列表里面存了 <code>n_layers</code> 个Encoder Layer。由于我们控制好了 Encoder Layer的输入和输出维度相同，所以可以直接用个 for 循环以嵌套的方式，将上一次Encoder Layer 的输出作为下一次 Encoder Layer 的输入。</p><p><strong>将<code>n_layers</code>个（本文为6个）EncoderLayer组件逐个拼起来，就是一个完整的Encoder。</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()</span><br><span class="line">        self.src_emb = nn.Embedding(src_vocab_size, d_model)  <span class="comment"># token Embedding</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Embedding解释</span></span><br><span class="line"><span class="string">        例如：如果你有一个词语表(vocabulary)，其中包含了3个词语：&quot;dog&quot;, &quot;cat&quot;, &quot;bird&quot;。并且你指定了src_vocab_size = 3</span></span><br><span class="line"><span class="string">        和d_model = 5，那么这个Embedding层就可以将每一个词语表示成一个5维的实数向量，比如：&quot;dog&quot;</span></span><br><span class="line"><span class="string">        可以表示为[0.1, 0.2, 0.3, 0.4, 0.5]。</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)  <span class="comment"># Transformer中位置编码时固定的，不需要学习</span></span><br><span class="line">        self.layers = nn.ModuleList([EncoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs)  <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        enc_outputs = self.pos_emb(enc_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># [batch_size, src_len, d_model]</span></span><br><span class="line">        <span class="comment"># Encoder输入序列的pad mask矩阵</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs)  <span class="comment"># [batch_size, src_len, src_len]</span></span><br><span class="line">        enc_self_attns = []  <span class="comment"># 在计算中不需要用到，它主要用来保存你接下来返回的attention的值（这个主要是为了你画热力图等，用来看各个词之间的关系</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:  <span class="comment"># for循环访问nn.ModuleList对象</span></span><br><span class="line">            <span class="comment"># 上一个block的输出enc_outputs作为当前block的输入</span></span><br><span class="line">            <span class="comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attn: [batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">            enc_outputs, enc_self_attn = layer(enc_outputs,</span><br><span class="line">                                               enc_self_attn_mask)  <span class="comment"># 传入的enc_outputs其实是input，传入mask矩阵是因为你要做self attention</span></span><br><span class="line">            enc_self_attns.append(enc_self_attn)  <span class="comment"># 这个只是为了可视化</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs, enc_self_attns</span><br></pre></td></tr></table></figure><h1 id="decoder-layer-decoder">Decoder Layer &amp; Decoder</h1><p>在 Decoder Layer 中会调用两次<code>MultiHeadAttention</code>，第一次是计算 Decoder Input 的self-attention，得到输出 <code>dec_outputs</code>。然后将<code>dec_outputs</code> 作为生成 Q 的元素，<code>enc_outputs</code>作为生成 K 和 V 的元素，再调用一次<code>MultiHeadAttention</code>，得到的是 Encoder 和 Decoder Layer之间的 context vector。最后将 <code>dec_outptus</code>做一次维度变换，然后返回。</p><p><strong>将<code>n_layers</code>个（本文为6个）DecoderLayer组件逐个拼起来，就是一个完整的Decoder。</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.dec_self_attn = MultiHeadAttention()</span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention()</span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len, d_model]</span></span><br><span class="line"><span class="string">        enc_outputs: [batch_size, src_len, d_model]</span></span><br><span class="line"><span class="string">        dec_self_attn_mask: [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"><span class="string">        dec_enc_attn_mask: [batch_size, tgt_len, src_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len]</span></span><br><span class="line">        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, dec_inputs,</span><br><span class="line">                                                        dec_self_attn_mask)  <span class="comment"># 这里的Q,K,V全是Decoder自己的输入</span></span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span></span><br><span class="line">        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs,</span><br><span class="line">                                                      dec_enc_attn_mask)  <span class="comment"># Attention层的Q(来自decoder) 和 K,V(来自encoder)</span></span><br><span class="line">        dec_outputs = self.pos_ffn(dec_outputs)  <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attn, dec_enc_attn  <span class="comment"># dec_self_attn, dec_enc_attn这两个是为了可视化的</span></span><br></pre></td></tr></table></figure><p>Decoder 中不仅要把 "pad"mask 掉，还要 mask未来时刻的信息，因此就有了下面这三行代码，其中<code>torch.gt(a, value)</code> 的意思是，将 a 中各个位置上的元素和value 比较，若大于 value，则该位置取 1，否则取 0</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(tgt_vocab_size, d_model)  <span class="comment"># Decoder输入的embed词表</span></span><br><span class="line">        self.pos_emb = PositionalEncoding(d_model)</span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)])  <span class="comment"># Decoder的blocks</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        enc_outputs: [batch_size, src_len, d_model]   # 用在Encoder-Decoder Attention层</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs)  <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        dec_outputs = self.pos_emb(dec_outputs.transpose(<span class="number">0</span>, <span class="number">1</span>)).transpose(<span class="number">0</span>, <span class="number">1</span>).to(</span><br><span class="line">            device)  <span class="comment"># [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="comment"># Decoder输入序列的pad mask矩阵（这个例子中decoder是没有加pad的，实际应用中都是有pad填充的）</span></span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs).to(device)  <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line">        <span class="comment"># Masked Self_Attention：当前时刻是看不到未来的信息的</span></span><br><span class="line">        dec_self_attn_subsequence_mask = get_attn_subsequence_mask(dec_inputs).to(</span><br><span class="line">            device)  <span class="comment"># [batch_size, tgt_len, tgt_len]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Decoder中把两种mask矩阵相加（既屏蔽了pad的信息，也屏蔽了未来时刻的信息）</span></span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask + dec_self_attn_subsequence_mask),</span><br><span class="line">                                      <span class="number">0</span>).to(device)  <span class="comment"># [batch_size, tgt_len, tgt_len]; torch.gt比较两个矩阵的元素，大于则返回1，否则返回0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 这个mask主要用于encoder-decoder attention层</span></span><br><span class="line">        <span class="comment"># get_attn_pad_mask主要是enc_inputs的pad mask矩阵(因为enc是处理K,V的，求Attention时是用v1,v2,..vm去加权的，要把pad对应的v_i的相关系数设为0，这样注意力就不会关注pad向量)</span></span><br><span class="line">        <span class="comment">#                       dec_inputs只是提供expand的size的</span></span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs)  <span class="comment"># [batc_size, tgt_len, src_len]</span></span><br><span class="line"></span><br><span class="line">        dec_self_attns, dec_enc_attns = [], []</span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attn: [batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [batch_size, h_heads, tgt_len, src_len]</span></span><br><span class="line">            <span class="comment"># Decoder的Block是上一个Block的输出dec_outputs（变化）和Encoder网络的输出enc_outputs（固定）</span></span><br><span class="line">            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, dec_self_attn_mask,</span><br><span class="line">                                                             dec_enc_attn_mask)</span><br><span class="line">            dec_self_attns.append(dec_self_attn)</span><br><span class="line">            dec_enc_attns.append(dec_enc_attn)</span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model]</span></span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure><h2 id="transformer">Transformer</h2><p><strong>这段代码实现了Transformer类，用到的三种架构--前面定义过的Encoder和Decoder，以及下面新定义的投影层Projection（projection实现的是decoder后面的linear，之所以没有实现softmax是因为后续的贪婪解码器替代了softmax层的工作，直接得到概率最大值的词表索引并输出）。</strong></p><p>在输入经过Encoder网络和Decoder网络处理后，得到的输出分别是enc_outputs和dec_outputs。最后再经过一个投影层，将dec_outputs映射成dec_logits，表示每个单词的词概率分布。返回dec_logits, enc_self_attns, dec_self_attns, dec_enc_attns。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()</span><br><span class="line">        <span class="comment"># Transformer类继承自PyTorch的nn.Module类。</span></span><br><span class="line">        <span class="comment"># 通过在Transformer的构造函数中调用super(Transformer, self).init()，</span></span><br><span class="line">        <span class="comment"># 可以调用nn.Module的构造函数，以便初始化nn.Module的一些内部状态，以及设置Transformer类对象的一些公共属性。</span></span><br><span class="line">        self.encoder = Encoder().to(device)</span><br><span class="line">        self.decoder = Decoder().to(device)</span><br><span class="line">        self.projection = nn.Linear(d_model, tgt_vocab_size, bias=<span class="literal">False</span>).to(device)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, dec_inputs</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;Transformers的输入：两个序列</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># tensor to store decoder outputs</span></span><br><span class="line">        <span class="comment"># outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># enc_outputs: [batch_size, src_len, d_model], enc_self_attns: [n_layers, batch_size, n_heads, src_len, src_len]</span></span><br><span class="line">        <span class="comment"># 经过Encoder网络后，得到的输出还是[batch_size, src_len, d_model]</span></span><br><span class="line">        enc_outputs, enc_self_attns = self.encoder(enc_inputs)</span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model], dec_self_attns: [n_layers, batch_size, n_heads, tgt_len, tgt_len], dec_enc_attn: [n_layers, batch_size, tgt_len, src_len]</span></span><br><span class="line">        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)</span><br><span class="line">        <span class="comment"># dec_outputs: [batch_size, tgt_len, d_model] -&gt; dec_logits: [batch_size, tgt_len, tgt_vocab_size]</span></span><br><span class="line">        dec_logits = self.projection(dec_outputs)</span><br><span class="line">        <span class="keyword">return</span> dec_logits.view(-<span class="number">1</span>, dec_logits.size(-<span class="number">1</span>)), enc_self_attns, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure><p><strong>view函数说明</strong></p><p>view 方法是 PyTorch 中 tensor 的一种 reshape 操作。它将一个 tensor 的shape 变成给定的形状。</p><p>在这段代码中， dec_logits.view(-1, dec_logits.size(-1)) 表示将dec_logits tensor 从原来的 shape 变成了一个新的 shape，其中第一维是-1，这意味着该维的长度是自动计算的（其他维度的长度已经确定了），第二维是dec_logits.size(-1)，这是一个数字，代表 dec_logits tensor的最后一维的长度。</p><p><strong>forward函数说明：</strong></p><p>通过进行一次前向传播的操作（比如 output = model(input)）时，PyTorch内部会对模型中每一个模块（包括 Encoder 和 EncoderLayer）中的 forward函数进行调用，以计算输出结果。</p><p>如果不手动定义 forward 函数，那么模型将不会被调用。因此，forward函数是必须被定义的，用于计算模型的前向传播过程。</p><h1 id="模型调用-损失函数-优化器">模型调用 &amp; 损失函数 &amp;优化器</h1><p>这段代码调用了Transformer模型，并设置了交叉熵损失函数（将ignore_index参数设置为0），优化器使用随机梯度下降（SGD）算法。（优化器将使用model.parameters()作为参数进行优化，学习率为1e-3，动量为0.99）。</p><p>ignore_index参数被设置为0，这样损失计算将忽略任何索引为0的输入，这通常是为NLP模型中的paddingtoken保留的。</p><p>使用Adam算法对于较小的数据量效果很差</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = Transformer().to(device)</span><br><span class="line"><span class="comment"># 这里的损失函数里面设置了一个参数 ignore_index=0，因为 &quot;pad&quot; 这个单词的索引为 0，这样设置以后，就不会计算 &quot;pad&quot; 的损失（因为本来 &quot;pad&quot; 也没有意义，不需要计算）</span></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=<span class="number">0</span>)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">1e-3</span>, momentum=<span class="number">0.99</span>)</span><br><span class="line"><span class="comment"># optimizer = optim.Adam(model.parameters(), lr=1e-9) # 用adam的话效果不好</span></span><br></pre></td></tr></table></figure><h1 id="训练">训练</h1><p><strong>最后三行代码是在进行一次反向传播迭代的操作。</strong></p><p>分三步执行：optimizer.zero_grad()：对梯度进行初始化，因为pytorch的梯度是累加的，所以每次计算前需要把梯度归零。loss.backward()：计算当前损失函数的梯度，并且完成反向传播。optimizer.step()：执行优化器的更新操作，根据梯度对模型参数进行更新。</p><p>总的来说，这三步代码是完成一次机器学习模型的参数优化的核心过程。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> enc_inputs, dec_inputs, dec_outputs <span class="keyword">in</span> loader:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        enc_inputs: [batch_size, src_len]</span></span><br><span class="line"><span class="string">        dec_inputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        dec_outputs: [batch_size, tgt_len]</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        enc_inputs, dec_inputs, dec_outputs = enc_inputs.to(device), dec_inputs.to(device), dec_outputs.to(device)</span><br><span class="line">        <span class="comment"># outputs: [batch_size * tgt_len, tgt_vocab_size]</span></span><br><span class="line">        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)</span><br><span class="line">        loss = criterion(outputs, dec_outputs.view(-<span class="number">1</span>))  <span class="comment"># dec_outputs.view(-1):[batch_size * tgt_len * tgt_vocab_size]</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Epoch:&#x27;</span>, <span class="string">&#x27;%04d&#x27;</span> % (epoch + <span class="number">1</span>), <span class="string">&#x27;loss =&#x27;</span>, <span class="string">&#x27;&#123;:.6f&#125;&#x27;</span>.<span class="built_in">format</span>(loss))</span><br><span class="line"></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure><p>训练输出如下：</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># OUTPUT</span><br><span class="line">    Epoch: 0001 loss = 2.752963</span><br><span class="line">    Epoch: 0002 loss = 2.625792</span><br><span class="line">    Epoch: 0003 loss = 2.508290</span><br><span class="line">    Epoch: 0004 loss = 2.260287</span><br><span class="line">    Epoch: 0005 loss = 2.026300</span><br><span class="line">    Epoch: 0006 loss = 1.715278</span><br><span class="line">    Epoch: 0007 loss = 1.559423</span><br><span class="line">    Epoch: 0008 loss = 1.339594</span><br><span class="line">    Epoch: 0009 loss = 1.082600</span><br><span class="line">    Epoch: 0010 loss = 0.945365</span><br><span class="line">    Epoch: 0011 loss = 0.758650</span><br><span class="line">    Epoch: 0012 loss = 0.581669</span><br><span class="line">    Epoch: 0013 loss = 0.461126</span><br><span class="line">    Epoch: 0014 loss = 0.340352</span><br><span class="line">    Epoch: 0015 loss = 0.252243</span><br><span class="line">    Epoch: 0016 loss = 0.190971</span><br><span class="line">    Epoch: 0017 loss = 0.161172</span><br><span class="line">    Epoch: 0018 loss = 0.130554</span><br><span class="line">    Epoch: 0019 loss = 0.101027</span><br><span class="line">    Epoch: 0020 loss = 0.093038</span><br><span class="line">    Epoch: 0021 loss = 0.079294</span><br><span class="line">    Epoch: 0022 loss = 0.070799</span><br><span class="line">    Epoch: 0023 loss = 0.062795</span><br><span class="line">    Epoch: 0024 loss = 0.044259</span><br><span class="line">    Epoch: 0025 loss = 0.056274</span><br><span class="line">    Epoch: 0026 loss = 0.033928</span><br><span class="line">    Epoch: 0027 loss = 0.037328</span><br><span class="line">    Epoch: 0028 loss = 0.035663</span><br><span class="line">    Epoch: 0029 loss = 0.032550</span><br><span class="line">    Epoch: 0030 loss = 0.029425</span><br><span class="line">    Epoch: 0031 loss = 0.028057</span><br><span class="line">    Epoch: 0032 loss = 0.024588</span><br><span class="line">    Epoch: 0033 loss = 0.019545</span><br><span class="line">    Epoch: 0034 loss = 0.025953</span><br><span class="line">    Epoch: 0035 loss = 0.018335</span><br><span class="line">    Epoch: 0036 loss = 0.028104</span><br><span class="line">    Epoch: 0037 loss = 0.015952</span><br><span class="line">    Epoch: 0038 loss = 0.014356</span><br><span class="line">    Epoch: 0039 loss = 0.015536</span><br><span class="line">    Epoch: 0040 loss = 0.013210</span><br><span class="line">    Epoch: 0041 loss = 0.015791</span><br><span class="line">    Epoch: 0042 loss = 0.013085</span><br><span class="line">    Epoch: 0043 loss = 0.011149</span><br><span class="line">    Epoch: 0044 loss = 0.009110</span><br><span class="line">    Epoch: 0045 loss = 0.007416</span><br><span class="line">    Epoch: 0046 loss = 0.005960</span><br><span class="line">    Epoch: 0047 loss = 0.006156</span><br><span class="line">    Epoch: 0048 loss = 0.004907</span><br><span class="line">    Epoch: 0049 loss = 0.004867</span><br><span class="line">    Epoch: 0050 loss = 0.005042</span><br><span class="line">    Epoch: 0051 loss = 0.005796</span><br><span class="line">    Epoch: 0052 loss = 0.005398</span><br><span class="line">    Epoch: 0053 loss = 0.004669</span><br><span class="line">    Epoch: 0054 loss = 0.004401</span><br><span class="line">    Epoch: 0055 loss = 0.003372</span><br><span class="line">    Epoch: 0056 loss = 0.002630</span><br><span class="line">    Epoch: 0057 loss = 0.002565</span><br><span class="line">    Epoch: 0058 loss = 0.002309</span><br><span class="line">    Epoch: 0059 loss = 0.003040</span><br><span class="line">    Epoch: 0060 loss = 0.002470</span><br><span class="line">    Epoch: 0061 loss = 0.002096</span><br><span class="line">    Epoch: 0062 loss = 0.002189</span><br><span class="line">    Epoch: 0063 loss = 0.002061</span><br><span class="line">    Epoch: 0064 loss = 0.001174</span><br><span class="line">    Epoch: 0065 loss = 0.001599</span><br><span class="line">    Epoch: 0066 loss = 0.001527</span><br><span class="line">    Epoch: 0067 loss = 0.001685</span><br><span class="line">    Epoch: 0068 loss = 0.001565</span><br><span class="line">    Epoch: 0069 loss = 0.001718</span><br><span class="line">    Epoch: 0070 loss = 0.001291</span><br><span class="line">    Epoch: 0071 loss = 0.001259</span><br><span class="line">    Epoch: 0072 loss = 0.001222</span><br><span class="line">    Epoch: 0073 loss = 0.001179</span><br><span class="line">    Epoch: 0074 loss = 0.000965</span><br><span class="line">    Epoch: 0075 loss = 0.001888</span><br><span class="line">    Epoch: 0076 loss = 0.001052</span><br><span class="line">    Epoch: 0077 loss = 0.000888</span><br><span class="line">    Epoch: 0078 loss = 0.001349</span><br><span class="line">    Epoch: 0079 loss = 0.000916</span><br><span class="line">    Epoch: 0080 loss = 0.001315</span><br><span class="line">    Epoch: 0081 loss = 0.001191</span><br><span class="line">    Epoch: 0082 loss = 0.001341</span><br><span class="line">    Epoch: 0083 loss = 0.001674</span><br><span class="line">    Epoch: 0084 loss = 0.001122</span><br><span class="line">    Epoch: 0085 loss = 0.001133</span><br><span class="line">    Epoch: 0086 loss = 0.000839</span><br><span class="line">    Epoch: 0087 loss = 0.001059</span><br><span class="line">    Epoch: 0088 loss = 0.001204</span><br><span class="line">    Epoch: 0089 loss = 0.001092</span><br><span class="line">    Epoch: 0090 loss = 0.000943</span><br><span class="line">    Epoch: 0091 loss = 0.000699</span><br><span class="line">    Epoch: 0092 loss = 0.001015</span><br><span class="line">    Epoch: 0093 loss = 0.000730</span><br><span class="line">    Epoch: 0094 loss = 0.000795</span><br><span class="line">    Epoch: 0095 loss = 0.000926</span><br><span class="line">    Epoch: 0096 loss = 0.000948</span><br><span class="line">    Epoch: 0097 loss = 0.000945</span><br><span class="line">    Epoch: 0098 loss = 0.000730</span><br><span class="line">    Epoch: 0099 loss = 0.000747</span><br><span class="line">    Epoch: 0100 loss = 0.000749</span><br></pre></td></tr></table></figure><h1 id="测试">测试</h1><p>这段代码是一个贪心解码器(greedydecoder)的实现，其作用是在给定编码输入(enc_input)和起始符号(start_symbol)的情况下，根据给定的模型(model)预测出目标序列(greedy_dec_predict)。</p><p>首先，编码器(encoder)对编码输入(enc_input)进行处理，生成编码输出(enc_outputs)和注意力权值(enc_self_attns)。</p><p>然后初始化解码器(decoder)的输入(dec_input)为一个空的tensor。</p><p>接着，在没有到达终止符的情况下，不断执行以下步骤：</p><ol type="1"><li>将解码器的输入(dec_input)拼接上当前的符号(next_symbol)。</li><li>解码器(decoder)对拼接后的输入(dec_input)、编码输入(enc_input)和编码输出(enc_outputs)进行处理，生成解码输出(dec_outputs)。</li><li>投影层(projection)将解码输出(dec_outputs)映射到词表上，生成预测概率分布(projected)。</li><li>根据预测概率分布(projected)，选择概率最大的下一个词，并将其作为下一个符号(next_symbol)。</li><li>如果下一个符号是终止符，终止循环。</li></ol><p>最后，返回除开初始符号以外的预测的目标序列(greedy_dec_predict)。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decoder</span>(<span class="params">model, enc_input, start_symbol</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;贪心编码</span></span><br><span class="line"><span class="string">    For simplicity, a Greedy Decoder is Beam search when K=1. This is necessary for inference as we don&#x27;t know the</span></span><br><span class="line"><span class="string">    target sequence input. Therefore we try to generate the target input word by word, then feed it into the transformer.</span></span><br><span class="line"><span class="string">    Starting Reference: http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding</span></span><br><span class="line"><span class="string">    :param model: Transformer Model</span></span><br><span class="line"><span class="string">    :param enc_input: The encoder input</span></span><br><span class="line"><span class="string">    :param start_symbol: The start symbol. In this example it is &#x27;S&#x27; which corresponds to index 8</span></span><br><span class="line"><span class="string">    :return: The target input</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    enc_outputs, enc_self_attns = model.encoder(enc_input)</span><br><span class="line">    dec_input = torch.zeros(<span class="number">1</span>, <span class="number">0</span>).type_as(enc_input.data)  <span class="comment"># 初始化一个空的tensor: tensor([], size=(1, 0), dtype=torch.int64)</span></span><br><span class="line">    terminal = <span class="literal">False</span></span><br><span class="line">    next_symbol = start_symbol</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> terminal:</span><br><span class="line">        <span class="comment"># 预测阶段：dec_input序列会一点点变长（每次添加一个新预测出来的单词）</span></span><br><span class="line">        dec_input = torch.cat([dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],</span><br><span class="line">                              -<span class="number">1</span>)</span><br><span class="line">        dec_outputs, _, _ = model.decoder(dec_input, enc_input, enc_outputs)</span><br><span class="line">        projected = model.projection(dec_outputs)</span><br><span class="line">        prob = projected.squeeze(<span class="number">0</span>).<span class="built_in">max</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">False</span>)[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 这行代码替代了softmax层的工作，直接得到概率最大值的词表索引</span></span><br><span class="line">        <span class="comment"># 1. 从 tensor 中删除所有维度大小为1的维：projected.squeeze(0)；</span></span><br><span class="line">        <span class="comment"># 2. 通过 dim=-1 参数，在最后一维（即维度的索引为 -1）上，找到最大的值的索引：max(dim=-1)；</span></span><br><span class="line">        <span class="comment"># 3. 通过 keepdim=False 参数，将最后一维的维度删除，同时返回结果的最大值的索引：[1]。</span></span><br><span class="line">        <span class="comment"># 因此，该行代码得到prob 变量的结果，存储了 projected 处理后得到的最大值的索引。</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 增量更新（我们希望重复单词预测结果是一样的）</span></span><br><span class="line">        <span class="comment"># 我们在预测时会选择性忽略重复的预测的词，只摘取最新预测的单词拼接到输入序列中</span></span><br><span class="line">        next_word = prob.data[-<span class="number">1</span>]  <span class="comment"># 拿出当前预测的单词(数字)。我们用x_t对应的输出z_t去预测下一个单词的概率，不用z_1,z_2..z_&#123;t-1&#125;</span></span><br><span class="line">        next_symbol = next_word</span><br><span class="line">        <span class="keyword">if</span> next_symbol == tgt_vocab[<span class="string">&quot;E&quot;</span>]:</span><br><span class="line">            terminal = <span class="literal">True</span></span><br><span class="line">        <span class="comment"># print(next_word)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># greedy_dec_predict = torch.cat(</span></span><br><span class="line">    <span class="comment">#     [dec_input.to(device), torch.tensor([[next_symbol]], dtype=enc_input.dtype).to(device)],</span></span><br><span class="line">    <span class="comment">#     -1)</span></span><br><span class="line">    greedy_dec_predict = dec_input[:, <span class="number">1</span>:]</span><br><span class="line">    <span class="keyword">return</span> greedy_dec_predict</span><br></pre></td></tr></table></figure><p>这段代码实现了一个简单的预测（因为数据量较小，因此测试集选用训练集中的一句）</p><ul><li>测试集（希望transformer能达到的效果）</li><li>输入："我 有 一 个 女 朋 友"</li><li>输出："i have a girl friend"</li></ul><p>过程如下：</p><ol type="1"><li>定义了一个句子列表sentences，其中包含一个中文句子和对应的空的英文句子。</li><li>使用make_data函数处理句子列表，获得编码句子、解码句子的输入和输出。</li><li>创建一个数据加载器test_loader，用于加载处理后的句子数据。</li><li>使用next函数从数据加载器中读取一个批次的数据，并将其分别赋给编码句子的输入。</li><li>对于每个编码句子，调用greedy_decoder函数，通过训练好的Transformer模型，将其翻译成英文句子。</li><li>最后，输出编码句子和对应的解码句子，以及它们的中英文词语对应关系。</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ==========================================================================================</span></span><br><span class="line"><span class="comment"># 预测阶段</span></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="comment"># enc_input                dec_input           dec_output</span></span><br><span class="line">    [<span class="string">&#x27;我 有 一 个 女 朋 友&#x27;</span>, <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line">test_loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line">enc_inputs, _, _ = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_loader))</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用训练好的Transformer模型将中文句子&#x27;我 有 一 个 女 朋 友&#x27; 翻译成英文句子: &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(enc_inputs)):</span><br><span class="line">    greedy_dec_predict = greedy_decoder(model, enc_inputs[i].view(<span class="number">1</span>, -<span class="number">1</span>).to(device), start_symbol=tgt_vocab[<span class="string">&quot;S&quot;</span>])</span><br><span class="line">    <span class="built_in">print</span>(enc_inputs[i], <span class="string">&#x27;-&gt;&#x27;</span>, greedy_dec_predict.squeeze())</span><br><span class="line">    <span class="built_in">print</span>([src_idx2word[t.item()] <span class="keyword">for</span> t <span class="keyword">in</span> enc_inputs[i]], <span class="string">&#x27;-&gt;&#x27;</span>,</span><br><span class="line">          [idx2word[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> greedy_dec_predict.squeeze()])</span><br></pre></td></tr></table></figure><pre><code>==============================利用训练好的Transformer模型将中文句子&#39;我 有 一 个 女 朋 友&#39; 翻译成英文句子: tensor([1, 2, 3, 4, 9, 6, 7]) -&gt; tensor([ 1,  2,  3,  7,  5, 10])[&#39;我&#39;, &#39;有&#39;, &#39;一&#39;, &#39;个&#39;, &#39;女&#39;, &#39;朋&#39;, &#39;友&#39;] -&gt; [&#39;i&#39;, &#39;have&#39;, &#39;a&#39;, &#39;girl&#39;, &#39;friend&#39;, &#39;.&#39;]</code></pre><p><strong>next 函数说明：</strong></p><p>"next"函数用于返回迭代器的下一个项目，即从迭代器中获取下一个数据项。在这段代码中，使用next(iter(test_loader)) 获取第一个批次的数据，赋值给 enc_inputs, _ , _三个变量。</p><p><strong>因为数据量较小，如果测试集选用新的句子，那么结果就不尽人意</strong></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ==========================================================================================</span></span><br><span class="line"><span class="comment"># 预测阶段</span></span><br><span class="line"><span class="comment"># 测试集</span></span><br><span class="line">sentences = [</span><br><span class="line">    <span class="comment"># enc_input                dec_input           dec_output</span></span><br><span class="line">    [<span class="string">&#x27;我 有 一 个 好 朋 友&#x27;</span>, <span class="string">&#x27;&#x27;</span>, <span class="string">&#x27;&#x27;</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">enc_inputs, dec_inputs, dec_outputs = make_data(sentences)</span><br><span class="line">test_loader = Data.DataLoader(MyDataSet(enc_inputs, dec_inputs, dec_outputs), <span class="number">2</span>, <span class="literal">True</span>)</span><br><span class="line">enc_inputs, _, _ = <span class="built_in">next</span>(<span class="built_in">iter</span>(test_loader))</span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;利用训练好的Transformer模型将中文句子&#x27;我 有 一 个 好 朋 友&#x27; 翻译成英文句子: &quot;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(enc_inputs)):</span><br><span class="line">    greedy_dec_predict = greedy_decoder(model, enc_inputs[i].view(<span class="number">1</span>, -<span class="number">1</span>).to(device), start_symbol=tgt_vocab[<span class="string">&quot;S&quot;</span>])</span><br><span class="line">    <span class="built_in">print</span>(enc_inputs[i], <span class="string">&#x27;-&gt;&#x27;</span>, greedy_dec_predict.squeeze())</span><br><span class="line">    <span class="built_in">print</span>([src_idx2word[t.item()] <span class="keyword">for</span> t <span class="keyword">in</span> enc_inputs[i]], <span class="string">&#x27;-&gt;&#x27;</span>,</span><br><span class="line">          [idx2word[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> greedy_dec_predict.squeeze()])</span><br></pre></td></tr></table></figure><pre><code>==============================利用训练好的Transformer模型将中文句子&#39;我 有 一 个 好 朋 友&#39; 翻译成英文句子: tensor([1, 2, 3, 4, 5, 6, 7]) -&gt; tensor([ 1,  2,  3,  7,  5, 10])[&#39;我&#39;, &#39;有&#39;, &#39;一&#39;, &#39;个&#39;, &#39;好&#39;, &#39;朋&#39;, &#39;友&#39;] -&gt; [&#39;i&#39;, &#39;have&#39;, &#39;a&#39;, &#39;girl&#39;, &#39;friend&#39;, &#39;.&#39;]</code></pre><h1 id="参考链接">参考链接</h1><h2 id="本文详细代码">本文详细代码</h2><ol type="1"><li><p>Colab:</p><p><ahref="https://github.com/serika-onoe/transformer_reproduction/blob/main/Transformer_Pytorch.ipynb">https://github.com/serika-onoe/transformer_reproduction/blob/main/Transformer_Pytorch.ipynb</a></p></li><li><p>Github:</p><p><ahref="https://github.com/serika-onoe/transformer_reproduction">https://github.com/serika-onoe/transformer_reproduction</a></p></li></ol><h2 id="参考网站">参考网站</h2><ol type="1"><li><p>Transformer 的 PyTorch 实现:</p><p><ahref="https://wmathor.com/index.php/archives/1455/">https://wmathor.com/index.php/archives/1455/</a></p></li><li><p>手把手教你用Pytorch代码实现Transformer模型（超详细的代码解读）:</p><p><ahref="https://blog.csdn.net/qq_43827595/article/details/120394042">https://blog.csdn.net/qq_43827595/article/details/120394042</a></p></li></ol><h2 id="完整代码实现">完整代码实现</h2><ul><li><p>哈佛大学NLP组的notebook，很详细文字和代码描述，用pytorch实现</p><p><ahref="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p></li><li><p>Google的TensorFlow官方的，用tf keras实现</p><p><ahref="https://www.tensorflow.org/tutorials/text/transformer">https://www.tensorflow.org/tutorials/text/transformer</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;本文主要介绍一下如何使用 PyTorch 复现
Transformer，实现简单的机器翻译任务。Transformer的原理部分可以参考我的这篇文章:&lt;a
href=&quot;https://serika-onoe.github.io/2023/01/11/Transforme</summary>
      
    
    
    
    <category term="笔记" scheme="https://serika-onoe.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="科研" scheme="https://serika-onoe.github.io/tags/%E7%A7%91%E7%A0%94/"/>
    
    <category term="项目" scheme="https://serika-onoe.github.io/tags/%E9%A1%B9%E7%9B%AE/"/>
    
    <category term="transformer" scheme="https://serika-onoe.github.io/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>Transformer解析，适合没有NLP基础的小白入门 （下）</title>
    <link href="https://serika-onoe.github.io/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/"/>
    <id>https://serika-onoe.github.io/2023/01/11/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8B%EF%BC%89/</id>
    <published>2023-01-11T09:00:10.000Z</published>
    <updated>2023-01-30T12:02:00.031Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>Transformer解析博客分为上下两篇，您现在阅读的是下篇——关于Transformer的详解，在阅读本篇博客之前了解过Encoder-Decoder和Attention机制可能会更有帮助，因此可以参考我的上篇博客，里面我也做了详细讲解和简单举例，上篇博客地址：<ahref="https://serika-onoe.github.io/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%20%EF%BC%88%E4%B8%8A%EF%BC%89/">Transformer解析，适合没有NLP基础的小白入门（上）</a></p><p>网上可以学习到Transformer的资料很多，大佬们大多写得都很专业，不过对于像我这样的初学者来说，很容易就感觉自己掉进了专业术语的坑里爬不上来了。所以借此机会，我总结出了这篇博客，在一边梳理脉络的同时，也希望能够帮到屏幕面前的读者。</p><h1 id="transformer之前的通用模型">Transformer之前的通用模型</h1><p>在Transformer之前，递归神经网络(RNN)一直是处理序列数据的首选方法，大家做机器翻译用的最多的就是基于RNN的Encoder-Decoder模型。</p><imgsrc="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-02.jpg" /><center><code>图1: RNN的工作方式</code></center><p>输入:</p><ul><li><p>输入向量 <span class="math inline">\(\vec{x_t}\)</span>(编码词)</p></li><li><p>隐藏状态向量 <spanclass="math inline">\(\vec{h_{t-1}}\)</span>（包含当前块之前的序列状态)</p></li></ul><p>输出：</p><ul><li>输出向量 <span class="math inline">\(\vec{o_t}\)</span></li></ul><p>权重：</p><ul><li><p><span class="math inline">\({W}\)</span>—— <spanclass="math inline">\(\vec{x_t}\)</span> 和 <spanclass="math inline">\(\vec{h_t}\)</span> 之间的权重</p></li><li><p><span class="math inline">\({V}\)</span>—— <spanclass="math inline">\(\vec{ h_{t-1} }\)</span> 和 <spanclass="math inline">\(\vec{h_t}\)</span> 之间的权重</p></li><li><p><span class="math inline">\({U}\)</span>—— <spanclass="math inline">\(\vec{h_t}\)</span> 和 <spanclass="math inline">\(\vec{o_t}\)</span> 之间的权重</p></li></ul><p>RNN的工作方式类似于前馈神经网络，它会将输入序列一个接一个地读取。因此在基于RNN的Encoder-Decoder模型中，编码器的目标是从顺序输入中提取数据，并将其编码为向量（即输入的表示形式）。而解码器代替输出固定长度向量的分类器，与单独使用输入中的每个符号的编码器一样，解码器在多个时间步长内生成每个输出符号。</p><imgsrc="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-03.jpg" /><center><code>图2: Encoder-Decoder进行英-法翻译的例子</code></center><p><br/><br/>例如，在机器翻译中，输入是英文句子，输出是翻译出的法语句子。Encoder将按顺序展开每个单词，并形成输入英文句子的固定长度向量表示（也就是上篇博客中的<spanclass="math inline">\(C\)</span>）。然后Decoder将固定长度的向量表示作为输入，依次产生每个法语单词，形成翻译后的法语句子。</p><h2 id="原有模型的缺陷">原有模型的缺陷</h2><p>原有的模型，即基于RNN的Encoder-Decoder存在一些问题：</p><ol type="1"><li><p>训练速度慢：输入数据需要一个接一个地顺序处理，这种串行的循环过程不适用于擅长并行计算的GPU。</p></li><li><p>难以处理长序列：</p><ul><li>如果输入序列太长，会出现梯度消失和爆炸问题。一般在训练过程中会在loss中看到NaN（Nota Number）。这些也称为 RNN 中的长期依赖问题。</li><li>上下文向量长度固定，使用固定长度的向量表示输入序列来解码一个全新的句子是很困难的。如果输入序列很大，则上下文向量无法存储所有信息。此外，也很难区分具有相似单词但具有不同含义的句子。</li></ul></li></ol><p>第1个问题很好理解，不过第2个问题中，关于梯度消失和爆炸的问题，可以先看看以下的补充说明。</p><h3 id="什么是梯度消失和梯度爆炸">什么是梯度消失和梯度爆炸</h3><p>高中数学有教过，我们可以利用微分的方法来求函数的最大值与最小值。在机器学习中，梯度是一个向量，它表示网络误差函数关于所有权重的偏导数。梯度优化算法就是<strong>通过不断计算梯度，并使用梯度优化算法来调整权重，使得代价函数CostFunction (预测结果究竟与实际答案差了多少)越来越小</strong>，也意味着网络能够更好地适应训练数据。当网络的CostFunction达到最小值时，网络就能对新的数据进行较好的预测。</p><p>当我们的代价函数是线性函数时，我们就能够用梯度下降法(GradientDescent)来快速的求出代价函数（在图中记为<spanclass="math inline">\(J(w)\)</span>）的最小值，如图：</p><img src="https://i.imgur.com/EFs14Nt.png" /><center><code>图3: 梯度下降法的示意图</code></center><p><br/><br/>而梯度消失和梯度爆炸是深度神经网络训练中的两种典型问题。</p><p>梯度消失（vanishinggradient）指在深层网络训练中，由于梯度的较小值逐层传递，导致较深层的权值参数的更新量非常小，趋近于0。这样会导致较深层网络的参数无法得到有效更新，从而使整个网络无法学习。</p><p>梯度爆炸（explodinggradient）指在深层网络训练中，由于梯度的较大值逐层传递，导致较深层的权值参数的更新量非常大，甚至无限大。这样会导致较深层网络的参数更新量过大，从而使整个网络无法学习。</p><p>如果有疑问请看下面RNN的例子：</p><h3 id="rnn梯度消失和爆炸的原理">RNN梯度消失和爆炸的原理</h3><p><strong>RNN的统一定义为</strong></p><p><span class="math display">\[\begin{equation}h_t = f\left(x_t, h_{t-1};\theta\right)\end{equation}\]</span></p><ul><li><spanclass="math inline">\(h_t\)</span>是每一步的输出，也就是隐藏状态，由当前输入<spanclass="math inline">\(x_t\)</span>和前一时刻的输出<spanclass="math inline">\(h_{t-1}\)</span>共同决定</li><li><span class="math inline">\(\theta\)</span>则是可训练的参数</li></ul><p>(在做基本分析时，我们可以假设<spanclass="math inline">\(h_t,x_t,\theta\)</span>都是一维的，这可以让我们获得最直观的理解，并且其结果对高维情形仍有参考价值。)</p><p><strong>RNN梯度的表达式为</strong></p><p><span class="math display">\[\begin{equation}\frac{d h_t}{d\theta} = \frac{\partial h_t}{\partialh_{t-1}}\frac{d h_{t-1}}{d\theta} + \frac{\partial h_t}{\partial\theta}\end{equation}\]</span></p><p>这个公式的意思是，我们可以递推地计算出每一个时间步的隐藏状态对于参数的偏导数，也就是梯度。这样就可以用梯度下降算法来更新网络中的参数，使得网络能够更好地适应训练数据。</p><p>可以看到，其实RNN的梯度也是一个RNN，当前时刻梯度<spanclass="math inline">\(\frac{d h_t}{d\theta}\)</span>是前一时刻梯度<spanclass="math inline">\(\frac{dh_{t-1}}{d\theta}\)</span>与当前运算梯度<spanclass="math inline">\(\frac{\partial h_t}{\partial\theta}\)</span>的函数。同时，从上式我们就可以看出，其实梯度消失或者梯度爆炸现象几乎是必然存在的：</p><ul><li>当<span class="math inline">\(\left|\frac{\partial h_t}{\partialh_{t-1}}\right| &lt;1\)</span>时，意味着历史的梯度信息逐步衰减，因此步数多了梯度必然消失（好比<spanclass="math inline">\(\lim\limits_{n\to\infty} 0.9^n \to0\)</span>）；</li><li>当<span class="math inline">\(\left|\frac{\partial h_t}{\partialh_{t-1}}\right| &gt;1\)</span>时，意味着历史的梯度信息逐步增强，因此步数多了梯度必然爆炸（好比<spanclass="math inline">\(\lim\limits_{n\to\infty} 1.1^n \to\infty\)</span>）</li><li>也有可能有些时刻大于1，有些时刻小于1，最终稳定在1附近，但这样概率很小，需要很精巧地设计模型和参数才行。</li></ul><imgsrc="https://editor.analyticsvidhya.com/uploads/51317greatlearning.png" /><center><code>图4: 梯度爆炸和梯度消失的示意图</code></center><p><br/><br/></p><blockquote><p>梯度消失就是梯度变成零吗？</p></blockquote><p>并不是，我们刚刚说梯度消失是<spanclass="math inline">\(\left|\frac{\partial h_t}{\partialh_{t-1}}\right|\)</span>一直小于1，历史梯度不断衰减，但不意味着总的梯度就为0了，具体来说，一直迭代下去，我们有</p><p><span class="math display">\[\begin{equation}\begin{aligned}\frac{d h_t}{d\theta} =&amp;\frac{\partial h_t}{\partial h_{t-1}}\frac{d h_{t-1}}{d\theta} +\frac{\partial h_t}{\partial \theta}\\=&amp; \frac{\partial h_t}{\partial \theta}+\frac{\partial h_t}{\partialh_{t-1}}\frac{\partial h_{t-1}}{\partial \theta}+\frac{\partialh_t}{\partial h_{t-1}}\frac{\partial h_{t-1}}{\partialh_{t-2}}\frac{\partial h_{t-2}}{\partial \theta}+\dots\\\end{aligned}\end{equation}\]</span></p><p>显然，其实只要<span class="math inline">\(\frac{\partialh_t}{\partial\theta}\)</span>不为0，那么总梯度为0的概率其实是很小的；但是一直迭代下去的话，那么<spanclass="math inline">\(\frac{\partial h_1}{\partial\theta}\)</span>这一项前面的稀疏就是<spanclass="math inline">\(t-1\)</span>项的连乘<spanclass="math inline">\(\frac{\partial h_t}{\partialh_{t-1}}\frac{\partial h_{t-1}}{\partial h_{t-2}}\cdots\frac{\partialh_2}{\partialh_1}\)</span>，如果它们的绝对值都小于1，那么结果就会趋于0，这样一来，<spanclass="math inline">\(\frac{dh_t}{d\theta}\)</span>几乎就没有包含最初的梯度<spanclass="math inline">\(\frac{\partial h_1}{\partial\theta}\)</span>的信息了。</p><p>这才是RNN中梯度消失的含义：距离当前时间步越长，那么其反馈的梯度信号越不显著，最后可能完全没有起作用，这就意味着RNN对长距离语义的捕捉能力失效了。<strong>说白了，优化过程都跟长距离的反馈没关系，那我们怎么保证学习出来的模型能有效捕捉长距离呢？</strong></p><p>所以对于一般的RNN模型来说，步数多了，梯度消失或爆炸几乎都是不可避免的，我们只能通过让RNN执行有限的步数来缓解这个问题。直到上世纪末提出的LSTM极大地改进了这个问题。</p><h3 id="针对梯度消失爆炸的改进lstm">针对梯度消失/爆炸的改进：LSTM</h3><p><ahref="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">[论文1]Hochreiter&amp; Schmidhuber. 1997. Long Short-Term Memory</a> 引入了长短期记忆(LSTM)网络，其明确设计用于避免长期依赖问题。每个LSTM单元允许过去的信息跳过当前单元的所有处理并移动到下一个单元；这允许内存保留更长时间，并使数据能够不变地与其一起流动。LSTM由一个决定要存储哪些新信息的输入门和一个决定要删除哪些信息的遗忘门组成。</p><imgsrc="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-05.jpg" /><center><code>图5: LSTM的工作方式</code></center><p><br/><br/> 当然，LSTM 具有改进的记忆力，能够处理比 RNN更长的序列。然而，由于LSTM更加复杂，使得LSTM与RNN相比运行更慢。</p><h3id="针对上下文向量长度固定的改进attention">针对上下文向量长度固定的改进：Attention</h3><figure><imgsrc="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-06.jpg"alt="使用固定长度的向量来表示输入序列。" /><figcaptionaria-hidden="true">使用固定长度的向量来表示输入序列。</figcaption></figure><center><code>图6: 长序列输入到原有模型的例子</code></center><p><br/><br/>假设有一段长文本（输入），将其记忆下来（转换为固定长度的向量），然后在不回顾这段文本的情况下，按顺序翻译出整段文本（输出）。这很难，也不是我们的目标做法。相反，当我们翻译一句话时，我们会一部分一部分地看，逐段关注句子的某一部分，从而保证翻译的准确性，这就引入了下文所提到的Attention机制。</p><p><a href="https://arxiv.org/abs/1409.0473">[论文2]Dzmitry Bahdanau.2014. Neural Machine Translation by Jointly Learning to Align andTranslate</a>提出了一种在编码器-解码器模型中搜索与预测目标词相关的源句子部分的方法，也就是Attention机制，我们可以使用Attention机制翻译相对较长的句子而不影响其性能。例如，翻译成“noir”（在法语中意为“黑色”），注意力机制将关注单词“black”和可能的“cat”，而忽略句子中的其他单词。</p><img src="https://i.imgur.com/5y6SCvU.png" /><center><code>图7: Attention机制让输出单词关注相关的输入部分</code></center><imgsrc="https://jinglescode.github.io/assets/img/posts/illustrated-guide-transformer-07.jpg" /><center><code>图8: Bahadanau的论文模型在测试集上长序列的 BLEU 分数明显更优</code></center><p><br/><br/>由此可见，Attention机制提高了编码器-解码器网络的性能，但速度瓶颈仍然是RNN必须逐字处理的工作机制。</p><p>很自然的，我们会接着考虑：<strong>可以用更好的模型替换掉RNN这种顺序结构的模型吗？</strong></p><p>答案是：<strong>Yes, attention is all you need!</strong></p><p>哈哈，有点一语双关的感觉（说不定GoogleBrain团队当时起名的时候就是这么想的）。在2017年，我们得到了一个令人满意的答案，一款名为Transformer的“神器”横空出世，至今风头不减当年。它也是我们这篇文章要探讨的主题。</p><h1 id="transformer简介">Transformer简介</h1><p>正如刚刚提到的，<ahref="https://arxiv.org/abs/1706.03762">[论文3]Ashish Vaswani., 2017 .Attention Is All You Need</a>第一次正式介绍了一款在翻译领域超越了RNN的新模型Transformer，<strong>Transformer是一种Encoder-Decoder架构，使用Attention机制来处理输入和生成输出。</strong></p><h1 id="transformer对比rnn">Transformer对比RNN</h1><p>Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成，更准确地讲，<strong>Transformer由且仅由SelfAttenion和Feed Forward NeuralNetwork组成</strong>。采用Attention机制是因为考虑到RNN（或者LSTM，GRU等）的计算限制是顺序处理的，也就是说RNN相关算法只能从左向右依次计算或者从右向左依次计算，这种机制带来了两个问题：</p><ol type="1"><li><p>时间<span class="math inline">\(t\)</span>的计算依赖<spanclass="math inline">\(t-1\)</span>时刻的计算结果，这样限制了模型的并行能力。</p></li><li><p>顺序计算的过程中RNN会因为长期依赖导致信息丢失问题。</p></li></ol><p>Transformer的提出解决了上面两个问题：</p><ol type="1"><li><p>不采用类似RNN的顺序结构，而是具有并行性，符合现有的GPU框架。</p></li><li><p>使用了Attention机制，将序列中的任意两个位置之间的距离缩小为一个常量，采用了和LSTM不同的思路，从而允许处理不同长度的输入序列，并且摆脱了长期依赖的影响。</p></li></ol><p>这也就是为什么它在机器翻译任务种打败了以前基于RNN的Encoder-Decoder模型，并且在其他应用中也非常受欢迎的原因。</p><h1 id="transformer原理">Transformer原理</h1><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer.jpg" width = "50%" /><center><code>图9: Transformer模型的整体架构</code></center><p><br/><br/>上图是Transformer的整体架构图，结构上看起来和Encoder-Decoder模型很相似，左边是Encoder部分，右边是Decoder部分。为了方便理解，下面把Transformer分成四个部分进行详细说明:</p><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts.jpg" width = "50%" /><center><code>图10: Transformer模型可分为4个组成部分</code></center><p>简单介绍一下各部分的任务：</p><ul><li><p><strong>Input</strong>：输入是单词的Embedding再加上位置编码，然后进入编码器或解码器。</p></li><li><p><strong>Encoder</strong>：这个结构可以循环很多次（N次），也就是说有很多层（N层）。每一层又可以分成Attention层和全连接层，再额外加了一些处理，比如SkipConnection，做跳跃连接，然后还加了Normalization层。其实它本身的模型还是很简单的。</p></li><li><p><strong>Decoder</strong>：同样可以循环N次，第一次输入是前缀信息，之后的就是上一次产出的Embedding，加入位置编码，然后进入一个可以重复很多次的模块。该模块可以分成三块来看，第一块也是Attention层，第二块是CrossAttention，不是SelfAttention，第三块是全连接层。也用了跳跃连接和Normalization。</p></li><li><p><strong>Output</strong>：最后的输出要通过Linear层（全连接层），再通过Softmax做预测。</p></li></ul><p>以英-中翻译为例：假设我们输入<code>"Why do we work?"</code>，输出可以是<code>"为什么我们要工作？"</code>。那么Transformer的工作步骤是：</p><ol type="1"><li>输入自然语言序列到编码器: Why do we work?</li><li>编码器输出的隐藏层, 再输入到解码器;</li><li>输入<spanclass="math inline">\(&lt;start&gt;\)</span>(起始)符号到解码器;</li><li>得到第一个字"为";</li><li>将得到的第一个字"为"落下来再输入到解码器;</li><li>得到第二个字"什";</li><li>将得到的第二字再落下来,重复5、6步的相关动作依次生成“么”、“我”、“们”、“要”、“工”、“作”、“？”，直到解码器输出<spanclass="math inline">\(&lt;end&gt;\)</span>(终止符),则代表序列生成完成。</li></ol><p>我们可以对Transformer的工作过程1~6进行可视化，如下所示:</p><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_process.png" width = "80%" /><center><code>图11: 图解Transformer的工作过程</code></center><h2 id="input">Input</h2><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(1).png" width = "100%" /><center><code>图12: Transformer模型的输入部分</code></center><p><br/><br/> 我们同样以上篇采用过的英-中翻译例子，我们输入"Tom chaseJerry"，期待输出的翻译结果为"汤姆追逐杰瑞"。</p><p>Transformer的Decoder的输入与Encoder的输入处理方法步骤是一样的，一个接受source数据，一个接受target数据，对应例子里面就是：Encoder接受英文"TomchaseJerry"，Decoder接受中文"汤姆追逐杰瑞"。当然对Decoder来说，只是在有target数据时（也就是在进行有监督训练时）才会接受OutputsEmbedding，进行预测时则不会接收。那么下面，以Encoder为例，描述输入过程和细节分析：</p><ol type="1"><li><p>首先，向Transformer输入文本 <code>"Tom chase Jerry"</code>。</p></li><li><p>随后，Transformer会将原始的英文句子"Tom chaseJerry"进行分词(tokenization)，比如得到单词序列['[START]', 'Tom','chase', 'Jerry','[END]']。请注意，分词后的token包括“[START]”和“[END]”标记。</p></li><li><p>接下来，将每个单词映射到对应的词向量上。<strong>实际上Transformer使用的是512维的向量</strong>，那么假设我们使用4维的词向量表示单词，那么对于单词'[START]','Tom', 'chase', 'Jerry', '[END]'，它们的词向量可能是：</p></li></ol><p>        <span class="math inline">\(v_{[START]} :[-0.1,0.2,-0.3,0.4]\)</span></p><p>        <span class="math inline">\(v_{Tom} : [0.5, 0.2, -0.1,0.3]\)</span></p><p>        <span class="math inline">\(v_{chase} : [-0.2, 0.4, 0.1,0.6]\)</span></p><p>        <span class="math inline">\(v_{Jerry} : [-0.2, 0.3, 0.1,-0.5]\)</span></p><p>        <span class="math inline">\(v_{[END]} :[0.3,-0.2,0.1,-0.4]\)</span></p><ol start="4" type="1"><li>通过使用sin和cos函数来生成位置向量，这种方式可以向模型描述各个单词之间的顺序关系，并且能够在维度空间上均匀分布位置向量。可以假设'[START]'的位置编号为1，"Tom"的位置编号为2，"chase"的位置编号为3，"Jerry"的位置编号为4，"[END]"的位置编号为5。那么它们位置向量可能是：</li></ol><p>        <span class="math inline">\(p_1 =[sin(\frac{1}{10000^{2*\frac{1}{4}}}),cos(\frac{1}{10000^{2*\frac{1}{4}}}),sin(\frac{2}{10000^{2*\frac{1}{4}}}),cos(\frac{2}{10000^{2*\frac{1}{4}}}) ]\)</span></p><p>        <span class="math inline">\(p_2 =[sin(\frac{2}{10000^{2*\frac{1}{4}}}),cos(\frac{2}{10000^{2*\frac{1}{4}}}),sin(\frac{3}{10000^{2*\frac{1}{4}}}),cos(\frac{3}{10000^{2*\frac{1}{4}}}) ]\)</span></p><p>        <span class="math inline">\(p_3 =[sin(\frac{3}{10000^{2*\frac{1}{4}}}),cos(\frac{3}{10000^{2*\frac{1}{4}}}),sin(\frac{4}{10000^{2*\frac{1}{4}}}),cos(\frac{4}{10000^{2*\frac{1}{4}}}) ]\)</span></p><p>        <span class="math inline">\(p_4 =[sin(\frac{4}{10000^{2*\frac{1}{4}}}),cos(\frac{4}{10000^{2*\frac{1}{4}}}),sin(\frac{5}{10000^{2*\frac{1}{4}}}),cos(\frac{5}{10000^{2*\frac{1}{4}}}) ]\)</span></p><p>        <span class="math inline">\(p_5 =[sin(\frac{5}{10000^{2*\frac{1}{4}}}),cos(\frac{5}{10000^{2*\frac{1}{4}}}),sin(\frac{6}{10000^{2*\frac{1}{4}}}),cos(\frac{6}{10000^{2*\frac{1}{4}}}) ]\)</span></p><ol start="5" type="1"><li>最后，输入到Transformer中的序列就是由词向量和位置向量相加得到的，例如，“Tomchase Jerry”的输入序列可能是:</li></ol><p>        [<span class="math inline">\(v_ {[START]}\)</span> + <spanclass="math inline">\(p_1\)</span> , <spanclass="math inline">\(v_{Tom}\)</span> + <spanclass="math inline">\(p_2\)</span>, <spanclass="math inline">\(v_{chase}\)</span> + <spanclass="math inline">\(p_3\)</span>, <spanclass="math inline">\(v_{Jerry}\)</span> + <spanclass="math inline">\(p_4\)</span>, <span class="math inline">\(v_{[END]}\)</span> + <span class="math inline">\(p_5\)</span>]</p><p>下面分别介绍过程中相关的知识点：</p><h3 id="分词tokenization">分词(Tokenization)</h3><blockquote><p>什么是分词</p></blockquote><p>Transformer模型的输入通常是序列数据，如文本、语音等。这些数据在输入之前需要进行预处理，其中一个重要的步骤就是分词。<strong>分词是获取词向量之前的一个必要步骤。</strong></p><p>分词是NLP的一个重要概念，<strong>表示将文本(text)切分成符号(token)的过程。</strong>token可以是以下三种类型：</p><ol type="1"><li><p>单词 (word) —— 例如，短语“dogs likecats”由三个词标记组成：“dogs”、“like”和“cats”。</p></li><li><p>字符 (character) —— 例如，短语“yourfish”由九个字符标记组成。<strong>（请注意，空格算作标记之一）</strong></p></li><li><p>子词 (subword) ——其中单个词可以是单个标记或多个标记。子词由词根、前缀或后缀组成。例如，使用子词作为标记的语言模型可能会将单词“dogs”视为两个标记（词根“dog”和复数后缀“s”）。相同的语言模型可能会将单个词“更高”视为两个子词（词根“high”和后缀“er”）。</p></li></ol><blockquote><p>分词的作用？</p></blockquote><p>通过分词，模型可以将文本分成若干个单独的词汇单元，可以减少翻译系统需要处理的信息量，提高翻译效率和准确性，同时更好地维护语言的语法结构。</p><h3 id="嵌入embedding">嵌入(Embedding)</h3><p>NLP中，<strong>使用分词后的词向量作为模型输入</strong>是常见的做法，而词向量是一种特定类型的嵌入。</p><blockquote><p>什么是嵌入？</p></blockquote><p>嵌入是NLP中使用的一种技术，以机器学习模型能够理解的数字格式表示单词、短语甚至整个句子。而其中的词向量用于在多维空间中表示单词。</p><blockquote><p>嵌入的作用？</p></blockquote><p>嵌入的目标在于<strong>通过机器学习模型能够理解的方式捕捉一个词的含义和上下文，并将词语的语义信息转化为数字，使得它们可以被计算机理解。</strong></p><p>当然从另一种角度来说，也可以认为嵌入是一种降维的形式。</p><p>比如除了嵌入之外，单词或句子也可以被表示为onehot编码向量，它是高维和稀疏的，当然它虽然避免了线性不可分的问题，但也意味着向量中的大多数元素都是零。这可能会导致计算效率低下，而且机器学习模型也难以通过学习来理解单词或句子的含义和相互关系。</p><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/onehot.png" width = "80%" /><center><code>图13: 图解one hot的编码方式</code></center><p><br/><br/>相反，嵌入是一种以较低维度的密集格式表示单词或句子的方法，更适合机器学习模型。<strong>通过将单词或句子映射到一个较低的维度空间，嵌入可以捕捉到单词或句子的含义和相互关系，同时舍弃不太重要的信息。</strong></p><p><strong>常见的词向量编码方式是word2vec。</strong>相比于one-hot编码只允许我们将单词作为单个不同的条目来解释，word2vec允许我们寻找每个单词和其他单词的关系，从而创建更好的特征表示。</p><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/word2.png" width = "80%" /><center><code>图14: 图解word2vec的编码方式</code></center><p><br/><br/>word2vec模型使用神经网络来学习一个词的向量表示。它将每个词映射到一个高维向量，其中语义相似的词在向量空间中是紧密相连的。例如，在word2vec模型中，"banana"这个词的向量表示可能是[-0.2,0.5, 0.1, 0.3,...]，代表这个词的含义和上下文，(一个词向量往往是300-1000维，向量中的每个元素代表这个词的意义或上下文的一个维度，单个数字的含义本身不可解释，只有在与其他词或句子的向量相关时才有意义)。</p><blockquote><p>输入的embedding是否需要经过训练</p></blockquote><p>将单词<spanclass="math inline">\(x\)</span>的embedding输入encoder，有两种常见的选择：</p><ol type="1"><li><p>使用Pre-trained的<strong>embeddings并固化</strong>，这种情况下embedding取自一个预先训练好的模型，在训练过程中不更新。实际就是一个LookupTable（查找表）。</p></li><li><p>对其进行随机初始化（当然也可以选择Pre-trained的结果），但<strong>设为Trainable</strong>。这样在training过程中不断地对embeddings进行改进。即End2End（端到端）训练方法，意味着模型从头到尾都被训练，所有的参数，包括嵌入都在训练过程中被更新。<strong>这也是Transformer选择的做法。</strong></p></li></ol><h3 id="位置编码positional-encoding">位置编码(Positional Encoding)</h3><blockquote><p>为什么需要知道每个单词的位置，并且添加位置编码呢？</p></blockquote><p>首先，咱们知道，一句话中同一个词如果的出现位置不同，意思可能发生翻天覆地的变化，就比如：我欠他100W和他欠我100W。这两句话的意思一个地狱一个天堂。可见获取词语出现在句子中的位置信息是一件很重要的事情。</p><p>而Transformer没有用RNN也没有卷积，它使用的注意力机制(主要是由于selfattention)，不能获取词语位置信息，就算打乱一句话中词语的位置，每个词还是能与其他词之间计算attention值。所以为了让模型能利用序列的顺序，必须输入序列中词的位置，所以Transformer采用的方法是给每一个词向量，包括包括“[START]”和“[END]”都需要添加位置编码。</p><blockquote><p>怎么得到positional encoding呢？</p></blockquote><p>Transformer使用的是正余弦位置编码。位置编码通过使用不同频率的正弦、余弦函数生成，然后和对应位置的词向量相加，位置向量维度必须和词向量的维度一致。过程如上图，PE（positionalencoding）计算公式如下：</p><p><span class="math display">\[PE_{(pos,2i)} =sin(\frac{pos}{10000^{\frac{2 i}{d_{model}}}})\]</span> <spanclass="math display">\[PE_{(pos,2i+1)} = cos(\frac{pos}{10000^{\frac{2i}{d_{model}}}})\]</span></p><p>解释一下上面的公式：</p><ul><li><p><spanclass="math inline">\(pos\)</span>表示单词在句子中的绝对位置，<spanclass="math inline">\(pos=0, 1, 2, \dots\)</span>，例如：Jerry在"Tomchase Jerry"中的pos=2；</p></li><li><p><spanclass="math inline">\(d_{model}\)</span>表示词向量的维度，一般<spanclass="math inline">\(d_{model}\)</span>=512；2i和2i+1表示奇偶性，i表示词向量中的第几维，例如这里<spanclass="math inline">\(d_{model}\)</span>=512，故<spanclass="math inline">\(i=0, 1, 2, \dots, 255\)</span>。</p></li></ul><p>至于上面两个公式是怎么得来的，其实不重要，很有可能是作者根据经验自己造的，而且公式也不唯一，后续Google在Bert中的采用类似词向量的方法通过训练PE，说明<strong>这种求位置向量的方法还是存在一定问题滴</strong>。</p><blockquote><p>为什么是将positional encoding与词向量相加，而不是拼接呢？</p></blockquote><p>事实上，拼接或者相加都可以，只是词向量本身的维度（512维）就已经蛮大了。再拼接一个512维的位置向量（变成1024维）这样训练起来会相对慢一些，影响学习效率。两者既然效果差不多，那当然是选择学习难度较小的相加了。</p><p>这段代码实现了Transformer模型中的位置编码，主要用于确定输入序列中每个单词的位置信息问题。</p><ul><li>d_model: 定义词向量的维度。</li><li>dropout: 一种正则化方式，随机让部分网络参数为0，以防过拟合。</li><li>max_len: 输入句子的最大长度。</li></ul><p>在初始化中，首先使用nn.Dropout类创建了一个dropout层。然后根据论文中的公式，预先计算出位置编码，并将其存储在pe变量中。</p><p>在forward函数中，将输入x加上pe变量中对应位置的位置编码，最后进行dropout操作（一种正则化方式，在训练时会随机将部分网络参数设置为0，从而防止过拟合）。</p><p>需要注意的是，这里的<strong>位置编码是在计算时预先计算好了并存储下来，而不是在运行时动态计算，这样可以减少计算量</strong>。</p><h2 id="encoder">Encoder</h2><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(2).png" width = "40%" /><center><code>图15: Transformer模型的编码器部分</code></center><p><br/><br/>在论文中，有6层编码器，即“Nx”的N=6。编码器的每一层是由4个sub-layer（子层）组成的：第一个子层是多头注意力机制 (multi-head self-attentionmechanism)，它负责计算输入序列之间的关系并生成新的表示。第二个子层是残差连接 (residual connection)，它将第一个子层的输出与输入相加。 第三个子层是归一化(normalization)，它对第二个子层的输出进行归一化。第四个子层是全连接前馈层（feed-forwardlayer），它对第三个子层的输出进行非线性变换。</p><p>这段代码实现了一个transformer编码器中的一个编码层EncoderLayer。这个编码层由两部分组成，分别是自注意力机制self-attn和前馈网络feed_forward。在初始化时，通过传入size、self_attn、feed_forward和dropout参数来初始化编码层。</p><p>在前向传播过程中，首先使用自注意力机制对输入x进行处理，然后将处理后的结果经过前馈网络进一步处理，最后返回处理结果。</p><p>具体来说，使用sublayer<ahref="x,%20lambda%20x:%20self.self_attn(x,%20x,%20x,%20mask)">0</a>调用SublayerConnection类对输入进行处理，进行自注意力机制。之后使用sublayer<ahref="x,%20self.feed_forward">1</a>将结果经过前馈网络进一步处理。</p><h3 id="multi-head-attention">Multi-Head Attention</h3><p><strong>Multi-Head Attention是SelfAttention机制的进一步细化，因此先从Self Attention讲起：</strong></p><h4 id="从self-attention讲起">从Self Attention讲起</h4><p>假设下面的句子是我们要翻译的输入句子：</p><p><code>The animal didn't cross the street because it was too tired</code></p><p>这句话中的“it”指的是什么？指的是街道还是动物？这对人来说是一个简单的问题，但对算法模型来说却不那么简单。</p><imgsrc="https://jalammar.github.io/images/t/transformer_self-attention_visualization.png" /><center><code>图17: 对单词“it”编码时Attention大部分集中在“The animal”上</code></center><p>在该例子中，当模型处理“it”这个词时，self attention允许它把“it”和“animal”联系起来。而广泛地说，<strong>当模型处理每个单词（输入序列中的每个位置）时，自注意力允许它查看输入序列中的其他位置以寻找有助于更好地编码该单词的线索。</strong></p><p>而上篇我们详细讲解过，Attention的本质，这里我们简单描述下：<strong>Attention实际上做的就是数据库中的检索操作，本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。</strong></p><p>这个类似于搜索引擎或者推荐系统的基本原理，以谷歌Google为例:</p><ul><li>用户给定需查询的问题(Query)</li><li>Google后台有各种文章标题(Key)和文章本身(Value)</li></ul><p>通过定义并计算问题Query与文章标题Key的相关性，用户就可以找到最匹配的文章Value。当然Attention的目标不是通过Query得到Value，而是通过Query得到Value的加权和。</p><p>那么回到计算Self Attention的过程上来，这次我们以新的输入“ThinkingMachines”为例进行过程描述：</p><p><strong>单词级别-第一步：</strong></p><p>从每个编码器的输入向量（词向量+位置编码）创建三个向量：一个Query查询向量、一个Key键向量和一个Value值向量。</p><imgsrc="https://jalammar.github.io/images/t/transformer_self_attention_vectors.png" /><br /><center><code>图18: qkv向量是通过将embedding分别乘以训练的三类权重矩阵而创建的</code></center><p><br/><br/> 比如产生"Thinking"的三个向量的过程如下：</p><ol type="1"><li>将<span class="math inline">\(X_1\)</span>乘以<spanclass="math inline">\(W^Q\)</span>权重矩阵产生<spanclass="math inline">\(q_1\)</span>，即与该词关联的Query向量。</li><li>将<span class="math inline">\(X_1\)</span>乘以<spanclass="math inline">\(W^K\)</span>权重矩阵产生<spanclass="math inline">\(k_1\)</span>，即与该词关联的Key向量。</li><li>将<span class="math inline">\(X_1\)</span>乘以<spanclass="math inline">\(W^V\)</span>权重矩阵产生<spanclass="math inline">\(v_1\)</span>，即与该词关联的Value向量。</li></ol><p>请注意，qkv向量的维度小于embedding向量。它们的维数是64，而嵌入和编码器输入/输出向量的维数是512。它们不必更小，这是一种使multi-headattention（大部分）计算保持不变的架构选择。</p><p><strong>单词级别-第二步</strong></p><p>计算每个单词的分数，分数是通过Query向量与当前正在评分单词的Key向量的点积计算得出的。<strong>当我们在特定位置对单词进行编码时，分数决定了将多少注意力放在输入句子的其他部分。</strong></p><p>假设我们正在计算本例中第一个词“Thinking”的自注意力。我们需要根据这个词对输入句子的每个词进行评分:</p><imgsrc="https://jalammar.github.io/images/t/transformer_self_attention_score.png" /><br /><center><code>图19: 单词的分数是对应位置Query向量和Key向量的点积</code></center><p><strong>单词级别-第三步</strong></p><p>将分数除以8，这个数字是论文中使用的关键向量维度64的平方根。</p><blockquote><p>上式为什么要除以<spanclass="math inline">\(\sqrt{d_k}\)</span>呢？</p></blockquote><p>因为为了防止维数过高时<spanclass="math inline">\(QK^T\)</span>的值过大导致softmax函数反向传播时发生梯度消失。</p><blockquote><p>为什么是<span class="math inline">\(\sqrt{d_k}\)</span>而不是<spanclass="math inline">\(d_k\)</span>呢？</p></blockquote><p>这就是个经验值，从理论上来说，就是还需要让<spanclass="math inline">\(QK^T\)</span>的值适度增加，但不能过度增加，如果是<spanclass="math inline">\(d_k\)</span>的话，可能就不增加了。</p><p><strong>单词级别-第四步</strong></p><p>然后通过 softmax 操作传递结果。softmax对分数进行归一化处理，使它们都为正且加起来为 1。</p><p>softmax分数决定了在这个位置上，输入句子的每个单词会被投入的注意力占比。</p><imgsrc="https://jalammar.github.io/images/t/self-attention_softmax.png" /><br /><center><code>图20: 缩小维度并进行归一化处理。</code></center><p><strong>单词级别-第五步</strong></p><p>将每个Value向量乘以 softmax分数（准备将它们相加）。这里的直觉是保持我们想要关注的单词的值不变，并淹没不相关的单词（例如，通过将它们乘以像0.001这样的小数字）。然后是对加权值向量求和。这会在该位置产生自注意层的输出。</p><imgsrc="https://jalammar.github.io/images/t/self-attention-output.png" /><br /><center><code>图21: 输出第一个单词的self attention计算结果</code></center><p><br/><br/>自注意力计算到此结束。生成的向量是我们可以发送到前馈神经网络的向量。然而，在实际实现中，此计算以矩阵形式完成。</p><blockquote><p>为什么实际要用矩阵而不是神经网络呢？</p></blockquote><p>因为矩阵运算能用GPU加速，会更快，同时参数量更少，更节省空间。</p><p>既然我们已经看到了单词级别的计算过程，那么让我们来看看SelfAttention实际使用的矩阵计算：</p><p><strong>矩阵计算-第一步</strong></p><p>计算Query,Key和Value共计三个矩阵。为此，我们将嵌入打包到矩阵X中，然后将其乘以我们训练过的权重矩阵<span class="math inline">\((W^Q、W^K、W^V)\)</span>。</p><imgsrc="https://jalammar.github.io/images/t/self-attention-matrix-calculation.png" /><center><code>图22: 计算QKV矩阵</code></center><p><strong>矩阵计算-第二步</strong></p><p>由于我们处理的是矩阵，我们可以将单词形式的第二步到第五步压缩为一个公式来计算自注意力层的输出。</p><imgsrc="https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png" /><br /><center><code>图23: 矩阵形式的self attention计算公式</code></center><h4 id="multi-head-attention原理">Multi-head Attention原理</h4><p>该论文通过添加一种称为Multi-headAttention的机制，进一步细化了自注意力层。主要体现在两个方面：</p><ol type="1"><li><p>它扩展了模型关注不同位置的能力。比如要翻译像“The animal didn'tcross the street because it was tootired”这样的句子，知道“it”指的是哪个词会很有用。</p></li><li><p>它为注意力层提供了多个representation subspaces(表示子空间)。正如我们接下来将看到的，对于Multi-headAttention，我们有多组QKV权重矩阵，其中的每一个都是随机初始化的。训练之后，每个集合用于将输入（初始输入或来自较低编码器/解码器的向量）投影到不同的表示子空间中。</p></li></ol><imgsrc="https://jalammar.github.io/images/t/transformer_attention_heads_qkv.png" /><br /><center><code>图24: Multi-head Attention每个头各产生不同的 QKV 矩阵</code></center><p><br/><br/></p><imgsrc="https://jalammar.github.io/images/t/transformer_attention_heads_z.png" /><br /><center><code>图25: 通过相同的计算过程，8个attention head最终会得到8个不同的Z矩阵</code></center><p><br/><br/></p><p>这给我们带来了一些挑战。前馈层不需要8个矩阵——它需要一个矩阵（每个单词一个向量）。所以我们需要一种方法将这8个压缩成一个矩阵。因此我们连接这些矩阵，然后将它们乘以一个额外的权重矩阵<span class="math inline">\(W^O\)</span>。</p><imgsrc="https://jalammar.github.io/images/t/transformer_attention_heads_weight_matrix_o.png" /><br /><center><code>图26: 将多个Z矩阵通过矩阵乘法合并成总的Z矩阵</code></center><p><br/><br/> <imgsrc="https://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png" /></p><p>现在我们已经谈到了attentionhead，让我们重新审视我们之前的例子，看看当我们在示例句子中对单词“it”进行编码时，不同的attentionhead集中在什么地方：</p><imgsrc="https://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png" /><br /><center><code>图28: 不同attention head对it的注意力权重不同</code></center><p><br/><br/> 当我们对“it”这个词进行编码时，一个attention head最关注“theanimal”，而另一个attention head则关注“tired”。我认为这说明不同attentionhead很可能是从不同角度来理解it和其他单词的关系，比如it的指代的对象是“theanimal”，而这个对象所处的状态是“tired”。因为Attention是注意力的意思而不是表示相等的意思，那么从不同角度看待同一个事物，得到不同的答案自然也是没问题的。</p><h3 id="add-normalize">Add &amp; Normalize</h3><p>可以注意到编码器/解码器的每个子层（比如self attention,ffnn）之后都带有一个 <strong>Add &amp; Normalize</strong>。</p><imgsrc="https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png" /><br /><center><code>图29: 每个编码器中都有2个Add &amp; Normalize子层</code></center><p><br/><br/></p><p>我之前没有听说过残差连接，因此看着这张图好久也没看出residual这个词体现在哪里，问了“Chat老师”才明白，Add表示残差连接，Norm表示LayerNorm，残差连接来源于<ahref="https://arxiv.org/abs/1512.03385">[论文4]Kaiming He. 2015. DeepResidual Learning for Image Recognition</a>，LayerNorm来源于<ahref="https://arxiv.org/abs/1607.06450">[论文5]Jimmy Lei Ba,. 2016.Layer Normalization</a>。</p><p>Encoder端和Decoder端每个子模块实际的输出为：<spanclass="math inline">\(LayerNorm(x+Sublayer(x))\)</span>，其中<spanclass="math inline">\({Sublayer}(x)\)</span>为子模块的输出。这样做有助于模型更好地捕捉长期依赖关系。</p><p>关于这一部分的更多技术细节我以问答的形式展示在下面。</p><p><strong>问题一</strong></p><blockquote><p>什么是残差连接</p></blockquote><p>残差连接（residualconnections）是一种网络设计方法，就是在每一层的输入和输出之间添加一个残差连接，并对结果进行标准化。（其实也就是Add&amp;Normolize）这种方法能够有效地缓解深度网络中的梯度消失问题，帮助模型更好地捕捉长期依赖关系。</p><p>例如在Transformer中，设计一个多头注意力机制的层，输入为x，输出为y，残差连接就是y=norm(x+multi-head-attention(x))</p><p>这样做的好处就是，当网络层数较深时，残差连接能够有效减少梯度消失问题，简化网络训练过程，使网络能够更好地捕捉长期依赖关系。</p><p><strong>问题二</strong></p><blockquote><p>Add操作做了什么</p></blockquote><p>Add，就是在Z的基础上加了一个残差块X。</p><p><imgsrc="https://img-blog.csdnimg.cn/94d2b8dbc1ea432c9ca85f62e29cb454.png#pic_center" />上图就是构造的一个残差块。</p><ul><li>X是这一层残差块的输入值</li><li>F(X)是经过第一层线性变化并激活后的输出，也称为残差</li></ul><p>该图表示在残差网络中，第二层进行线性变化之后、激活之前，F(X)加入了这一层输入值X，然后再进行激活后输出。在第二层输出值激活前加入X，这条路径称作shortcut连接。</p><p>加入残差块X的目的是为了防止在深度神经网络训练中发生退化问题，退化的意思就是深度神经网络通过增加网络的层数，Loss逐渐减小，然后趋于稳定达到饱和，然后再继续增加网络层数，Loss反而增大。</p><p><strong>问题三</strong></p><blockquote><p>为什么深度神经网络会发生退化？</p></blockquote><p>举个例子：假如某个神经网络的最优网络层数是18层，但是我们在设计的时候并不知道到底多少层是最优解，本着层数越深越好的理念，我们设计了32层，那么32层神经网络中有14层其实是多余地，我们要想达到18层神经网络的最优效果，必须保证这多出来的14层网络必须进行恒等映射，恒等映射的意思就是说，输入什么，输出就是什么，可以理解成F(x)=x这样的函数，因为只有进行了这样的恒等映射咱们才能保证这多出来的14层神经网络不会影响我们最优的效果。<br />但现实是神经网络的参数都是训练出来地，要想保证训练出来地参数能够很精确的完成F(x)=x的恒等映射其实是很困难地。多余的层数较少还好，对效果不会有很大影响，但多余的层数一多，可能结果就不是很理想了。这个时候大神们就提出了ResNet残差神经网络来解决神经网络退化的问题。</p><p><strong>问题四</strong></p><blockquote><p>为什么添加了残差块能防止神经网络退化问题呢？</p></blockquote><p>咱们再来看看添加了残差块后，咱们之前说的要完成恒等映射的函数变成什么样子了。是不是就变成h(X)=F(X)+X，我们要让h(X)=X，那么是不是相当于只需要让F(X)=0就可以了，这里就巧妙了！神经网络通过训练变成0是比变成X容易很多地，因为大家都知道咱们一般初始化神经网络的参数的时候就是设置的[0,1]之间的随机数嘛。所以经过网络变换后很容易接近于0。举个例子：</p><p><imgsrc="https://img-blog.csdnimg.cn/20200326001443472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /></p><p>假设该网络只经过线性变换，没有bias也没有激活函数。我们发现因为随机初始化权重一般偏向于0，那么经过该网络的输出值为[0.60.6]，很明显会更接近与[0 0]，而不是[21]，相比与学习h(x)=x，模型要更快到学习F(x)=0。<br />并且ReLU能够将负数激活为0，过滤了负数的线性变化，也能够更快的使得F(x)=0。这样当网络自己决定哪些网络层为冗余层时，使用ResNet的网络很大程度上解决了学习恒等映射的问题，用学习残差F(x)=0更新该冗余层的参数来代替学习h(x)=x更新冗余层的参数。</p><p>这样当网络自行决定了哪些层为冗余层后，通过学习残差F(x)=0来让该层网络恒等映射上一层的输入，使得有了这些冗余层的网络效果与没有这些冗余层的网络效果相同，这样很大程度上解决了网络的退化问题。</p><p>到这里，关于Add中为什么需要加上一个X，要进行残差网络中的shortcut你清楚了吗？Transformer中加上的X也就是Multi-HeadAttention的输入，X矩阵。</p><p><strong>问题五</strong></p><blockquote><p>为什么要进行Normalize呢？<br />在神经网络进行训练之前，都需要对于输入数据进行Normalize归一化，目的有二：1，能够加快训练的速度。2.提高训练的稳定性。</p></blockquote><p><strong>问题六</strong></p><blockquote><p>为什么使用Layer Normalization（LN）而不使用BatchNormalization（BN）呢？</p></blockquote><p><imgsrc="https://img-blog.csdnimg.cn/20200326202939489.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /><br />先看图，LN是在同一个样本中不同神经元之间进行归一化，而BN是在同一个batch中不同样本之间的同一位置的神经元之间进行归一化。<br />BN是对于相同的维度进行归一化，但是咱们NLP中输入的都是词向量，一个300维的词向量，单独去分析它的每一维是没有意义地，在每一维上进行归一化也是适合地，因此这里选用的是LayerNorm。</p>因此，我们可以可视化Add &amp; Normalize操作，如下图所示： <imgsrc="https://jalammar.github.io/images/t/transformer_resideual_layer_norm_2.png" /><br /><center><code>图30: 可视化Add &amp; Normalize操作</code></center><h3 id="feed-forward">Feed Forward</h3><p>每一层经过self attention之后，还会有一个Feed ForwardNetwork(FFN)，这个FFN的作用就是空间变换。FFN包含了2层lineartransformation层，中间的激活函数是ReLu。</p><p><span class="math display">\[FFN(x) = \max(0, xW_1 + b_1 )W_2 +b_2\]</span></p><blockquote><p>attention层的output最后会和 <span class="math inline">\(W_O\)</span>相乘，为什么这里又要增加一个2层的FFN网络？</p></blockquote><p>这是因为FFN的加入引入了非线性(ReLu激活函数)，变换了attentionoutput的空间,从而增加了模型的表现能力。把FFN去掉模型也是可以用的，但是效果差了很多。</p><p>这段代码实现了一个全连接前馈层（feed-forwardlayer）。这个层的输入是一个 <span class="math inline">\(d_model\)</span>维的向量 x，输出也是一个 <span class="math inline">\(d_model\)</span>维的向量。代码中，它包含了两个线性变换和一个dropout，它们分别是 <spanclass="math inline">\(w_1\)</span>, <spanclass="math inline">\(w_2\)</span> 和 dropout。</p><p><span class="math inline">\(w_1\)</span>是一个线性变换，它将输入 x转换成一个 d_ff 维的向量。</p><p><span class="math inline">\(w_2\)</span>是一个线性变换，它将 <spanclass="math inline">\(w_1\)</span> 的输出转换成 d_model 维的向量。</p><p>dropout是为了防止过拟合。</p><p>F.relu是激活函数，它会把w_1的输出中小于0的数转换成0.</p><p>最后，输出是<spanclass="math inline">\(w_2\)</span>的输出和dropout的结果。</p><p>总而言之，这个类实现了一个全连接层，它接受 <spanclass="math inline">\(d_model\)</span>维的输入，经过两个线性变换和一个激活函数，最终输出 <spanclass="math inline">\(d_model\)</span> 维的向量。</p><h3 id="skip-connection图中的虚线输入">SkipConnection——图中的虚线输入</h3><p>skipconnection最早是在计算机视觉的ResNet里面提到的，是微软亚洲研究院的何凯明做的，主要是想解决当网络很深时，误差向后传递会越来越弱，训练就很困难，那如果产生跳跃连接，如果有误差，可以从不同路径传到早期的网络层，这样的话误差就会比较明确地传回来。这就是跳跃层的来历。</p><p>跳跃层不是必须的，但在Transformer中，作者建议这样做，在SelfAttention的前后和每一个FeedForwar前后都用了跳跃层，如下图中的虚线所示。</p><imgsrc="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKszwql2x03nnGJe%2Fskip-connection-and-layer-normalization.jpg?generation=1583677013154775&amp;alt=media" /><center><code>图30: 图中的虚线代表Skip Connection</code></center><h2 id="decoder">Decoder</h2><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(3).png" width = "50%" /><center><code>图31: Transformer模型的编码器部分</code></center><p>论文中Decoder也是N=6层堆叠的结构。被分为3个sub-layer，Encoder与Decoder有<strong>三大主要的不同</strong>：</p><ol type="1"><li><p>Decoder sub-layer-1使用的是“<strong>Masked</strong>” Multi-HeadedAttention机制，<strong>防止为了模型看到要预测的数据，防止泄露</strong>。</p></li><li><p>sub-layer-2是一个Encoder-Decoder Multi-head Attention。</p></li><li><p>LinearLayer和SoftmaxLayer作用于sub-layer-3的输出后面，来预测对应的word的概率。</p></li></ol><p>如果你弄懂了Encoder部分，Decoder部分也就没有那么可怕了：</p><ul><li>输入都是 embedding + positional Encoding。</li><li>Decoder也是N=6层堆叠的结构。被分为3个sub-layer，具体细节方面：<ol type="1"><li>masked multi-headattention：由于在机器翻译中，Decode的过程是一个顺序的过程，也就是当解码第k个位置时，我们只能看到第k- 1及其之前的解码结果，因此<strong>加了mask，防止模型看到要预测的数据。这点和Encoder不同</strong></li><li>Encoder-Decoder Multi-HeadAttention：和Encoder的类似，每一层Decoder都会接受Encoder最后一层输出作为key和value，而当前解码器输出作为query。然后计算输入序列和目标序列中每个位置之间的相似度，最后将所有头的结果拼接在一起得到最终的输出。</li><li>FeedForward：和Encoder一样</li></ol></li><li>最后都连接了LinearLayer和SoftmaxLayer</li></ul><imgsrc="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKtDCvJ7TGjfZPuP%2Fencoder-decoder-2.jpg?generation=1583677008527428&amp;alt=media" /><center><code>图33: Transformer每一层Decoder都会分别接受Encoder最后一层输出</code></center><p>由此可见，<strong>只有masked multi-headattention需要详细讲解，其余的在encoder处都已经掌握了。</strong></p><h3 id="masked-multi-head-attention">Masked Multi-Head-Attention</h3><p>MaskedMulti-Head-Attention则是在传统的多头注意力层的基础上，在计算过程中添加了一个遮挡（mask）机制。这个遮挡机制可以避免解码器在生成目标序列时看到未来的信息。</p><p>具体来说，在计算解码器在当前位置的输出值时，如果该位置对应的输入位置在目标序列中出现的位置在当前位置之后，那么这个输入位置就会被遮挡，不会被用来计算输出值。这样做能够避免解码器在生成目标序列时看到未来的信息，提高模型的效果。</p><p><strong>问题一</strong></p><blockquote><p>什么是mask</p></blockquote><p>mask表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer模型里面涉及两种mask，分别是padding mask和sequence mask。 其中，padding mask在所有的scaleddot-product attention 里面都需要用到，而sequencemask只有在Decoder的Self-Attention里面用到。</p><p><strong>问题二</strong></p><blockquote><p>什么是padding mask？</p></blockquote><p>因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的Attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。</p><p>具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过softmax，这些位置的概率就会接近0！而我们的padding mask实际上是一个张量，每个值都是一个Boolean，值为false的地方就是我们要进行处理的地方。</p><p><strong>问题三</strong></p><blockquote><p>什么是sequence mask 文章前面也提到，sequencemask是为了使得Decoder不能看见未来的信息。也就是对于一个序列，在time_step为t的时刻，我们的解码输出应该只能依赖于t时刻之前的输出，而不能依赖t之后的输出。因此我们需要想一个办法，把t之后的信息给隐藏起来。那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。</p></blockquote><p>sequence mask的目的是防止Decoder “seeing thefuture”，就像防止考生偷看考试答案一样。这里mask是一个下三角矩阵，对角线以及对角线左下都是1，其余都是0。下面是个10维度的下三角矩阵：$ [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],</p><p>[1, 1, 0, 0, 0, 0, 0, 0, 0, 0],</p><p>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0],</p><p>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],</p><p>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0],</p><p>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],</p><p>[1, 1, 1, 1, 1, 1, 1, 0, 0, 0],</p><p>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],</p><p>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],</p><p>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]] $</p><p>对于Decoder的Self-Attention，里面使用到的scaled dot-productattention，同时需要padding mask和sequencemask作为attn_mask，具体实现就是两个mask相加作为attn_mask。</p><p>其他情况，attn_mask一律等于padding mask。</p><p>举个例子：</p><p>假设最大允许的序列长度为10，先令padding mask为</p><p>[0 0 0 0 0 0 0 0 0 0]</p><p>然后假设当前句子一共有5个单词（加一个起始标识），在输入第三个单词的时候，前面有一个开始标识和两个单词，则此刻的sequencemask为</p><p>[1 1 1 0 0 0]</p><p>然后padding mask和sequence mask相加，得</p><p>[1 1 1 0 0 0 0 0 0 0]</p><p><strong>问题四</strong></p><blockquote><p>为什么在模型训练阶段，Decoder的初始输入需要整体右移（ShiftedRight）一位？</p></blockquote><p>因为<span class="math inline">\(T-1\)</span>时刻需要预测<spanclass="math inline">\(T\)</span>时刻的输出，所以Decoder的输入需要整体后移一位</p><p>举例说明：<code>汤姆追逐杰瑞</code> →<code>Tom chase Jerry</code></p><p>位置关系： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0-“Tom”</span><br><span class="line">1-“chase”</span><br><span class="line">2-“Jerry”</span><br></pre></td></tr></table></figure> 操作：整体右移一位（Shifted Right）<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">0-&lt;/s&gt;【起始符】目的是为了预测下一个Token</span><br><span class="line">1-“Tom”</span><br><span class="line">2-“chase”</span><br><span class="line">3-“Jerry”</span><br></pre></td></tr></table></figure></p><imgsrc="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKtPv2OdAZhNaaw3%2Ftransformer-process-2.gif?generation=1583677021471399&amp;alt=media" /><center><code>图31: 另一个例子说明Transformer模型的解码工作过程(省略了&lt;/s&gt;)</code></center><h2 id="output">Output</h2><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/transformer_4parts(4).png" /><center><code>图34: Decoder之后的输出部分</code></center><p>Decoder最终输出的结果是一个浮点型数据的向量，我们要如何把这个向量转为一个单词呢？这个就是Linear和softmax要做的事情了。</p><p>Linear层是一个全连接的神经网络，输出神经元个数一般等于我们的词汇表大小。Decoder输出的结果会输入到Linear层，然后再用softmax进行转换，得到的是词汇表大小的向量，向量的每个值对应的是当前Decoder是对应的这个词的概率，我们只要取概率最大的词，就是当前词语Decoder的结果了。</p><p>也就是说，Decoder的输出值首先经过一次线性变换，然后Softmax得到输出的概率分布，然后通过词典，输出概率最大的对应的单词作为我们的预测输出。</p><h1 id="transformer训练tricks">Transformer训练Tricks</h1><p>这里有两个训练小技巧，第一个是label平滑，第二个就是学习率要有个wormup过程，然后再下降。</p><p>1、Label Smoothing（regularization）</p><p>由传统的 <span class="math display">\[\begin{equation}P_i=\begin{cases}1&amp; \text{ $ i = y $ } \\0&amp; \text{ $ i \neq y $ }\end{cases}\end{equation}\]</span></p><p>变为</p><p><span class="math display">\[\begin{equation}P_i=\begin{cases}1−ϵ&amp; \text{ $ i = y $ } \\\frac{ϵ}{K−1}&amp; \text{ $ i \neq y $ }\end{cases}\end{equation}\]</span></p><p>注：<span class="math inline">\(K\)</span>表示多分类的类别总数，<spanclass="math inline">\(\epsilon\)</span>是一个较小的超参数。</p><p>2、<a href="https://arxiv.org/pdf/1812.10464.pdf">[论文6] MikelArtetxe &amp; Holger Schwenk. 2018. Massively Multilingual SentenceEmbeddings for Zero-Shot Cross-Lingual Transfer and Beyond</a></p><p>Noam learning rateschedule是一种在训练深度学习模型时调整学习率的方法。这种方法是由GoogleAI团队的NoamShazeer在2018年提出的。它的基本思想是，随着模型的训练进程，学习率应该逐渐降低。具体来说，学习率是根据训练步数的对数来调整的。这个方法可以帮助模型在训练初期快速收敛，并在训练后期更稳定地优化。</p><p>学习率不按照Noam Learning RateSchedule，可能就得不到一个好的Transformer。</p><p><spanclass="math display">\[lr=d_{model}^{−0.5}⋅min(step_{num}^{−0.5}, step_{num}\cdotwarmup\_steps^{−1.5})\]</span></p><p>公式表示学习率随着训练步数的增加而逐渐降低，在训练的前<spanclass="math inline">\(warmup_steps\)</span>步中学习率是线性增长的,之后学习率是指数下降的。如图所示：</p><imgsrc="https://4143056590-files.gitbook.io/~/files/v0/b/gitbook-legacy-files/o/assets%2F-LpO5sn2FY1C9esHFJmo%2F-M1uVIrSPBnanwyeV0ps%2F-M1uVKtJ7Yme6Ll5rgo9%2Flr-worm-up.jpg?generation=1583677009478369&amp;alt=media" /><center><code>图32: Noam learning rate schedule学习率随着训练步数的增加先上升后下降</code></center><h1 id="transformer特点">Transformer特点</h1><h2 id="优点">优点</h2><ol type="1"><li><p>每层计算<strong>复杂度比RNN要低</strong>。</p></li><li><p>可以进行<strong>并行计算</strong>。</p></li><li><p>从计算一个序列长度为n的信息要经过的路径长度来看,CNN需要增加卷积层数来扩大视野，RNN需要从1到n逐个进行计算，而Self-attention只需要一步矩阵计算就可以。Self-Attention可以比RNN<strong>更好地解决长时依赖问题</strong>。当然如果计算量太大，比如序列长度N大于序列维度D这种情况，也可以用窗口限制Self-Attention的计算数量。</p></li><li><p>从作者在附录中给出的例子可以看出，Self-Attention<strong>模型更可解释，Attention结果的分布表明了该模型学习到了一些语法和语义信息</strong>。</p></li></ol><h2 id="缺点">缺点</h2><p>在原文中没有提到缺点，是后来在UniversalTransformers中指出的，主要是两点：</p><ol type="1"><li><p>有些RNN轻易可以解决的问题Transformer没做到，比如复制String，或者推理时碰到的sequence长度比训练时更长（因为碰到了没见过的positionembedding）</p></li><li><p>理论上：transformers不是computationallyuniversal(图灵完备)，而RNN图灵。完备，这种非RNN式的模型是非图灵完备的的，<strong>无法单独完成NLP中推理、决策等计算问题</strong>（包括使用transformer的bert模型等等）。</p></li></ol><h1 id="transformer面试题及答案">Transformer面试题及答案</h1><h1 id="参考链接">参考链接</h1><h2 id="整体代码实现">整体代码实现</h2><ul><li><p>哈佛大学NLP组的notebook，很详细文字和代码描述，用pytorch实现</p><p><ahref="https://nlp.seas.harvard.edu/2018/04/03/attention.html">https://nlp.seas.harvard.edu/2018/04/03/attention.html</a></p></li><li><p>Google的TensorFlow官方的，用tf keras实现</p><p><ahref="https://www.tensorflow.org/tutorials/text/transformer">https://www.tensorflow.org/tutorials/text/transformer</a></p></li></ul><h2 id="参考网站">参考网站</h2><ol type="1"><li><p>Self-Attention和Transformer</p><p><ahref="https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#%E8%AF%8D%E5%90%91%E9%87%8FEmbedding%E8%BE%93%E5%85%A5">https://luweikxy.gitbook.io/machine-learning-notes/self-attention-and-transformer#%E8%AF%8D%E5%90%91%E9%87%8FEmbedding%E8%BE%93%E5%85%A5</a></p></li><li><p>史上最小白之Transformer详解:</p><p><ahref="https://blog.csdn.net/Tink1995/article/details/105080033">https://blog.csdn.net/Tink1995/article/details/105080033</a></p></li><li><p>史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！:</p><p><ahref="https://zhuanlan.zhihu.com/p/148656446">https://zhuanlan.zhihu.com/p/148656446</a></p></li><li><p>The Illustrated Transformer:</p><p><ahref="https://jalammar.github.io/illustrated-transformer/">https://jalammar.github.io/illustrated-transformer/</a></p></li><li><p>A Brief Overview of Recurrent Neural Networks (RNN):</p><p><ahref="https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/">https://www.analyticsvidhya.com/blog/2022/03/a-brief-overview-of-recurrent-neural-networks-rnn/</a></p></li><li><p>Practical PyTorch: Translation with a Sequence to SequenceNetwork and Attention:</p><p><ahref="https://notebook.community/spro/practical-pytorch/seq2seq-translation/seq2seq-translation-batched">https://notebook.community/spro/practical-pytorch/seq2seq-translation/seq2seq-translation-batched</a></p></li><li><p>也来谈谈RNN的梯度消失/爆炸问题:</p><p><ahref="https://kexue.fm/archives/7888">https://kexue.fm/archives/7888</a></p></li><li><p>Transformer 架构逐层功能介绍和详细解释:</p><p><ahref="https://avoid.overfit.cn/post/a895d880dab245609c177db7598446d4">https://avoid.overfit.cn/post/a895d880dab245609c177db7598446d4</a></p></li><li><p>Pytorch中 nn.Transformer的使用详解与Transformer的黑盒讲解:</p><p><ahref="https://blog.51cto.com/u_11466419/5530949">https://blog.51cto.com/u_11466419/5530949</a></p></li><li><p>【深度学习】Attention is All You Need : Transformer模型:</p><p><ahref="https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/">https://www.hrwhisper.me/deep-learning-attention-is-all-you-need-transformer/</a></p></li><li><p>Bert前篇：手把手带你详解Transformer原理:</p><p><ahref="https://zhuanlan.zhihu.com/p/364659780">https://zhuanlan.zhihu.com/p/364659780</a></p></li><li><p>ChatGPT3：</p><p><ahref="https://chat.openai.com/chat">https://chat.openai.com/chat</a></p></li><li><p>一文搞懂one-hot和embedding：</p><p><ahref="https://blog.csdn.net/Alex_81D/article/details/114287498">https://blog.csdn.net/Alex_81D/article/details/114287498</a></p></li><li><p>残差网络(Residual Network)：</p><p><ahref="https://www.cnblogs.com/gczr/p/10127723.html">https://www.cnblogs.com/gczr/p/10127723.html</a></p></li><li><p>【经典精读】万字长文解读Transformer模型和Attention机制</p><p><ahref="https://zhuanlan.zhihu.com/p/104393915?utm_source=ZHShareTargetIDMore">https://zhuanlan.zhihu.com/p/104393915?utm_source=ZHShareTargetIDMore</a></p></li></ol><h2 id="参考文献">参考文献</h2><ul><li><p><ahref="https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext">[论文1]Hochreiter&amp; Schmidhuber. 1997. Long Short-Term Memory</a></p></li><li><p><a href="https://arxiv.org/abs/1409.0473">[论文2]DzmitryBahdanau. 2014. Neural Machine Translation by Jointly Learning to Alignand Translate</a></p></li><li><p><a href="https://arxiv.org/abs/1706.03762">[论文3]AshishVaswani., 2017 . Attention Is All You Need</a></p></li><li><p><a href="https://arxiv.org/abs/1512.03385">[论文4]Kaiming He.2015. Deep Residual Learning for Image Recognition</a></p></li><li><p><a href="https://arxiv.org/abs/1607.06450">[论文5]Jimmy Lei Ba,.2016. Layer Normalization</a></p></li><li><p><a href="https://arxiv.org/pdf/1812.10464.pdf">[论文6] MikelArtetxe &amp; Holger Schwenk. 2018. Massively Multilingual SentenceEmbeddings for Zero-Shot Cross-Lingual Transfer and Beyond</a></p></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;p&gt;Transformer解析博客分为上下两篇，您现在阅读的是下篇——关于Transformer的详解，在阅读本篇博客之前了解过Encoder-Decoder和Attention机制可能会更有帮助，因此可以参考我的上篇博客，里面我也做了</summary>
      
    
    
    
    <category term="笔记" scheme="https://serika-onoe.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="科研" scheme="https://serika-onoe.github.io/tags/%E7%A7%91%E7%A0%94/"/>
    
    <category term="项目" scheme="https://serika-onoe.github.io/tags/%E9%A1%B9%E7%9B%AE/"/>
    
    <category term="transformer" scheme="https://serika-onoe.github.io/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>FEDformer论文阅读</title>
    <link href="https://serika-onoe.github.io/2022/12/31/FEDformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    <id>https://serika-onoe.github.io/2022/12/31/FEDformer%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/</id>
    <published>2022-12-31T03:47:23.000Z</published>
    <updated>2022-12-31T03:47:23.791Z</updated>
    
    <content type="html"><![CDATA[]]></content>
    
    
      
      
    <summary type="html">
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>CS231n Assignment 1 (Updating)</title>
    <link href="https://serika-onoe.github.io/2022/12/20/CS231n-Assignment-1/"/>
    <id>https://serika-onoe.github.io/2022/12/20/CS231n-Assignment-1/</id>
    <published>2022-12-20T14:49:13.000Z</published>
    <updated>2022-12-30T14:02:48.011Z</updated>
    
    <content type="html"><![CDATA[<h1 id="cs231n-assignment-1">CS231n Assignment 1</h1><p>作业1分为五个部分：KNN、SVM、Softmax classifier、2层神经网络、HigherLevel Representations: Image Features.</p><p>建议作业完成顺序：</p><ol type="1"><li>k近邻分类：knn.ipynb &amp; k_nearest_neighbor.py</li><li>svm线性分类：svm.ipynb &amp; linear_svm.py &amp;linear_classifier.py</li><li>softmax线性分类：softmax.ipynb &amp; softmax.py</li><li>两层神经网络：two_layer_net.ipynb &amp; neural_net.py</li></ol><h1 id="k-nearest-neighbor-knn">k-Nearest Neighbor (kNN)</h1><p>在knn.ipynb中，调用了k_nearest_neighbor.py文件。</p><p>k近邻分类算法步骤如下介绍：</p><ol type="1"><li>记住所有训练图像</li><li>计算测试图像与所有训练图像的距离（常用L2距离）</li><li>选择与测试图像距离最小的k张训练图像</li><li>计算这k张图像所对应的类别出现的次数，选择出现次数最多的类别记为预测类别</li></ol><h2 id="k_nearest_neighbor.py">k_nearest_neighbor.py</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">KNearestNeighbor</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; a kNN classifier with L2 distance &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#定义一个k近邻分类器的类</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="训练">训练</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, X, y</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Train the classifier. For k-nearest neighbors this is just</span></span><br><span class="line"><span class="string">    memorizing the training data.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_train, D) containing the training data</span></span><br><span class="line"><span class="string">      consisting of num_train samples each of dimension D.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing the training labels, where</span></span><br><span class="line"><span class="string">         y[i] is the label for X[i].</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment">#self.X_train 是训练数据，维度是 (N,D)，训练集有N个样本，每个样本特征是D维</span></span><br><span class="line">    <span class="comment">#self.y_train 是标签，维度是（N,）,即N个训练样本对应的标签</span></span><br><span class="line">    self.X_train = X</span><br><span class="line">    self.y_train = y</span><br></pre></td></tr></table></figure><h3id="预测计算测试图像和所有训练图像的l2距离">预测：计算测试图像和所有训练图像的L2距离</h3><p>预测时首先需要计算测试样本与所有训练样本的距离,然后根据距离判断样本的类别。</p><p>计算距离需要我们实现三种方法，分别为需要双重循环，单循环，不需要循环。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X, k=<span class="number">1</span>, num_loops=<span class="number">0</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Predict labels for test data using this classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_test, D) containing test data consisting</span></span><br><span class="line"><span class="string">         of num_test samples each of dimension D.</span></span><br><span class="line"><span class="string">    - k: The number of nearest neighbors that vote for the predicted labels.</span></span><br><span class="line"><span class="string">    - num_loops: Determines which implementation to use to compute distances</span></span><br><span class="line"><span class="string">      between training points and testing points.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">      test data, where y[i] is the predicted label for the test point X[i].</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> num_loops == <span class="number">0</span>:</span><br><span class="line">        dists = self.compute_distances_no_loops(X)</span><br><span class="line">    <span class="keyword">elif</span> num_loops == <span class="number">1</span>:</span><br><span class="line">        dists = self.compute_distances_one_loop(X)</span><br><span class="line">    <span class="keyword">elif</span> num_loops == <span class="number">2</span>:</span><br><span class="line">        dists = self.compute_distances_two_loops(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">&quot;Invalid value %d for num_loops&quot;</span> % num_loops)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.predict_labels(dists, k=k)</span><br></pre></td></tr></table></figure><hr /><h4 id="双重循环实现">双重循环实现</h4><p>第i个测试样本与第j个训练样本的距离<spanclass="math inline">\(dist[i,j]\)</span>等于用第i个测试图像的特征向量减去第j个训练图像的特征向量的值</p><p><spanclass="math inline">\(dist[i,j]=\sqrt{\sum_{d}(x_{test[i,d]}-x_{train[j,d]})^2}\)</span></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/dij.png" /></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_distances_two_loops</span>(<span class="params">self, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using a nested loop over both the training data and the</span></span><br><span class="line"><span class="string">    test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_test, D) containing test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">      is the Euclidean distance between the ith test point and the jth training</span></span><br><span class="line"><span class="string">      point.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_test):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_train):</span><br><span class="line">            <span class="comment">#####################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                             #</span></span><br><span class="line">            <span class="comment"># Compute the l2 distance between the ith test point and the jth    #</span></span><br><span class="line">            <span class="comment"># training point, and store the result in dists[i, j]. You should   #</span></span><br><span class="line">            <span class="comment"># not use a loop over dimension, nor use np.linalg.norm().          #</span></span><br><span class="line">            <span class="comment">#####################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            dists[i][j]=np.sqrt(np.<span class="built_in">sum</span>(np.square(X[i,:]-self.X_train[j,:])))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure><hr /><h4 id="单循环实现">单循环实现</h4><p>利用numpy的broadcast机制，可以直接计算第i张测试图像与所有训练样本的距离</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/dij_single.png" /></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_distances_one_loop</span>(<span class="params">self, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using a single loop over the test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_test):</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                               #</span></span><br><span class="line">        <span class="comment"># Compute the l2 distance between the ith test point and all training #</span></span><br><span class="line">        <span class="comment"># points, and store the result in dists[i, :].                        #</span></span><br><span class="line">        <span class="comment"># Do not use np.linalg.norm().                                        #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        dists[i,:]=np.sqrt(np.<span class="built_in">sum</span>(np.square(X[i,:]-self.X_train),axis=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure><hr /><h4 id="无循环实现">无循环实现</h4><p>两个矩阵不能直接相减，不用循环计算距离，考虑距离公式，同时需保证最后得到的dists.shape满足(num_test,num_train)=(500,5000)</p><p><span class="math inline">\((a-b)^2=a^2+b^2-2ab\)</span></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/dij_no.png" /></p><p>注意点：</p><ol type="1"><li>np.square，返回原列表每个元素的平方值，shape不变。</li><li>np.sum(,axis=1)按列相加（横向），我之前在这个地方没想通。以np.sum(np.square(self.X_train),axis = 1)为例：</li></ol><ul><li>np.shape(self.X_train)=(5000,3072)</li><li>np.shape(np.square(self.X_train))=(5000,3072)</li><li>np.shape(np.sum(np.square(self.X_train)))=(5000,)这表示的是一个含有5000个元素的一维数组，并不是(5000,1)具有5000行的二维list。因此可以被boardcasting,(1,5000)-&gt;(500,5000)</li></ul><ol start="3" type="1"><li>transpose针对二维及以上list有效。以np.transpose([np.sum(np.square(X),axis = 1)]))为例，结合第2点可知:</li></ol><ul><li>得到的np.sum(np.square(X), axis =1)]))是一个一维数组，shape为(500,)</li><li>因此加上[]变成(1,500)</li><li>之后转置得到(500,1),因此可以被boardcasting,(500,1)-&gt;(500,5000)</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_distances_no_loops</span>(<span class="params">self, X</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using no explicit loops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">    <span class="comment"># Compute the l2 distance between all test points and all training      #</span></span><br><span class="line">    <span class="comment"># points without using any explicit loops, and store the result in      #</span></span><br><span class="line">    <span class="comment"># dists.                                                                #</span></span><br><span class="line">    <span class="comment">#                                                                       #</span></span><br><span class="line">    <span class="comment"># You should implement this function using only basic array operations; #</span></span><br><span class="line">    <span class="comment"># in particular you should not use functions from scipy,                #</span></span><br><span class="line">    <span class="comment"># nor use np.linalg.norm().                                             #</span></span><br><span class="line">    <span class="comment">#                                                                       #</span></span><br><span class="line">    <span class="comment"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span></span><br><span class="line">    <span class="comment">#       and two broadcast sums.                                         #</span></span><br><span class="line">    <span class="comment">#########################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    dists = np.sqrt(-<span class="number">2</span>*np.dot(X, self.X_train.T) + np.<span class="built_in">sum</span>(np.square(self.X_train), axis = <span class="number">1</span>) + np.transpose([np.<span class="built_in">sum</span>(np.square(X), axis = <span class="number">1</span>)]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure><h3id="预测根据k个最邻近距离中的多数确定标签">预测：根据K个最邻近距离中的多数确定标签</h3><ul><li>np.argsort() 返回一个数组排好序后各元素对应的原来的位置序号</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#One dimensional array:</span></span><br><span class="line"></span><br><span class="line">x = np.array([<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">np.argsort(x)</span><br><span class="line">array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#Two-dimensional array:</span></span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">0</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">x</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">2</span>]])</span><br><span class="line">ind = np.argsort(x, axis=<span class="number">0</span>)  <span class="comment"># sorts along first axis (down)</span></span><br><span class="line">ind</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">1</span>, <span class="number">0</span>]])</span><br><span class="line">np.take_along_axis(x, ind, axis=<span class="number">0</span>)  <span class="comment"># same as np.sort(x, axis=0)</span></span><br><span class="line">array([[<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure><ul><li>np.bincount() 计算非负整数数组中每个值的出现次数。</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#[0,1,2,3,4]</span></span><br><span class="line">np.bincount(np.arange(<span class="number">5</span>))</span><br><span class="line">array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">np.bincount(np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">7</span>]))</span><br><span class="line">array([<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><hr /><p>需要实现两个功能</p><ol type="1"><li><p>选择与测试图像最相似（距离最小）的k张训练图像np.argsort(dists[i])函数是将dist中的i行元素从小到大排列，并得到对应的index。然后再取前k个索引（也就是得到距离最近的k张图像的索引）</p></li><li><p>计算这k张图像所对应的类别出现的次数，选择出现次数最多的类别</p></li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict_labels</span>(<span class="params">self, dists, k=<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Given a matrix of distances between test points and training points,</span></span><br><span class="line"><span class="string">    predict a label for each test point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">      gives the distance betwen the ith test point and the jth training point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">      test data, where y[i] is the predicted label for the test point X[i].</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    num_test = dists.shape[<span class="number">0</span>]</span><br><span class="line">    y_pred = np.zeros(num_test)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_test):</span><br><span class="line">        <span class="comment"># A list of length k storing the labels of the k nearest neighbors to</span></span><br><span class="line">        <span class="comment"># the ith test point.</span></span><br><span class="line">        closest_y = []</span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">        <span class="comment"># Use the distance matrix to find the k nearest neighbors of the ith    #</span></span><br><span class="line">        <span class="comment"># testing point, and use self.y_train to find the labels of these       #</span></span><br><span class="line">        <span class="comment"># neighbors. Store these labels in closest_y.                           #</span></span><br><span class="line">        <span class="comment"># Hint: Look up the function numpy.argsort.                             #</span></span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        closest_y = self.y_train[np.argsort(dists[i])[:k]]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">        <span class="comment"># Now that you have found the labels of the k nearest neighbors, you    #</span></span><br><span class="line">        <span class="comment"># need to find the most common label in the list closest_y of labels.   #</span></span><br><span class="line">        <span class="comment"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span></span><br><span class="line">        <span class="comment"># label.                                                                #</span></span><br><span class="line">        <span class="comment">#########################################################################</span></span><br><span class="line">        <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">        y_pred[i] = np.argmax(np.bincount(closest_y))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="knn.ipynb">knn.ipynb</h2><p>讨论knn中k的取值问题</p><ul><li>np.array_split() 将一个数组拆分为多个子数组，可以大小不等。</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = np.arange(<span class="number">8.0</span>)</span><br><span class="line">np.array_split(x, <span class="number">3</span>)</span><br><span class="line">[array([<span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>]), array([<span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>]), array([<span class="number">6.</span>,  <span class="number">7.</span>])]</span><br><span class="line"></span><br><span class="line">x = np.arange(<span class="number">9</span>)</span><br><span class="line">np.array_split(x, <span class="number">4</span>)</span><br><span class="line">[array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]), array([<span class="number">3</span>, <span class="number">4</span>]), array([<span class="number">5</span>, <span class="number">6</span>]), array([<span class="number">7</span>, <span class="number">8</span>])]</span><br></pre></td></tr></table></figure><ul><li>dictionary.setdefault(keyname, value)返回具有指定键的项目的值。</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">car = &#123;</span><br><span class="line">  <span class="string">&quot;brand&quot;</span>: <span class="string">&quot;Ford&quot;</span>,</span><br><span class="line">  <span class="string">&quot;model&quot;</span>: <span class="string">&quot;Mustang&quot;</span>,</span><br><span class="line">  <span class="string">&quot;year&quot;</span>: <span class="number">1964</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">x = car.setdefault(<span class="string">&quot;color&quot;</span>, <span class="string">&quot;White&quot;</span>)</span><br></pre></td></tr></table></figure><ul><li>hstack() 按列顺序(横向)把数组给堆叠起来</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b=[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line"><span class="built_in">print</span>(np.hstack((a,b)))</span><br><span class="line"><span class="comment">#[1 2 3 4 5 6 ]</span></span><br><span class="line"></span><br><span class="line">a=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">b=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">c=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line">d=[[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]]</span><br><span class="line"><span class="built_in">print</span>(np.hstack((a,b,c,d)))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[1 1 1 1]</span></span><br><span class="line"><span class="string"> [2 2 2 2]</span></span><br><span class="line"><span class="string"> [3 3 3 3]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><hr /><p>knn需要计算这k张图像所对应的类别出现的次数，选择出现次数最多的类别，那么问题来了，k应该取几效果会比较好呢？</p><p>这需要做两个任务：</p><ol type="1"><li>划分训练集</li><li>交叉验证</li></ol><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_folds = <span class="number">5</span></span><br><span class="line">k_choices = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">X_train_folds = []</span><br><span class="line">y_train_folds = []</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Split up the training data into folds. After splitting, X_train_folds and    #</span></span><br><span class="line"><span class="comment"># y_train_folds should each be lists of length num_folds, where                #</span></span><br><span class="line"><span class="comment"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span></span><br><span class="line"><span class="comment"># Hint: Look up the numpy array_split function.                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">X_train_folds = np.array_split(X_train,num_folds)</span><br><span class="line">y_train_folds = np.array_split(y_train,num_folds)</span><br><span class="line"></span><br><span class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A dictionary holding the accuracies for different values of k that we find</span></span><br><span class="line"><span class="comment"># when running cross-validation. After running cross-validation,</span></span><br><span class="line"><span class="comment"># k_to_accuracies[k] should be a list of length num_folds giving the different</span></span><br><span class="line"><span class="comment"># accuracy values that we found when using that value of k.</span></span><br><span class="line">k_to_accuracies = &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Perform k-fold cross validation to find the best value of k. For each        #</span></span><br><span class="line"><span class="comment"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span></span><br><span class="line"><span class="comment"># where in each case you use all but one of the folds as training data and the #</span></span><br><span class="line"><span class="comment"># last fold as a validation set. Store the accuracies for all fold and all     #</span></span><br><span class="line"><span class="comment"># values of k in the k_to_accuracies dictionary.                               #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">    k_to_accuracies.setdefault(k, [])</span><br><span class="line">    <span class="comment">#print(k_to_accuracies)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_folds):</span><br><span class="line">    classifier = KNearestNeighbor()</span><br><span class="line">    X_val_train = np.vstack(X_train_folds[<span class="number">0</span>:i] + X_train_folds[i+<span class="number">1</span>:])</span><br><span class="line">    y_val_train = np.hstack(y_train_folds[<span class="number">0</span>:i] + y_train_folds[i+<span class="number">1</span>:])</span><br><span class="line">    <span class="comment">#print(X_val_train, y_val_train)</span></span><br><span class="line">    classifier.train(X_val_train, y_val_train)</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> k_choices:</span><br><span class="line">        y_val_pred = classifier.predict(X_train_folds[i], k)</span><br><span class="line">        num_correct = np.<span class="built_in">sum</span>(y_val_pred == y_train_folds[i])</span><br><span class="line">        accuracy = <span class="built_in">float</span>(num_correct) / <span class="built_in">len</span>(y_val_pred)</span><br><span class="line">        k_to_accuracies[k] += [accuracy]</span><br><span class="line">        <span class="comment">#print(k,k_to_accuracies[k])</span></span><br><span class="line">    <span class="comment">#print(k_to_accuracies)</span></span><br><span class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the computed accuracies</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">sorted</span>(k_to_accuracies):</span><br><span class="line">    <span class="keyword">for</span> accuracy <span class="keyword">in</span> k_to_accuracies[k]:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;k = %d, accuracy = %f&#x27;</span> % (k, accuracy))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#OUTPUT</span></span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.263000</span></span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.257000</span> </span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.264000</span></span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.278000</span></span><br><span class="line">    k = <span class="number">1</span>, accuracy = <span class="number">0.266000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.239000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.249000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.240000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.266000</span></span><br><span class="line">    k = <span class="number">3</span>, accuracy = <span class="number">0.254000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.248000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.266000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.280000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.292000</span></span><br><span class="line">    k = <span class="number">5</span>, accuracy = <span class="number">0.280000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.262000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.282000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.273000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.290000</span></span><br><span class="line">    k = <span class="number">8</span>, accuracy = <span class="number">0.273000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.265000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.296000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.276000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.284000</span></span><br><span class="line">    k = <span class="number">10</span>, accuracy = <span class="number">0.280000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.260000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.295000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.279000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.283000</span></span><br><span class="line">    k = <span class="number">12</span>, accuracy = <span class="number">0.280000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.252000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.289000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.278000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.282000</span></span><br><span class="line">    k = <span class="number">15</span>, accuracy = <span class="number">0.274000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.270000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.279000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.279000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.282000</span></span><br><span class="line">    k = <span class="number">20</span>, accuracy = <span class="number">0.285000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.271000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.288000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.278000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.269000</span></span><br><span class="line">    k = <span class="number">50</span>, accuracy = <span class="number">0.266000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.256000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.270000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.263000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.256000</span></span><br><span class="line">    k = <span class="number">100</span>, accuracy = <span class="number">0.263000</span></span><br></pre></td></tr></table></figure><p>通过这行代码可以选出最好的k值</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_k = k_choices[accuracies_mean.argmax()]</span><br></pre></td></tr></table></figure><hr /><h3 id="inline-question-1"><strong>Inline Question 1</strong></h3><p>Notice the structured patterns in the distance matrix, where somerows or columns are visibly brighter. (Note that with the default colorscheme black indicates low distances while white indicates highdistances.)</p><ul><li>What in the data is the cause behind the distinctly brightrows?</li><li>What causes the columns?</li></ul><p><span class="math inline">\(\color{blue}{\textit YourAnswer:}\)</span> <em>fill this in.</em></p><ol type="1"><li><p>The test image is far different from all the trainimage.</p></li><li><p>The train image is unsimilar to all the test image.</p></li></ol><hr /><h3 id="inline-question-2"><strong>Inline Question 2</strong></h3><p>We can also use other distance metrics such as L1 distance. For pixelvalues <span class="math inline">\(p_{ij}^{k}\)</span> at location <spanclass="math inline">\((i,j)\)</span> of some image <spanclass="math inline">\(I_k\)</span>,</p><p>the mean <span class="math inline">\(\mu\)</span> across all pixelsover all images is <spanclass="math display">\[\mu=\frac{1}{nhw}\sum_{k=1}^n\sum_{i=1}^{h}\sum_{j=1}^{w}p_{ij}^{(k)}\]</span>And the pixel-wise mean <span class="math inline">\(\mu_{ij}\)</span>across all images is <spanclass="math display">\[\mu_{ij}=\frac{1}{n}\sum_{k=1}^np_{ij}^{(k)}.\]</span>The general standard deviation <spanclass="math inline">\(\sigma\)</span> and pixel-wise standard deviation<span class="math inline">\(\sigma_{ij}\)</span> is definedsimilarly.</p><p>Which of the following preprocessing steps will not change theperformance of a Nearest Neighbor classifier that uses L1 distance?Select all that apply.</p><ol type="1"><li><p>Subtracting the mean <span class="math inline">\(\mu\)</span>(<spanclass="math inline">\(\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu\)</span>.)</p></li><li><p>Subtracting the per pixel mean <spanclass="math inline">\(\mu_{ij}\)</span> (<spanclass="math inline">\(\tilde{p}_{ij}^{(k)}=p_{ij}^{(k)}-\mu_{ij}\)</span>.)</p></li><li><p>Subtracting the mean <span class="math inline">\(\mu\)</span> anddividing by the standard deviation <spanclass="math inline">\(\sigma\)</span>.</p></li><li><p>Subtracting the pixel-wise mean <spanclass="math inline">\(\mu_{ij}\)</span> and dividing by the pixel-wisestandard deviation <spanclass="math inline">\(\sigma_{ij}\)</span>.</p></li><li><p>Rotating the coordinate axes of the data.</p></li></ol><p><span class="math inline">\(\color{blue}{\textit YourAnswer:}\)</span></p><p>1, 3 will not change the L1 Distance.</p><p><span class="math inline">\(\color{blue}{\textit YourExplanation:}\)</span></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/inline1.jpg" /></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/inline2.jpg" /></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/inline3.jpg" /></p><hr /><h3 id="inline-question-3"><strong>Inline Question 3</strong></h3><p>Which of the following statements about <spanclass="math inline">\(k\)</span>-Nearest Neighbor (<spanclass="math inline">\(k\)</span>-NN) are true in a classificationsetting, and for all <span class="math inline">\(k\)</span>? Select allthat apply. 1. The decision boundary of the k-NN classifier is linear.2. The training error of a 1-NN will always be lower than or equal tothat of 5-NN. 3. The test error of a 1-NN will always be lower than thatof a 5-NN. 4. The time needed to classify a test example with the k-NNclassifier grows with the size of the training set. 5. None of theabove.</p><p><span class="math inline">\(\color{blue}{\textit YourAnswer:}\)</span></p><p>2, 4.</p><p><span class="math inline">\(\color{blue}{\textit YourExplanation:}\)</span></p><ol type="1"><li><p>False. It depends on the given categories of data, if you give acategory with a circle boundary to its neighborhood, it isnon-linear.</p></li><li><p>True. In fact the training error of a 1-NN is always 0, and5-NN's lower bound is 0. It is because the nearest neighbor of test datais always going to be itself in 1-NN.</p></li><li><p>False. The value of k is thus data-dependent, that is why we needto perform cross validation to determine the best k for your intendedapplication and dataset.</p></li><li><p>True. At test, KNN needs to make a full pass through the entiredata set and sort points by distance. The time needed thus grows withthe size of the data.</p></li></ol><hr /><h2 id="knn部分参考链接">KNN部分参考链接:</h2><ol type="1"><li><p>cs231n官网: <ahref="https://cs231n.github.io/">https://cs231n.github.io/</a></p></li><li><p>cs231n作业，assignment1-knn详解（注重算法与代码的结合）: <ahref="https://blog.csdn.net/qq_24906797/article/details/89245722">https://blog.csdn.net/qq_24906797/article/details/89245722</a></p></li><li><p>cs231n assignment1 knn: <ahref="https://blog.csdn.net/SpicyCoder/article/details/94992552">https://blog.csdn.net/SpicyCoder/article/details/94992552</a></p></li><li><p>【本课程配套的代码作业讲解见置顶评论】斯坦福CS231N计算机视觉作业讲解：<ahref="https://www.bilibili.com/video/BV1t4411U78z/?spm_id_from=333.337.search-card.all.click&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a">https://www.bilibili.com/video/BV1t4411U78z/?spm_id_from=333.337.search-card.all.click&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a</a></p></li><li><p>CS231N作业详解零基础版： <ahref="https://www.bilibili.com/video/BV19z411b7u9/?p=6&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a">https://www.bilibili.com/video/BV19z411b7u9/?p=6&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a</a></p></li></ol><h1 id="support-vector-machine-svm">Support Vector Machine (SVM)</h1><p>在svm.ipynb中，调用了linear_svm.py和linear_classifier.py两个文件。</p><p>为方便理解，先介绍SVM的引入基于的几个概念：</p><ol type="1"><li><p>我们要实现一种更强大的方法来解决图像分类问题，该方法可以自然地延伸到神经网络和卷积神经网络上。这种方法主要有两部分组成：</p><ul><li><p>一个是评分函数（scorefunction），它是原始图像数据到类别分值的映射。</p></li><li><p>另一个是损失函数（lossfunction），它是用来量化预测分类标签的得分与真实标签之间一致性的。</p></li></ul><p>该方法可转化为一个最优化问题，在最优化过程中，将通过更新评分函数的参数来最小化损失函数值。从图像像素值到所属类别的评分函数（scorefunction）</p></li><li><p>该我们现在定义评分函数为<span class="math inline">\(f:R^D \toR^K\)</span>，该函数是原始图像像素到分类分值的映射。在<strong>线性分类器</strong>中，一个线性映射：<spanclass="math inline">\(f(x_i,W,b)=Wx_i+b\)</span>。在函数中，数据<spanclass="math inline">\((x_i,y_i)\)</span>是给定的，不能修改。但是我们可以调整权重矩阵<spanclass="math inline">\(W\)</span>这个参数，使得评分函数的结果与训练数据集中图像的真实类别一致，即评分函数在正确的分类的位置应当得到最高的评分（score）。</p></li><li><p>我们将使用损失函数（****Loss Function）（有时也叫代价函数****CostFunction或目标函数****Objective）来衡量我们对结果的不满意程度。直观地讲，当评分函数输出结果与真实结果之间差异越大，损失函数输出越大，反之越小。多类支持向量机（SVM）损失函数是其中一种。SVM的损失函数想要SVM在正确分类上的得分始终比不正确分类上的得分高出一个边界值Delta$ $。</p></li></ol><h2 id="svm简介">SVM简介</h2><h3 id="公式说明">公式说明</h3><p>SVM算法由两个部分组成：数据损失（dataloss），即所有样例的的平均损失L_i，以及正则化损失（regularizationloss）。完整公式如下：</p><p><imgsrc="https://camo.githubusercontent.com/19467d143a4397b56c0aaf83d66de6ba9a4615c801e5ed3f2e1272b80748e789/687474703a2f2f7a686968752e636f6d2f6571756174696f6e3f7465783d4c253344253543646973706c61797374796c652b253543756e64657262726163652537422b25354366726163253742312537442537424e25374425354373756d5f692b4c5f692537445f253742646174612b2535432b2b6c6f7373253744253242253543756e64657262726163652537422535436c616d6264612b52253238572532392537445f253742726567756c6172697a6174696f6e2b2535432b6c6f7373253744" /></p><p>将其展开完整公式是：</p><p><span class="math inline">\(L=\frac{1}{N}\sum_i\sum_{j \not=y_i}[\max(0,f(x_i;W)j-f(x_i;W){y_i}+\Delta)]+\lambda \sum_k \sum_lW^2_{k,l}\)</span></p><p>其中参数意义如下： * X(N,D),N是训练集的数据量。 *W(D,C),C代表图片分类的数量。 * y(N,) * i 是迭代第N个训练集数据 * j是第C个图片分类 * <span class="math inline">\(\lambda\)</span>正则化惩罚，添加到了损失函数里面，并用超参数<spanclass="math inline">\(\lambda\)</span>来计算其权重。该超参数无法简单确定，需要通过交叉验证来获取。引入正则化惩罚还带来很多良好的性质，其中最好的性质就是对大数值权重进行惩罚，可以提升其泛化能力，因为这就意味着没有哪个维度能够独自对于整体分值有过大的影响。</p><h3 id="注意点">注意点</h3><p>超参数在绝大多数情况下设为<spanclass="math inline">\(\Delta\)</span>=1.0都是安全的。超参数<spanclass="math inline">\(\Delta\)</span>和<spanclass="math inline">\(\lambda\)</span>看起来是两个不同的超参数，但实际上他们一起控制同一个权衡：即损失函数中的数据损失和正则化损失之间的权衡。</p><h2 id="linear_svm.py">linear_svm.py</h2><p>损失函数公式： <span class="math inline">\(L=\frac{1}{N}\sum_i\sum_{j\not= y_i}[\max(0,f(x_i;W)j-f(x_i;W){y_i}+\Delta)]+\lambda \sum_k \sum_lW^2_{k,l}\)</span></p><p>我们想通过一个方法来得到损失函数L的最小值，这里考虑使用计算W的梯度来不停的对L进行优化，这里想的就是初始化一个W，然后计算W的梯度，接着不停的迭代W，直到收敛或者达到迭代次数。那问题就变成如何求L对于W的梯度了。</p><h3 id="循环求解">循环求解</h3><p>后面的正则项，就是<span class="math inline">\(\lambda \sum_k \sum_lW^2_{k,l}\)</span>，求导即为 <span class="math display">\[\frac{dL}{dw}(正则项)=2*\lambda*W\]</span></p><p>主要是求前面数据损失函数的梯度。那么，我们先把L给拆分一下，这样可以去掉一个求和符号<span class="math display">\[L_i=\sum_{j \not=y_i}\max(0,x_iw_j-x_iw_{y_i}+\Delta)\]</span></p><p>（1）考虑<span class="math inline">\(j \not= y_i\)</span></p><p><span class="math display">\[\begin{aligned}\frac{dL_i}{dw_j}=1(x_iw_j-x_iw_{y_i}+\Delta&gt;0) *    \begin{array}    {|c|}    x_{i1} \\    x_{i2}\\    \vdots&amp;\\    x_{iD}    \end{array}\end{aligned}\]</span> 所以得到 <span class="math display">\[\frac{dL_i}{dw_j}=1(x_iw_j-x_iw_{y_i}+\Delta&gt;0) *{x_i}\]</span></p><p>（2）考虑<span class="math inline">\(j = y_i\)</span>，则满足</p><ul><li><spanclass="math display">\[\frac{d(x_iw_j)}{dw_{y_i}}=0\]</span></li><li><spanclass="math display">\[\frac{d(-x_iw_{y_i})}{dw_{y_i}}=-x_i\]</span></li></ul><p>因此代入以下公式 <span class="math display">\[\begin{aligned}\frac{dL_i}{dw_{y_i}}=-\sum_{j \not= y_i}1(x_iw_j-x_iw_{y_i}+\Delta&gt;0) *    \begin{array}    {|c|}    x_{i1} \\    x_{i2}\\    \vdots&amp;\\    x_{iD}    \end{array}\end{aligned}\]</span></p><p>最终可以得到</p><p><span class="math display">\[\frac{dL_i}{dw_{y_i}}=-\sum_{j \not= y_i}1(x_iw_j-x_iw_{y_i}+\Delta&gt;0) * x_i\]</span></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">svm_loss_naive</span>(<span class="params">W, X, y, reg</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Structured SVM loss function, naive implementation (with loops).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    dW = np.zeros(W.shape)  <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># compute the loss and the gradient</span></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        correct_class_score = scores[y[i]]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            margin = scores[j] - correct_class_score + <span class="number">1</span>  <span class="comment"># note delta = 1</span></span><br><span class="line">            <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</span><br><span class="line">                loss += margin</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Right now the loss is a sum over all training examples, but we want it</span></span><br><span class="line">    <span class="comment"># to be an average instead so we divide by num_train.</span></span><br><span class="line">    loss /= num_train</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add regularization to the loss.</span></span><br><span class="line">    loss += reg * np.<span class="built_in">sum</span>(W * W)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Compute the gradient of the loss function and store it dW.                #</span></span><br><span class="line">    <span class="comment"># Rather that first computing the loss and then computing the derivative,   #</span></span><br><span class="line">    <span class="comment"># it may be simpler to compute the derivative at the same time that the     #</span></span><br><span class="line">    <span class="comment"># loss is being computed. As a result you may need to modify some of the    #</span></span><br><span class="line">    <span class="comment"># code above to compute the gradient.                                       #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_train):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(num_classes):</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            margin = scores[j] - correct_class_score + <span class="number">1</span>  <span class="comment">#note delta = 1 </span></span><br><span class="line">            <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</span><br><span class="line">                dW[:,j]+=X[i,:]</span><br><span class="line">                dW[:,y[i]]+=-X[i,:]</span><br><span class="line">    </span><br><span class="line">    dW /= num_train</span><br><span class="line">    dW += <span class="number">2</span>*reg*dW</span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><h3 id="向量化实现">向量化实现</h3><p>第一个部分，损失函数。公式和前面基本一致，数据损失函数部分</p><p>分类正确即对应 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scores[<span class="built_in">range</span>(N),y]=<span class="number">0</span> </span><br></pre></td></tr></table></figure></p><p>分类错误即对应</p><p><span class="math display">\[L_i=\sum_{j \not=y_i}\max(0,x_iw_j-x_iw_{y_i}+\Delta)\]</span></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">scores = np.dot(X,W) - scores[<span class="built_in">range</span>(N),[y]].T + <span class="number">1</span></span><br><span class="line">np.maximum(<span class="number">0</span>,scores)</span><br></pre></td></tr></table></figure><p>因为X(N,D),N是训练集的数据量。W(D,C),C代表图片分类的数量。所以一开始保证维数一致:<spanclass="math display">\[scores = X * W\]</span></p><ul><li><p>np.dot(X,W) shape: (N,C)</p></li><li><p>scores shape: (N,C)</p><ul><li>scores[range(N),[y]].T shape: (N,C) -&gt; (1,N) -&gt; (N,1)</li><li>表示选取每个图片的正确分类，给它们评分函数的相应位置置0，说明损失为0，而其他位置则需要按照SVM的公式计算损失并且和0比较大小。</li></ul></li></ul><p>后面的正则项损失函数部分，</p><p><span class="math display">\[\frac{dL}{dw}(正则项)=2*\lambda*W\]</span></p><p>二维数组的np.sum, shape是( , ) 也就是一个数值。</p><p>第二个部分，梯度求解。用到了链式法则： <span class="math display">\[\frac{dL}{dw}=\frac{dL}{dS}*\frac{dS}{dw}\]</span></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/SVMdw.png" /></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">svm_loss_vectorized</span>(<span class="params">W, X, y, reg</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Structured SVM loss function, vectorized implementation.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as svm_loss_naive.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros(W.shape)  <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the structured SVM loss, storing the    #</span></span><br><span class="line">    <span class="comment"># result in loss.                                                           #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    N=<span class="built_in">len</span>(y)</span><br><span class="line">    scores = np.dot(X,W)</span><br><span class="line">    scores -= scores[<span class="built_in">range</span>(N),[y]].T</span><br><span class="line">    scores += <span class="number">1</span></span><br><span class="line">    scores[<span class="built_in">range</span>(N),y]=<span class="number">0</span></span><br><span class="line">    margin = np.maximum(<span class="number">0</span>,scores)</span><br><span class="line">    loss = np.<span class="built_in">sum</span>(margin) / N + reg * np.<span class="built_in">sum</span>(np.square(W)) </span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the gradient for the structured SVM     #</span></span><br><span class="line">    <span class="comment"># loss, storing the result in dW.                                           #</span></span><br><span class="line">    <span class="comment">#                                                                           #</span></span><br><span class="line">    <span class="comment"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span></span><br><span class="line">    <span class="comment"># to reuse some of the intermediate values that you used to compute the     #</span></span><br><span class="line">    <span class="comment"># loss.                                                                     #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    ds = np.zeros_like(margin)</span><br><span class="line">    ds[margin&gt;<span class="number">0</span>]=<span class="number">1</span></span><br><span class="line">    ds[<span class="built_in">range</span>(N),y]-=np.<span class="built_in">sum</span>(ds,axis=<span class="number">1</span>)</span><br><span class="line">    ds /= N</span><br><span class="line">    dW = X.T.dot(ds)</span><br><span class="line">    dW += <span class="number">2</span> * reg * W</span><br><span class="line"></span><br><span class="line">    <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure><h2 id="linear_classifier.py">linear_classifier.py</h2><h3 id="训练-1">训练</h3><ul><li>np.random.choice() 从给定的一维数组生成随机样本。</li></ul><p>Examples:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#从大小为 3 的 np.arange(5) 生成均匀随机样本：</span></span><br><span class="line">np.random.choice(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">array([<span class="number">0</span>, <span class="number">3</span>, <span class="number">4</span>]) <span class="comment"># random</span></span><br><span class="line"><span class="comment">#This is equivalent to np.random.randint(0,5,3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#从大小为 3 的 np.arange(5) 生成一个非均匀随机样本：</span></span><br><span class="line">np.random.choice(<span class="number">5</span>, <span class="number">3</span>, p=[<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0</span>])</span><br><span class="line">array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">0</span>]) <span class="comment"># random</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#从大小为 3 的 np.arange(5) 生成一个统一的随机样本，无需替换，说明没有重复取值：</span></span><br><span class="line">np.random.choice(<span class="number">5</span>, <span class="number">3</span>, replace=<span class="literal">False</span>)</span><br><span class="line">array([<span class="number">3</span>,<span class="number">1</span>,<span class="number">0</span>]) <span class="comment"># random</span></span><br><span class="line"><span class="comment">#This is equivalent to np.random.permutation(np.arange(5))[:3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#从大小为 3 的 np.arange(5) 生成非均匀随机样本，无需替换：</span></span><br><span class="line">np.random.choice(<span class="number">5</span>, <span class="number">3</span>, replace=<span class="literal">False</span>, p=[<span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0.3</span>, <span class="number">0.6</span>, <span class="number">0</span>])</span><br><span class="line">array([<span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>]) <span class="comment"># random</span></span><br></pre></td></tr></table></figure><p>实现train函数，作用是从每一个 iteration 中选出 batch_size个训练样本投入到 SVM 中，然后再计算一次 Loss函数进行梯度下降，避免计算太频繁导致时间消耗过大。有两部分需要补全，第一个是随机选择数据，第二个是梯度下降，实现都比较简单</p><p>梯度下降公式： <imgsrc="https://math.jianshu.com/math?formula=%5Ctheta%20%3D%5Ctheta%20-%5Ceta%20%5Ccdot%20%5Ctriangledown%20_%7B%5Ctheta%20%7DJ%5Cleft%20(%20%5Ctheta%20%5Cright%20)" /></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LinearClassifier</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.W = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">        X,</span></span><br><span class="line"><span class="params">        y,</span></span><br><span class="line"><span class="params">        learning_rate=<span class="number">1e-3</span>,</span></span><br><span class="line"><span class="params">        reg=<span class="number">1e-5</span>,</span></span><br><span class="line"><span class="params">        num_iters=<span class="number">100</span>,</span></span><br><span class="line"><span class="params">        batch_size=<span class="number">200</span>,</span></span><br><span class="line"><span class="params">        verbose=<span class="literal">False</span>,</span></span><br><span class="line"><span class="params">    </span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        Train this linear classifier using stochastic gradient descent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (N, D) containing training data; there are N</span></span><br><span class="line"><span class="string">          training samples each of dimension D.</span></span><br><span class="line"><span class="string">        - y: A numpy array of shape (N,) containing training labels; y[i] = c</span></span><br><span class="line"><span class="string">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span></span><br><span class="line"><span class="string">        - learning_rate: (float) learning rate for optimization.</span></span><br><span class="line"><span class="string">        - reg: (float) regularization strength.</span></span><br><span class="line"><span class="string">        - num_iters: (integer) number of steps to take when optimizing</span></span><br><span class="line"><span class="string">        - batch_size: (integer) number of training examples to use at each step.</span></span><br><span class="line"><span class="string">        - verbose: (boolean) If true, print progress during optimization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Outputs:</span></span><br><span class="line"><span class="string">        A list containing the value of the loss function at each training iteration.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        num_train, dim = X.shape</span><br><span class="line">        num_classes = (</span><br><span class="line">            np.<span class="built_in">max</span>(y) + <span class="number">1</span></span><br><span class="line">        )  <span class="comment"># assume y takes values 0...K-1 where K is number of classes</span></span><br><span class="line">        <span class="keyword">if</span> self.W <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># lazily initialize W</span></span><br><span class="line">            self.W = <span class="number">0.001</span> * np.random.randn(dim, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run stochastic gradient descent to optimize W</span></span><br><span class="line">        loss_history = []</span><br><span class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">            X_batch = <span class="literal">None</span></span><br><span class="line">            y_batch = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Sample batch_size elements from the training data and their           #</span></span><br><span class="line">            <span class="comment"># corresponding labels to use in this round of gradient descent.        #</span></span><br><span class="line">            <span class="comment"># Store the data in X_batch and their corresponding labels in           #</span></span><br><span class="line">            <span class="comment"># y_batch; after sampling X_batch should have shape (batch_size, dim)   #</span></span><br><span class="line">            <span class="comment"># and y_batch should have shape (batch_size,)                           #</span></span><br><span class="line">            <span class="comment">#                                                                       #</span></span><br><span class="line">            <span class="comment"># Hint: Use np.random.choice to generate indices. Sampling with         #</span></span><br><span class="line">            <span class="comment"># replacement is faster than sampling without replacement.              #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            bindex = np.random.choice(num_train,batch_size)</span><br><span class="line">            X_batch = X[bindex]</span><br><span class="line">            y_batch = y[bindex]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># evaluate loss and gradient</span></span><br><span class="line">            loss, grad = self.loss(X_batch, y_batch, reg)</span><br><span class="line">            loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># perform parameter update</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Update the weights using the gradient and the learning rate.          #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            self.W = self.W - learning_rate * grad</span><br><span class="line"></span><br><span class="line">            <span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;iteration %d / %d: loss %f&quot;</span> % (it, num_iters, loss))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss_history</span><br></pre></td></tr></table></figure><h3 id="预测">预测</h3><p>调参环节，从不同的 learning rate 与 regularization strengths中选出使验证集正确率最高的组合。对每一种组合都训一遍SVM，然后计算一次正确率。不过在 learning rate较大的两个情况训练时，发生了计算溢出的情况。题面中说这是正常现象，正确率接近39%​就算成功。</p><ul><li>np.mean(y_train == y_train_pred)解释：<ul><li>mean是求平均值的意思</li><li>y_train ==y_train_pred意思就是判断训练的值和预测的值是否相同，相等返回1</li><li>将相等的全部加起来/总训练数，就是训练集的准确率了，mean这里就是统计相等的做除法算出准确率的作用</li><li>所以 np.mean(y_train == y_train_pred)就是算训练集准确率的意思</li></ul></li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Use the validation set to tune hyperparameters (regularization strength and</span></span><br><span class="line"><span class="comment"># learning rate). You should experiment with different ranges for the learning</span></span><br><span class="line"><span class="comment"># rates and regularization strengths; if you are careful you should be able to</span></span><br><span class="line"><span class="comment"># get a classification accuracy of about 0.39 (&gt; 0.385) on the validation set.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Note: you may see runtime/overflow warnings during hyper-parameter search. </span></span><br><span class="line"><span class="comment"># This may be caused by extreme values, and is not a bug.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># results is dictionary mapping tuples of the form</span></span><br><span class="line"><span class="comment"># (learning_rate, regularization_strength) to tuples of the form</span></span><br><span class="line"><span class="comment"># (training_accuracy, validation_accuracy). The accuracy is simply the fraction</span></span><br><span class="line"><span class="comment"># of data points that are correctly classified.</span></span><br><span class="line">results = &#123;&#125;</span><br><span class="line">best_val = -<span class="number">1</span>   <span class="comment"># The highest validation accuracy that we have seen so far.</span></span><br><span class="line">best_svm = <span class="literal">None</span> <span class="comment"># The LinearSVM object that achieved the highest validation rate.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Write code that chooses the best hyperparameters by tuning on the validation #</span></span><br><span class="line"><span class="comment"># set. For each combination of hyperparameters, train a linear SVM on the      #</span></span><br><span class="line"><span class="comment"># training set, compute its accuracy on the training and validation sets, and  #</span></span><br><span class="line"><span class="comment"># store these numbers in the results dictionary. In addition, store the best   #</span></span><br><span class="line"><span class="comment"># validation accuracy in best_val and the LinearSVM object that achieves this  #</span></span><br><span class="line"><span class="comment"># accuracy in best_svm.                                                        #</span></span><br><span class="line"><span class="comment">#                                                                              #</span></span><br><span class="line"><span class="comment"># Hint: You should use a small value for num_iters as you develop your         #</span></span><br><span class="line"><span class="comment"># validation code so that the SVMs don&#x27;t take much time to train; once you are #</span></span><br><span class="line"><span class="comment"># confident that your validation code works, you should rerun the validation   #</span></span><br><span class="line"><span class="comment"># code with a larger value for num_iters.                                      #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Provided as a reference. You may or may not want to change these hyperparameters</span></span><br><span class="line">learning_rates = [<span class="number">1e-7</span>, <span class="number">5e-5</span>]</span><br><span class="line">regularization_strengths = [<span class="number">2.5e4</span>, <span class="number">5e4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line"><span class="keyword">for</span> rs <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">  <span class="keyword">for</span> lr <span class="keyword">in</span> learning_rates:</span><br><span class="line">    svm = LinearSVM()</span><br><span class="line">    loss_hist = svm.train(X_train,y_train,lr,rs,num_iters=<span class="number">1500</span>)</span><br><span class="line">    y_train_pred = svm.predict(X_train)</span><br><span class="line">    train_accuracy = np.mean(y_train == y_train_pred)</span><br><span class="line">    y_val_pred = svm.predict(X_val)</span><br><span class="line">    val_accuracy = np.mean(y_val == y_val_pred)</span><br><span class="line">    <span class="keyword">if</span> val_accuracy &gt; best_val:</span><br><span class="line">      best_val = val_accuracy</span><br><span class="line">      best_svm = svm</span><br><span class="line">    results[(lr,rs)] = train_accuracy,val_accuracy</span><br><span class="line"><span class="comment"># *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Print out results.</span></span><br><span class="line"><span class="keyword">for</span> lr, reg <span class="keyword">in</span> <span class="built_in">sorted</span>(results):</span><br><span class="line">    train_accuracy, val_accuracy = results[(lr, reg)]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;lr %e reg %e train accuracy: %f val accuracy: %f&#x27;</span> % (</span><br><span class="line">                lr, reg, train_accuracy, val_accuracy))</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;best validation accuracy achieved during cross-validation: %f&#x27;</span> % best_val)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#output result</span></span><br><span class="line">    lr <span class="number">1.000000e-07</span> reg <span class="number">2.500000e+04</span> train accuracy: <span class="number">0.371204</span> val accuracy: <span class="number">0.387000</span></span><br><span class="line">    lr <span class="number">1.000000e-07</span> reg <span class="number">5.000000e+04</span> train accuracy: <span class="number">0.351857</span> val accuracy: <span class="number">0.356000</span></span><br><span class="line">    lr <span class="number">5.000000e-05</span> reg <span class="number">2.500000e+04</span> train accuracy: <span class="number">0.054959</span> val accuracy: <span class="number">0.068000</span></span><br><span class="line">    lr <span class="number">5.000000e-05</span> reg <span class="number">5.000000e+04</span> train accuracy: <span class="number">0.100265</span> val accuracy: <span class="number">0.087000</span></span><br><span class="line">    best validation accuracy achieved during cross-validation: <span class="number">0.387000</span></span><br></pre></td></tr></table></figure><h3 id="inline-question-1-1"><strong>Inline Question 1</strong></h3><p>It is possible that once in a while a dimension in the gradcheck willnot match exactly. What could such a discrepancy be caused by? Is it areason for concern? What is a simple example in one dimension where agradient check could fail? How would change the margin affect of thefrequency of this happening? <em>Hint: the SVM loss function is notstrictly speaking differentiable</em></p><p><span class="math inline">\(\color{blue}{\textit YourAnswer:}\)</span></p><p>It is possible that the numerical gradient does not match the actualgradient, because the max function is non-linear, continuous at 0 butnot derivable, so the numerical gradient is inaccurate at thissituation.</p><h3 id="inline-question-2-1"><strong>Inline question 2</strong></h3><p>Describe what your visualized SVM weights look like, and offer abrief explanation for why they look the way they do.</p><p><span class="math inline">\(\color{blue}{\textit YourAnswer:}\)</span></p><p>Each class of weighted visual image shows roughly the shape of theobjects in that class as well as the background colour. When an imagehas a shape or background colour similar to that class, there is a highprobability that it will be classified as such.</p><h2 id="svm部分参考链接">SVM部分参考链接:</h2><ol type="1"><li><p>cs231n官网: <ahref="https://cs231n.github.io/">https://cs231n.github.io/</a></p></li><li><p>深度学习课程 CS231n Assignment1 SVM部分: <ahref="http://marvolo.top/archives/17202">http://marvolo.top/archives/17202</a></p></li><li><p>CS231-Multi-calss SVM的求导: <ahref="https://www.cnblogs.com/chenyusheng0803/p/10018306.html">https://www.cnblogs.com/chenyusheng0803/p/10018306.html</a></p></li><li><p>机器学习算法：梯度下降法——原理篇 <ahref="https://www.jianshu.com/p/424b7b70df7b">https://www.jianshu.com/p/424b7b70df7b</a></p></li><li><p>CS231N作业详解零基础版： <ahref="https://www.bilibili.com/video/BV19z411b7u9/?p=9&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a">https://www.bilibili.com/video/BV19z411b7u9/?p=9&amp;vd_source=f0de9c6453942ba082fa767eb7aa958a</a></p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;cs231n-assignment-1&quot;&gt;CS231n Assignment 1&lt;/h1&gt;
&lt;p&gt;作业1分为五个部分：KNN、SVM、Softmax classifier、2层神经网络、Higher
Level Representations: Image F</summary>
      
    
    
    
    <category term="笔记" scheme="https://serika-onoe.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="介绍" scheme="https://serika-onoe.github.io/tags/%E4%BB%8B%E7%BB%8D/"/>
    
    <category term="深度学习" scheme="https://serika-onoe.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="cs231n" scheme="https://serika-onoe.github.io/tags/cs231n/"/>
    
    <category term="作业" scheme="https://serika-onoe.github.io/tags/%E4%BD%9C%E4%B8%9A/"/>
    
  </entry>
  
  <entry>
    <title>Transformer解析，适合没有NLP基础的小白入门 （上）</title>
    <link href="https://serika-onoe.github.io/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8A%EF%BC%89/"/>
    <id>https://serika-onoe.github.io/2022/12/14/Transformer%E8%A7%A3%E6%9E%90%EF%BC%8C%E9%80%82%E5%90%88%E6%B2%A1%E6%9C%89NLP%E5%9F%BA%E7%A1%80%E7%9A%84%E5%B0%8F%E7%99%BD%E5%85%A5%E9%97%A8%EF%BC%88%E4%B8%8A%EF%BC%89/</id>
    <published>2022-12-14T01:42:13.000Z</published>
    <updated>2023-01-19T14:47:51.767Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言">前言</h1><p>我尝试在这篇文章，通过<strong>Encoder-Decoder-&gt;Attention-&gt;Transformer</strong>的顺序，为自己后续学习Transformer的不同模型变体打下基础。由于我此前没有NLP基础，所以可能更能够体会初学者的心路历程。为了方便读者阅读，我分为了上下两篇博客进行讲解，已经了解Attention基础的读者可以直接阅读（下）篇。</p><p>同时我发现使用Chatgpt进行代码讲解对一个新手来说真的非常友好。读者可以尝试一下，Chatgpt，我一般尊称它为“Chat老师”，它是一个非常耐心的老师，（最重要的，它还是免费哒！）这让我想起了柏拉图或者孔夫子的教学方式——通过青年问禅师的对话体，来回答读者的困惑并启发更深层次的哲学思考。</p><p><strong>Encoder-Decoder, Attention, Transformer三者的关系简述</strong></p><table><colgroup><col style="width: 33%" /><col style="width: 33%" /><col style="width: 33%" /></colgroup><thead><tr class="header"><th style="text-align: center;">概念</th><th style="text-align: center;">说明</th><th style="text-align: center;">应用</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">Encoder-Decoder</td><td style="text-align: center;">解决序列-序列问题的框架</td><td style="text-align: center;">文本摘要、文本翻译、问答系统</td></tr><tr class="even"><td style="text-align: center;">Attention</td><tdstyle="text-align: center;">改善Encoder-Decoder输出结果的机制，主要是帮助解码器更好地生成输出序列</td><tdstyle="text-align: center;">文本翻译、问答系统、图像标注、语音识别</td></tr><tr class="odd"><td style="text-align: center;">Transformer</td><tdstyle="text-align: center;">解决序列-序列问题的架构，使用了Encoder-Decoder框架和Attention机制</td><td style="text-align: center;">NLP的新架构，目前往多模态方向拓展</td></tr></tbody></table><h1 id="encoder-decoder框架">Encoder-Decoder框架</h1><h2 id="简介">简介</h2><p>对于自然语言处理(NLP)领域的典型问题，都可以简化为处理一个句子对&lt;Source,Target&gt;：</p><ul><li>文本摘要，Source是一篇文章，Target是概括性的几句描述语句</li><li>文本翻译，Source是中文句子，Target是英文句子</li><li>问答系统，Source是一句问句，Target是一句回答</li></ul><p>我们的目标是给定输入句子<code>Source</code>，通过Encoder-Decoder框架，最后生成目标句子<code>Target</code>。Source和Target分别由各自的单词序列构成。这其中的Encoder-Decoder是一种用于处理序列-序列问题的框架，编码器(Encoder)输入一个序列并输出一个编码，解码器(Decoder)使用这个编码来生成一个输出序列。</p><p>Encoder-Decoder框架不仅仅在文本领域广泛使用，在语音识别、图像处理等领域也经常使用。区别在于，文本处理和语音识别的Encoder部分通常采用RNN模型，图像处理的Encoder一般采用CNN模型。用句子对&lt;Source,Target&gt;举例来说就是：</p><ul><li>语音识别，Source是语音流，Target是对应的文本信息</li><li>图像描述，Source是一副图片，Target是图片内容的描述语。</li></ul><h2 id="原理">原理</h2><p>举个更具体的例子，我们以翻译给定的输入句子<spanclass="math inline">\(X\)</span>为例，通过Encoder-Decoder框架，最后生成目标句子<spanclass="math inline">\(Y\)</span>。其中<spanclass="math inline">\(X\)</span>和<spanclass="math inline">\(Y\)</span>分别由单词序列构成</p><ul><li>原始句子 <span class="math inline">\(X = (x_1, x_2, \cdots,x_m)\)</span></li><li>翻译的目标句子 <span class="math inline">\(Y = (y_1, y_2, \cdots,y_n)\)</span></li></ul><p>Encoder任务就是对输入句子<spanclass="math inline">\(X\)</span>进行编码，将输入句子通过非线性变换转化为中间语义表示<spanclass="math inline">\(C\)</span>：</p><p><span class="math display">\[C = F(x_1, x_2, \cdots, x_m)\]</span></p><p>Decoder任务就是根据句子<spanclass="math inline">\(X\)</span>的中间语义表示<spanclass="math inline">\(C\)</span>和之前已经生成的历史信息<spanclass="math inline">\(y_1, y_2, \cdots,y_{i-1}\)</span>来生成i时刻要生成的单词<spanclass="math inline">\(yi\)</span>。</p><p><span class="math display">\[y_i = G(C, y_1, y_2, \cdots, y_{i-1})\]</span></p><p>每个<spanclass="math inline">\(y_i\)</span>都依次这么产生，最终看起来就是整个系统根据输入句子<spanclass="math inline">\(X\)</span>生成了目标句子<spanclass="math inline">\(Y\)</span>。</p><p>Encoder-Decoder是通用的计算框架，Encoder,Decoder具体用什么模型，都可以自己选择。<code>(因此这可以是一个创新点)</code>Encoder-Decoder架构图如下</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/encoder-decoder.jpg"title="Encoder-Decoder架构图" /></p><center><code>图1: Encoder-Decoder架构图</code></center><h2 id="经典decoder形式及其问题">经典Decoder形式及其问题</h2>其中经典的Decoder有两种形式，对应两篇论文：<br /><a href="https://arxiv.org/abs/1406.1078">[论文1]Cho et al., 2014 .Learning Phrase Representations using RNN Encoder-Decoder forStatistical Machine Translation.</a><br /><imgsrc="https://img-blog.csdnimg.cn/20200322154916564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /><br /><center><code>图2: 输入序列的语义编码C均等影响输出序列</code></center><p><br/><br/> 论文1中指出，Ecoder、Decoder均使用了RNN，因为语义编码<spanclass="math inline">\({C}\)</span>包含了整个输入序列的信息，所以在计算每一时刻的输出<spanclass="math inline">\(y_t\)</span>时，都应该输入语义编码<spanclass="math inline">\({C}\)</span>，也就是在解码的每一步都引入输入信息<spanclass="math inline">\({C}\)</span>。下面用公式表达：</p><ul><li>Decoder中<span class="math inline">\(t\)</span>时刻的内部状态的<spanclass="math inline">\(h_{&lt;t&gt;}\)</span>为:</li></ul><p><spanclass="math display">\[h_{&lt;t&gt;}=f(h_{t-1},y_{t-1},C)\]</span></p><ul><li><span class="math inline">\(t\)</span>时刻的输出概率为</li></ul><p><spanclass="math display">\[p(y_t|y_{t-1},y_{t-2},\cdot,y_{1},C)=g(h_{&lt;t&gt;},y_{t−1},C)\]</span></p><p><spanclass="math inline">\(h_t\)</span>为当前t时刻的隐藏层的值，<spanclass="math inline">\(y_{t-1}\)</span>为上一时刻的预测输出。</p><a href="https://arxiv.org/abs/1409.3215">[论文2]Sutskever et al., 2014.Sequence to Sequence Learning with Neural Networks.</a><br /><img src="https://img-blog.csdnimg.cn/20200322162508369.png" /><br /><center><code>图3: 只在Decoder的初始输入引入语义编码C</code></center><p><br/><br/>论文2的方式相对简单，只在Decoder的初始输入引入语义编码<spanclass="math inline">\({C}\)</span>，将语义编码<spanclass="math inline">\({C}\)</span>作为隐藏层状态值<spanclass="math inline">\(h_{&lt;0&gt;}\)</span>的初始值，公式如下：</p><p><span class="math display">\[h_{&lt;0&gt;}=C\]</span></p><p><spanclass="math display">\[h_{&lt;t&gt;}=f(h_{&lt;t-1&gt;},y_{t-1})\]</span></p><p><spanclass="math display">\[p(y_t)=g(h_{&lt;t&gt;},y_{t−1})\]</span></p><p>这两种的Encoder-Decoder结构至少存在以下几个问题：</p><ol type="1"><li><p>如果按照论文1解码，意味着输入序列<spanclass="math inline">\(X\)</span>中任意单词对生成某个目标单词<spanclass="math inline">\(y_i\)</span>来说影响力都是相同的（其实如果Encoder是RNN的话，理论上越是后输入的单词影响越大，并非等权的，所以这种情况下有时逆序输入得到的准确率可能更高）</p></li><li><p>如果按照论文2解码，意味着输入序列的所有信息压缩在了一个语义编码<spanclass="math inline">\(C\)</span>中。如果输入序列过短，导致<spanclass="math inline">\(C\)</span>包含的信息不足，那么生成的输出序列可能不够准确。如果输入序列很长，那么Decoder在生成输出序列时可能会丢失重要信息，还可能出现梯度消失问题。</p></li></ol><h2 id="问题的简单例子">问题的简单例子</h2><p>相比Attention模型，原始的Encoder-Decoder模型可以看作是注意力不集中的模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下：</p><p><span class="math display">\[y_i = G(C, y_1, y_2, \cdots, y_{i-1})\]</span><br />代入具体值，即为<br /><span class="math display">\[y_1 = G(C)\]</span><br /><span class="math display">\[y_2 = G(C, y_1)\]</span><br /><span class="math display">\[y_3 = G(C, y_1, y_2)\]</span></p><p>其中<spanclass="math inline">\(f\)</span>是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码<spanclass="math inline">\(C\)</span>都是一样的，没有任何区别。</p><p>拿机器翻译来解释这个模型的Encoder-Decoder框架更好理解，比如输入的是英文句子：<code>Tom chase Jerry</code>，Encoder-Decoder框架逐步生成中文单词：“汤姆”，“追逐”，“杰瑞”。</p><p>在翻译“杰瑞”这个中文单词的时候，模型里面的每个英文单词对于翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要，但该模型无法体现这一点。</p><p>既然只引入一个<spanclass="math inline">\(C\)</span>不行，那咱们就用多个<spanclass="math inline">\(C\)</span>，让不同的语义编码<spanclass="math inline">\(C_i\)</span>对应生成目标句子的不同单词。这个时候我们就引入了改善Encoder-Decoder输出结果的Attention机制。</p><h1 id="attention机制">Attention机制</h1><h2 id="简介-1">简介</h2><p>总结一下前文：在文本翻译的Encoder-Decoder模型中，编码器会输出一个向量序列，而解码器则需要逐步生成目标语言的单词。其中的每一步，解码器都需要根据之前生成的单词和编码器的输出来预测下一个单词。为了让解码器能够更好地使用编码器的输出，我们可以使用Attention 机制。</p><p>Attention机制可以让解码器在每一步时重点关注编码器输出的某些部分，这样解码器就可以更准确地生成目标语言的单词。同时，编码器也可以使用Attention机制。在编码器的每一层中，它会使用自注意力机制来注意输入序列的不同部分。</p><h2 id="原理-1">原理</h2><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" /><center><code>图4: 引入了Attention机制的Encoder-Decoder框架</code></center><p><br/><br/>上图就是引入了Attention机制的Encoder-Decoder框架，我们一眼就能看出它不再只有一个单一的语义编码<spanclass="math inline">\(C\)</span>，而是有多个<spanclass="math inline">\(C_1,C_2,C_3\)</span>这样的编码。当我们在预测<spanclass="math inline">\(Y_1\)</span>时，可能<spanclass="math inline">\(Y_1\)</span>的注意力是放在<spanclass="math inline">\(C_1\)</span>上，那咱们就用<spanclass="math inline">\(C_1\)</span>作为语义编码，当预测<spanclass="math inline">\(Y_2\)</span>时，<spanclass="math inline">\(Y_2\)</span>的注意力集中在<spanclass="math inline">\(C_2\)</span>上，那咱们就用<spanclass="math inline">\(C_2\)</span>作为语义编码，以此类推...</p><h3 id="问题一">问题一</h3><blockquote><p>如何判断每次在做解码的时候，注意力应该放在输入序列的哪个位置呢？即怎么计算出<spanclass="math inline">\(C1，C2，...Cn\)</span>呢？</p></blockquote><p>我们可以假设，输入序列的每个单词对于翻译“杰瑞”的<strong>注意力应该不一样</strong>，如翻译“杰瑞”时：<span class="math display">\[(Tom, 0.3), \;  (Chase, 0.2), \;  (Jerry, 0.5)\]</span>每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。这对于正确翻译目标单词是会有帮助的。因为针对输入的每个单词，输出的不同位置都引入了新的信息，并且放大了对应输入部分的注意力概率。</p><p>而这样做的前提在于，目标句子中的每个单词都应该学会分配对应的源句子中每个单词的注意力概率信息。也就是说，在生成每个单词<spanclass="math inline">\(Y_i\)</span>的时候，源句子中对应的相关单词概率会高于无关的单词。我的理解是，Attention的意思就是让模型学会在输出下一个单词时，对输入序列与之相关的单词投入更多的注意力。这就实现了将相同的中间语义表示<spanclass="math inline">\(C\)</span>，替换成了根据当前生成单词而不断变化的<spanclass="math inline">\(C_i\)</span>。<strong>这是Attention机制的关键</strong>，具体的实现过程举例如下：</p><p>请观察下目标句子Target中每个单词的生成过程，和原模型相比，<spanclass="math inline">\(C\)</span>变成了<spanclass="math inline">\(C_i\)</span>：</p><p><span class="math display">\[y_i = G(C_i, y_1, y_2, \cdots, y_{i-1})\]</span><br />代入具体值，即为<br /><span class="math display">\[y_1 = G(C_1)\]</span><br /><span class="math display">\[y_2 = G(C_2, y_1)\]</span><br /><span class="math display">\[y_3 = G(C_3, y_1, y_2)\]</span></p><p>那么对于刚才的例子<code>“汤姆追逐杰瑞”</code>来说，<spanclass="math inline">\(C_i\)</span>可以表示为：</p><p><imgsrc="https://img-blog.csdnimg.cn/20200322173132643.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /></p><ul><li><spanclass="math inline">\(f2\)</span>函数代表Encoder对输入英文单词的某种变换函数，如果Encoder是用的RNN模型的话，这个<spanclass="math inline">\(f2\)</span>函数的结果往往是某个时刻输入<spanclass="math inline">\(x_i\)</span>后隐藏层节点的状态值</li><li><spanclass="math inline">\(g\)</span>函数代表Encoder根据单词的中间表示，合成整个句子中间语义表示的变换函数，一般的做法中，<spanclass="math inline">\(g\)</span>函数就是对构成元素加权求和，即下列公式：<span class="math display">\[C_i = ∑_{j=1}^{Lx}a_{ij}h_j\]</span><br /></li><li><ul><li><p><spanclass="math inline">\(L_x\)</span>代表输入句子Source的长度</p></li><li><p><spanclass="math inline">\(a_{ij}\)</span>代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数</p></li><li><p><spanclass="math inline">\(h_j\)</span>是Source输入句子中第j个单词的语义编码。</p><p><strong>假设<spanclass="math inline">\(C_i\)</span>指的就是上面例子所说的“汤姆”</strong></p></li><li><p><span class="math inline">\(L_x=3\)</span></p></li><li><p><spanclass="math inline">\(h_1=f2(&#39;Tom&#39;)，h_2=f2(&#39;Chase&#39;),h_3=f2(&#39;Jerry&#39;)\)</span></p></li><li><p><spanclass="math inline">\(a_{11}=0.6,a_{12}=0.2,a_{13}=0.2\)</span></p></li></ul>由此可见，我们通过一个加权求和函数，计算出<spanclass="math inline">\(C_i\)</span>，也就是不同解码位置对应的注意力概率分配，在这个例子中这个函数记作<spanclass="math inline">\(g\)</span>函数，如下图：</li></ul><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention2.jpg" /><center><code>图5: g函数过程可视化</code></center><h3 id="问题二">问题二</h3><blockquote><p>生成目标句子某个单词的时候，我们如何知道这个单词的注意力概率分配值各是多少呢？</p><p>比如在生成“汤姆”这个中文翻译词的时候，模型生成了“汤姆”对应的输入句子Source中各个单词的概率分布：<spanclass="math display">\[(Tom,0.6)(Chase,0.2) (Jerry,0.2)\]</span>这个分布是如何得到的呢？</p></blockquote><p>为了便于说明，我们假设对初始的Encoder-Decoder框架进行细化，Encoder,Decoder都采用RNN模型，这是比较常见的一种模型配置，架构图如下。</p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention3.jpg" /><center><code>图6: 初始Encoder-Decoder常用框架</code></center><p><br/><br/></p><p>对于采用RNN的Decoder来说，在时刻<spanclass="math inline">\(i\)</span>，如果要生成<spanclass="math inline">\(y_i\)</span>单词，我们是可以知道Target在生成<spanclass="math inline">\(y_i\)</span>之前的时刻 <spanclass="math inline">\(i-1\)</span> 时，隐层节点 <spanclass="math inline">\(i-1\)</span> 时刻的输出值<spanclass="math inline">\(H_{i-1}\)</span>的，而我们的目的是要计算生成<spanclass="math inline">\(y_i\)</span>时输入句子中的单词“Tom”、“Chase”、“Jerry”对<spanclass="math inline">\(y_i\)</span>来说的注意力分配概率分布，那么可以用Target输出句子<span class="math inline">\(i-1\)</span> 时刻的隐层节点状态<spanclass="math inline">\(H_{i-1}\)</span>去和输入句子Source中每个单词对应的RNN隐层节点状态<spanclass="math inline">\(h_j\)</span>进行对比，即<strong>通过函数<spanclass="math inline">\(F(h_j,H_{i-1})\)</span>来获得目标单词<spanclass="math inline">\(y_i\)</span>和每个输入单词对应的对齐可能性</strong>，（这个<spanclass="math inline">\(F\)</span>函数在不同论文里可能会采取不同的方法），<strong>然后函数<spanclass="math inline">\(F\)</span>的输出经过Softmax进行归一化，就得到了符合概率分布取值区间的注意力分配概率分布数值</strong>。比如<code>“汤姆追逐杰瑞”</code>的例子可以表示如下</p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention4.jpg" /><center><code>图7: “汤姆追逐杰瑞”的例子中“Tom”的注意力分配系数计算过程</code></center><p><br/><br/> 注意一下，这里咱们使用的是<strong>SoftAttention</strong>，所有的数据都会被注意到，并计算出相应的注意力权值，不会设置筛选条件。还有一种<strong>HardAttention</strong>，会在生成注意力权重后筛选掉一部分不符合条件的注意力，让它的注意力权值为0，即可以理解为不再注意这些不符合条件的部分。</p><h2 id="本质">本质</h2><p>如果把Attention机制从上文讲述例子中的Encoder-Decoder框架中剥离，并进一步做抽象，可以更容易看懂Attention机制的本质思想。</p><h3 id="从一个问题说起">从一个问题说起</h3><p><a href="https://arxiv.org/abs/1706.03762">[论文3]Ashish Vaswani.,2017 . Attention Is All You Need</a> 这篇论文讨论了使用Query,Key和Value向量计算Attention的方法。</p><blockquote><p>那么，QKV向量分别代表什么，为什么要使用这三个向量？</p></blockquote><p>Attention简单来说就是一些值的加权平均值，就像“汤姆追逐杰瑞”例子中提到的：<span class="math display">\[C_i = ∑_{j=1}^{Lx}a_{ij}h_j\]</span><br />- <span class="math inline">\(L_x\)</span>代表输入句子Source的长度 -<spanclass="math inline">\(a_{ij}\)</span>代表在Target输出第i个单词时Source输入句子中第j个单词的注意力分配系数- <spanclass="math inline">\(h_j\)</span>是Source输入句子中第j个单词的语义编码。</p><p>Transformer的关键思想是Attention机制。相比于循环网络（如 RNN/LSTM）必须一个接一个地查看每个单词（因此存在长期记忆问题），Transformer不仅考虑来自一个方向的输入，而且从整体上可以同时看到输入中的所有标记，以确定每个单词应该“注意”哪些内容。<strong>也就是说，我们期待给定一个输入Source之后，输出能同时看到所有的输入信息，然后每个输出的位置根据不同权重选择自己的注意点做输出。</strong></p><p>这个类似于搜索引擎或者推荐系统的基本原理，以谷歌Google为例:</p><ul><li>用户给定需查询的问题(Query)</li><li>Google后台有各种文章标题(Key)和文章本身(Value)</li></ul><p>通过定义并计算问题Query与文章标题Key的相关性，用户就可以找到最匹配的文章Value。当然Attention的目标不是通过Query得到Value，而是通过Query得到Value的加权和。因此我们可以认为，<strong>Attention实际上做的就是数据库中的检索操作(Attention is actually working as a retrieval operation in a database.)</strong></p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention_bz1.jpg" /><center><code>图8: Attention机制的本质思想</code></center><p><br/><br/>所以我们可以这样来看待Attention机制：我们的输入、输出记为句子对&lt;Source,Target&gt;。将Source中的构成元素想象成是由一系列的&lt;Key,Value&gt;数据对构成，此时给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，得到每个Key对应Value的权重系数，然后对Value进行加权求和，即得到了最终的Attention数值。所以<strong>本质上Attention机制是对Source中元素的Value值进行加权求和，而Query和Key用来计算对应Value的权重系数。</strong>即可以将其本质思想改写为如下公式，（其中，<spanclass="math inline">\(L_x=||Source||\)</span>代表Source的长度）：</p><p><img src="https://img-blog.csdnimg.cn/20200322195829296.png" /></p><p>Key和Value具有强关联性，所以它们也可以是同一个东西，比如Google搜索理论上可以直接搜索到文章本身，而不用列出文章的标题列表。再比如“汤姆追逐杰瑞”的翻译例子里，Source中的Key和Value指向的都是同一个东西：输入句子中每个单词对应的语义编码<spanclass="math inline">\(C\)</span>或<spanclass="math inline">\(C_i\)</span>，因此可能不容易看出这种能够体现本质思想的结构。</p><p>当然，从概念上理解，把Attention仍然理解为从大量信息中有选择地筛选出少量重要信息并聚焦到这些重要信息上，忽略大多不重要的信息，这种思路仍然成立。聚焦的过程体现在权重系数的计算上，权重越大越聚焦于其对应的Value值上，即权重代表了信息的重要性，而Value是其对应的信息。</p><p>刚才提到“Attention实际上做的就是数据库中的检索操作”，因此也可以用寻址类比检索过程：我们将Attention机制看作一种软寻址(SoftAddressing):Source可以看作存储器内存储的内容，元素由地址Key和值Value组成，当前有个<spanclass="math inline">\(Key=Query\)</span>的查询，目的是取出存储器中对应的Value值，即Attention数值。通过Query和存储器内元素Key的地址进行相似性比较来寻址，之所以说是软寻址，是因为不像一般寻址只从存储内容里面找出一条内容，而是可能从每个Key地址都会取出内容，取出内容的重要性根据Query和Key的相似性来决定，之后对Value进行加权求和，这样就可以取出最终的Value值，也即Attention值。</p><h2 id="计算过程">计算过程</h2><p>Attention机制的具体计算过程，如果对目前大多数方法进行抽象的话，可以将其归纳为两个过程：</p><ol type="1"><li>根据Query和Key计算权重系数<ul><li>根据Query和Key计算两者的相似性或者相关性</li><li>对第一阶段的原始分值进行归一化处理</li></ul></li><li>根据权重系数对Value进行加权求和。</li></ol><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention_bz2.jpg" /></p><p>在该图中，把上述的第一个过程细化为两个阶段。因此三个阶段分别为</p><ul><li>阶段1：Query与每一个Key计算相似性得到相似性评分<spanclass="math inline">\(s\)</span></li><li>阶段2：将<spanclass="math inline">\(s\)</span>评分进行softmax转换成[0,1]之间的概率分布</li><li>阶段3：将[a1,a2,a3…an]作为权值矩阵对Value进行加权求和得到最后的Attention值</li></ul><h3 id="阶段1">阶段1</h3><p>在阶段1，可以引入不同的函数和计算机制，根据Query和某个<spanclass="math inline">\(Key_i\)</span>，计算两者的相似性或者相关性，最常见的方法包括：求两者的向量点积、求两者的向量Cosine相似性或者通过再引入额外的神经网络来求值：</p><p><img src="https://img-blog.csdnimg.cn/20200322200849586.png" /></p><h3 id="阶段2">阶段2</h3><p>阶段1产生的分值根据具体产生的方法不同，其结果数值取值范围也不一样，因此阶段2引入类似SoftMax的计算方式对第一阶段的得分进行数值转换，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过SoftMax的内在机制更加突出重要元素的权重。即一般采用如下公式计算：</p><p><img src="https://img-blog.csdnimg.cn/20200322205913297.png" /></p><h3 id="阶段3">阶段3</h3><p>阶段2的计算结果<span class="math inline">\(a_i\)</span>即为<spanclass="math inline">\(Value_i\)</span>对应的权重系数，然后阶段3进行加权求和即可得到Attention数值：</p><p><img src="https://img-blog.csdnimg.cn/20200322210027426.png" /></p><p>通过如上三个阶段的计算，即可求出针对Query的Attention数值，目前绝大多数具体的注意力机制计算方法都符合上述的三阶段抽象计算过程。</p><h2 id="优缺点">优缺点</h2><p>Attention是一种常用的解决长序列处理问题的技术，不过它真的无懈可击吗？</p><p><strong>优点：</strong></p><ol type="1"><li><p><strong>可并行计算：</strong>Attention机制解决了RNN不能并行计算的问题。这里需要说明一下，咱们在训练Attention机制的seq2seq模型的时候，decoder并不是说预测出了一个词，然后再把这个词作为下一个输入，而是有监督训练：咱们已经有了target的数据，所以是可以并行输入的，可以并行计算decoder的每一个输出，但是实际做预测的时候，是没有target数据的，这个时候就需要基于上一个时间节点的预测值来当做下一个时间节点decoder的输入。所以节省的是训练的时间。</p></li><li><p><strong>改善长序列处理能力：</strong>Attention机制可以帮助模型在处理长序列时，选择关注其中重要的部分，从而提高模型的准确性。</p></li><li><p><strong>模型权重可视化：</strong>Attention机制通过计算权重系数，能够得到每个输入序列中元素对输出结果的重要性，这有助于我们理解模型学到了什么。</p></li></ol><p><strong>缺点：</strong></p><p>1.<strong>Encoder部分未实现并行运算：</strong>Encoder部分依旧采用的是RNN，LSTM这些按照顺序编码的模型，不够完美。</p><p>2.<strong>计算复杂度高：</strong>Attention机制通常需要对整个输入序列进行遍历，计算量大，尤其是在处理长序列时。</p><h3 id="改进self-attention">改进：Self Attention</h3><p>为了改进上面两个缺点，更加完美的Self-Attention出现了。</p><p>在一般任务的Encoder-Decoder框架中，输入Source和输出Target内容是不一样的，比如对于英-中机器翻译来说，Source是英文句子，Target是对应的翻译出的中文句子，Attention机制发生在Target的元素和Source中的所有元素之间。而SelfAttention顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为<spanclass="math inline">\(Target=Source\)</span>这种特殊情况下的注意力计算机制。其具体计算过程是一样的，只是计算对象发生了变化而已，相当于是<spanclass="math inline">\(Query=Key=Value\)</span>，计算过程与attention一样，所以这里不再赘述其计算过程细节。</p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Transformer/self_attention.jpg" /><center><code>图9: self-attention可以帮助模型理解代词的含义</code></center><p><br/><br/>上图是self-attention的一个例子。我们想知道这句话中的its，在这句话里its指代的是什么，与哪一些单词相关，那么就可以将its作为Query，然后将这一句话作为Key和Value来计算attention值，找到与这句话中its最相关的单词。通过self-attention我们发现its在这句话中与之最相关的是Law和application，通过我们分析语句意思也十分吻合。</p><p>如此引入SelfAttention后，会更容易捕获句子中长距离的依赖关系，因为如果是RNN或者LSTM，需要依次序列计算，对于远距离的相互依赖的特征，要经过若干时间步步骤的信息累积才能将两者联系起来，而距离越远，有效捕获的可能性越小。但是SelfAttention在计算过程中会直接将句子中任意两个单词的联系通过一个计算步骤直接联系起来，所以远距离依赖特征之间的距离被极大缩短。</p><p>除此外，Self Attention还做了其他的改进，主要如下：</p><ol type="1"><li><p>将输入序列中每个元素和其他元素之间的关系考虑进来，从而更好的理解语境。</p></li><li><p>通过添加多头机制来提高模型的泛化能力，从而增加了Encoder计算的并行性。</p></li><li><p>通过添加位置编码来确保序列元素之间的相对顺序对于结果的影响。</p></li></ol><p>关于这部分内容，我会在下篇文章啊详细讲解SelfAttention的工作原理。</p><h1 id="结束语">结束语</h1><p>在这篇文章中，我们已经了解了 Encoder-Decoder 和 Attention的概念，这些概念在自然语言处理中非常有用。不过，我们希望这些模型不仅仅可以用于处理自然语言，还可以用在更多的领域，比如我最终的目的是使用Transformer来预测时序数据，因此Transformer将会是我在下一篇文章的重点。且看它如何在Encoder-decoder 和 Attention 的基础上开疆拓土，让我们拭目以待！</p><h1 id="参考链接">参考链接</h1><ol type="1"><li><p>Tensorflow的官方教程：Neural machine translation with aTransformer and Keras:</p><p><ahref="https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb">https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb</a></p></li><li><p>史上最小白之Attention详解:</p><p><ahref="https://blog.csdn.net/Tink1995/article/details/105012972">https://blog.csdn.net/Tink1995/article/details/105012972</a></p></li><li><p>史上最全Transformer面试题系列（一）：灵魂20问帮你彻底搞定Transformer-干货！:</p><p><ahref="https://zhuanlan.zhihu.com/p/148656446">https://zhuanlan.zhihu.com/p/148656446</a></p></li><li><p>Encoder-Decoder Models for Natural Language Processing</p><p><ahref="https://www.baeldung.com/cs/nlp-encoder-decoder-models">https://www.baeldung.com/cs/nlp-encoder-decoder-models</a></p></li><li><p>ChatGPT3：</p><p><ahref="https://chat.openai.com/chat">https://chat.openai.com/chat</a></p></li><li><p>自然语言处理中的Attention Model：是什么以及为什么[一]：</p><p><ahref="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247484196&amp;idx=1&amp;sn=efa6b79d24b138ff79f33ec3426c79e2&amp;chksm=ebb43bf0dcc3b2e6e69ff3b7d7cabf28744e276ee08ccfcfc1dc2c3f0bf52a122c9bd77ff319&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247484196&amp;idx=1&amp;sn=efa6b79d24b138ff79f33ec3426c79e2&amp;chksm=ebb43bf0dcc3b2e6e69ff3b7d7cabf28744e276ee08ccfcfc1dc2c3f0bf52a122c9bd77ff319&amp;scene=21#wechat_redirect</a></p></li><li><p>Query, Key and Value in Attention mechanism</p><p><ahref="https://lih-verma.medium.com/query-key-and-value-in-attention-mechanism-3c3c6a2d4085">https://lih-verma.medium.com/query-key-and-value-in-attention-mechanism-3c3c6a2d4085</a></p></li><li><p>如何理解 Transformer 中的 Query、Key 与 Value</p><p><ahref="https://blog.csdn.net/yangyehuisw/article/details/116207892">https://blog.csdn.net/yangyehuisw/article/details/116207892</a></p></li><li><p>深度学习中的注意力机制(2017版)</p><p><ahref="https://blog.csdn.net/malefactor/article/details/78767781">https://blog.csdn.net/malefactor/article/details/78767781</a></p></li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;前言&lt;/h1&gt;
&lt;p&gt;我尝试在这篇文章，通过
&lt;strong&gt;Encoder-Decoder-&amp;gt;Attention-&amp;gt;Transformer&lt;/strong&gt;
的顺序，为自己后续学习Transformer的不同模型变体打下基础。由于我此前</summary>
      
    
    
    
    <category term="笔记" scheme="https://serika-onoe.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="科研" scheme="https://serika-onoe.github.io/tags/%E7%A7%91%E7%A0%94/"/>
    
    <category term="项目" scheme="https://serika-onoe.github.io/tags/%E9%A1%B9%E7%9B%AE/"/>
    
    <category term="transformer" scheme="https://serika-onoe.github.io/tags/transformer/"/>
    
  </entry>
  
  <entry>
    <title>强化学习初探</title>
    <link href="https://serika-onoe.github.io/2022/12/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/"/>
    <id>https://serika-onoe.github.io/2022/12/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/</id>
    <published>2022-12-12T15:17:01.000Z</published>
    <updated>2022-12-13T02:17:47.012Z</updated>
    
    <content type="html"><![CDATA[<h1 id="强化学习">强化学习</h1><p>强化学习是一类对目标导向的学习与决策问题进行理解和自动化处理的算法。它强调智能体通过与环境的直接互动来学习，无需像监督学习一样密集的样本级标签标注，通过奖励来学习合理的策略。</p><p>强化学习包含2个可以进行交互的对象：智能体和环境，它们的定义与介绍如下：</p><p>智能体：可以感知环境的状态，并根据反馈的奖励学习选择一个合适的动作，我们希望它能最大化长期总收益。环境：环境会接收智能体执行的一系列动作，对这一系列动作进行评价并转换为一种可量化的信号反馈给智能体。环境对智能体来说是一套相对固定的规则。</p><h2 id="目的">目的</h2><p>强化学习的目的是让计算机自行学习，以获得最好的未来结果。它通过不断的尝试和评估来实现这一目标，从而使计算机能够根据结果自动调整自身来获得最佳结果。</p><h2 id="典型算法">典型算法</h2><p>强化学习中使用的典型算法有：Q学习、蒙特卡洛树搜索、模仿学习、深度强化学习等。</p><h2 id="优点">优点</h2><p>强化学习的优点有：</p><ul><li>可以解决复杂的决策问题，比如游戏、控制和规划。</li><li>可以快速解决不断变化的环境问题和复杂的决策问题。</li><li>可以在非常少的知识和计算量情况下学习。</li></ul><h2 id="缺点">缺点</h2><p>强化学习的缺点有：</p><ul><li>需要大量的试验数据，因此在小数据集上表现不佳。</li><li>可能会出现“收敛停滞”现象，即它可能会在局部最优解上停止收敛。</li><li>它可能会陷入错误的局部最优解。</li></ul><h2 id="应用">应用</h2><p>强化学习在实际应用中有很多，比如：自动驾驶、智能家居、游戏、虚拟助手、自动投资、博弈机器人等。</p><h2 id="参考网站">参考网站</h2><ol type="1"><li>Reinforcement Learning and ArtificialIntelligence：https://www.reinforcementlearning.ai/</li><li>Deep ReinforcementLearning：https://deepreinforcementlearning.ai/</li><li>Andrew Ng Reinforcement Learning：https://www.andrewng.org/</li><li>Open AI Gym：https://gym.openai.com/</li><li>Google DeepMind：https://deepmind.com/research/open-source/</li><li>Berkeley AI Research：https://bair.berkeley.edu/</li><li>Udacity ReinforcementLearning：https://www.udacity.com/course/reinforcement-learning--ud600</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;强化学习&quot;&gt;强化学习&lt;/h1&gt;
&lt;p&gt;强化学习是一类对目标导向的学习与决策问题进行理解和自动化处理的算法。它强调智能体通过与环境的直接互动来学习，无需像监督学习一样密集的样本级标签标注，通过奖励来学习合理的策略。&lt;/p&gt;
&lt;p&gt;强化学习包含2个可以进行交互的对</summary>
      
    
    
    
    <category term="介绍" scheme="https://serika-onoe.github.io/categories/%E4%BB%8B%E7%BB%8D/"/>
    
    
    <category term="强化学习" scheme="https://serika-onoe.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="人工智能" scheme="https://serika-onoe.github.io/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
    <category term="算法" scheme="https://serika-onoe.github.io/tags/%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>我的项目经历</title>
    <link href="https://serika-onoe.github.io/2022/12/09/%E6%88%91%E7%9A%84%E7%A7%91%E7%A0%94%E7%BB%8F%E5%8E%86/"/>
    <id>https://serika-onoe.github.io/2022/12/09/%E6%88%91%E7%9A%84%E7%A7%91%E7%A0%94%E7%BB%8F%E5%8E%86/</id>
    <published>2022-12-09T03:10:23.000Z</published>
    <updated>2022-12-13T02:06:24.224Z</updated>
    
    <content type="html"><![CDATA[<h1 id="我的项目经历">我的项目经历</h1><p>目录：</p><!-- vscode-markdown-toc --><p><strong>一、软件编程项目</strong> * 1. <ahref="#">无人机集群电磁仿真设计</a> * 2. <ahref="#-1">弱监督条件下的点云语义理解</a> * 3. <ahref="#Python">Python爬取国家统计数据</a> * 4. <ahref="#app">app制作与安全性分析</a> * 5. <ahref="#-1">绕过认证系统实验</a> * 6. <a href="#AI">AI玩俄罗斯方块</a> *7. <a href="#GUI">手写数字识别GUI</a> * 8. <ahref="#DIY">马里奥DIY版</a> * 9. <a href="#-1">简易版魔塔</a> * 10. <ahref="#-1">疫情地图小程序</a></p><p><strong>二、硬件控制项目</strong> * 1. <ahref="#-1">玩具狗的多种开关方式</a> * 2. <a href="#GPS">GPS欺骗</a> * 3.<a href="#Arduino">基于Arduino的音乐播放器</a><!-- vscode-markdown-toc-config    numbering=true    autoSave=true    /vscode-markdown-toc-config --> <!-- /vscode-markdown-toc --></p><h1 id="软件编程项目">软件编程项目</h1><h2 id="无人机集群电磁仿真设计">1.<a name=''></a>无人机集群电磁仿真设计</h2><p>2021.1 - 2021.8</p><hr /><h3 id="描述">1.1. <a name='-1'></a>描述：</h3><p>我们通过以下步骤解决无人机集群的目标电磁特征数据的获取问题，提出了解决无人机集群问题目标检测的创新思路。</p><p>首先，以 "Gremlin"无人机为代表的典型单架固定翼无人机为例，基于多级快速多极法（MLFMM）进行电磁计算。然后，利用雷达散射截面（RCS）仿真数据和二维反合成孔径雷达（ISAR）成像来验证上述仿真结果的结果准确性。最后，我们对无人机集群的RCS模拟数据进行了模拟和验证。</p><p>用到的编程语言和软件工具有：</p><p>（1）Solidworks -- 建立两类无人机的三维模型 （2）Feko电磁仿真软件 --基于MLFMM对无人机进行电磁仿真计算 （3）MATLAB --仿真数据清洗和处理，ISAR成像算法实现</p><h3 id="成绩">1.2. <a name='-1'></a>成绩：</h3><p>该项目过程和结论已在2021年的CIE雷达会议上提出。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/uav_prove.jpg" /></p><p>我们总结和分析了无人机目标 "Gremlin"在单一和集群情况下的电磁散射计算，这些都是基于的应用要求和技术困难。全极化静态电磁散射计算了"Gremlin"在典型频段的全极化静态电磁散射特征数据，并将其结果用于进行集群目标成像，可以清楚地看到"Gremlin "的翼尖特征。</p><h4 id="固定翼代表1--美军捕食者无人机">1.2.1.<a name='1--'></a>固定翼代表1--美军“捕食者”无人机</h4><p><imgsrc="https://gimg2.baidu.com/image_search/src=http%3A%2F%2Finews.gtimg.com%2Fnewsapp_match%2F0%2F2985298796%2F0.jpg&amp;refer=http%3A%2F%2Finews.gtimg.com&amp;app=2002&amp;size=f9999,10000&amp;q=a80&amp;n=0&amp;g=0n&amp;fmt=jpeg?sec=1617869104&amp;t=54d2c95a6f95de01679291e9b76837dd" /></p><center>固定翼代表1--美军“捕食者”无人机实物图</center><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/uav1.jpg" /></p><center>固定翼代表1--美军“捕食者”无人机模型图</center><h4 id="固定翼代表2--美军小精灵无人机">1.2.2.<a name='2--'></a>固定翼代表2--美军“小精灵”无人机</h4><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/uav2.jpg" /></p><center>固定翼代表2--美军“小精灵”无人机实物图</center><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/uav3.jpg" /></p><center>固定翼代表2--美军“小精灵”无人机模型图</center><h4 id="旋翼代表--大疆f450无人机">1.2.3.<a name='--F450'></a>旋翼代表--大疆F450无人机</h4><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/uav4.jpg" /></p><center>旋翼代表--大疆F450无人机实物图</center><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/uav5.jpg" /></p><center>旋翼代表--大疆F450无人机模型图</center><h2 id="弱监督条件下的点云语义理解">2.<a name='-1'></a>弱监督条件下的点云语义理解</h2><p>2020.10 - 2021.1</p><hr /><h3 id="描述-1">2.1. <a name='-1'></a>描述：</h3><p>为解决三维点云语义分割中数据标注昂贵的问题，尝试使用弱监督学习的方法进行研究。进行了论文综述，同时复现了“PointNet++”代码。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/pointcloud%20mind.png" /></p><p>用到的编程语言和软件工具有：</p><p>（1）Python -- 通过Jupyter Notebook复现代码</p><h3 id="成绩-1">2.2. <a name='-1'></a>成绩：</h3><p>基于百度AI平台的PaddlePaddle框架，对十组家具图片生成的无序点云进行分类处理，复现了“PointNet++”论文中91.9%的准确率。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/point%20cloud2.jpg" /></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/point%20cloud3.jpg" /></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/point%20cloud4.jpg" /></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/point%20cloud5.jpg" /></p><h2 id="python爬取国家统计数据">3.<a name='Python'></a>Python爬取国家统计数据</h2><p>2021.1</p><hr /><h3 id="描述-2">3.1. <a name='-1'></a>描述：</h3><p>独立完成，爬取“国家统计局”八个省份、六个季度的城乡居民收支基本情况</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/crawl%20mind.jpg" /></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/crawl2.jpg" /></p><p>用到的编程语言和软件工具有：</p><p>（1）Python -- 通过panda库实现爬虫功能，通过xlwings库实现表格处理</p><h3 id="成绩-2">3.2. <a name='-1'></a>成绩：</h3><p>爬取国家统计局八个省份、六个季度的表格数据到excel表格中，同时代码可筛去无效数据，自动整理excel表格，通过xlwings库实现数据居中、自适应列宽等功能。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/crawl3.jpg" /></p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/crawl4.jpg" /></p><h2 id="app制作与安全性分析">4.<a name='app'></a>app制作与安全性分析</h2><p>2019.10 - 2020.1</p><hr /><h3 id="描述-3">4.1. <a name='-1'></a>描述：</h3><p>app实现要求:该app具有用户/口令登录功能，并可供使用者注册。注册时口令只作长度限制（如8位长度），但强度暂不作要求。用户名/口令保存在手机上，口令保存时作加密处理（自行选择加密算法）。</p><p>功能比较简单，弹出一浮窗，显示app需要获取存储空间、设备信息、地理位置权限的提示，可选择授权或拒绝。通过在手机上运行此app，注册若干个账号，口令设置时有强口令，也有弱口令，然后分析其安全性，加以改进。</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/app1.png" /></p><p>客户端登录功能的相关代码（Kotlin）：</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/app2.png" /></p><p>获取存储空间、设备信息、地理位置权限这些权限的相关语句：</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/app3.png" /></p><p>Androbugs分析截图：</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/app5.png" /></p><p>分析后修改了原有app的注册/登录认证方式，采用OAuth2规范中的授权码模式：</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/app6.png" /></p><p>将外部存储改为内部存储：</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/app7.png" /></p><p>用到的编程语言和软件工具有：</p><p>（1）Kotlin -- 通过Android Studio实现app功能 （2）Androbugs --分析app安全性</p><h3 id="成绩-3">4.2. <a name='-1'></a>成绩：</h3><p>完整的运行视频如下：</p><p><iframe height=498 width=510 src='https://player.youku.com/embed/XNDg0MTQwNjUwOA==' frameborder=0 allowfullscreen></iframe></p><h2 id="绕过认证系统实验">5. <a name='-1'></a>绕过认证系统实验</h2><p>2019.9</p><hr /><h3 id="描述-4">5.1. <a name='-1'></a>描述：</h3><p>很多商场、饭店的商业WIFI采用了WEBPortal认证方式，但有些认证系统存在漏洞，可以利用 DNS TUNNEL绕过网关计费系统。存在这种漏洞的商业WIFI环境，并且可验证能够利用 DNSTUNNEL 穿越网关计费系统。</p><p>DNSTunnel真正用来“免密上网”，其实不太实际。尽管我们组已经“砍掉了”云服务器的开支（把代理服务器搬到本地来进行了），结果整个实验还是花掉了6块钱来购买域名。</p><p>用到的编程语言和软件工具有：</p><p>（1）树莓派 -- 搭建本地代理服务器 （2）Portal --拓扑结构分析和DNS仿真配置</p><h3 id="成绩-4">5.2. <a name='-1'></a>成绩：</h3><p>整个实验其实是告诉我们：黑客会“见缝插针”，DNS这样专门用于域名查询的协议，也可以被拿来传输数据。若将来需要做网络应用层的协议设计、维护工作，一定要加倍小心，在网络安全方面要非常谨慎。另外对于个人来说，如果连接到公共网络，一定要提高警惕，谨防“高科技偷窃”，因为我们难以知道黑客下一个目标是哪里。</p><iframe height="498" width="510" src="https://player.youku.com/embed/XNDg5NzI1OTY0OA==" frameborder="0" allowfullscreen></iframe><h2 id="ai玩俄罗斯方块">6. <a name='AI'></a>AI玩俄罗斯方块</h2><p>2018.9</p><hr /><h3 id="描述-5">6.1. <a name='-1'></a>描述：</h3><p>利用pygame实现俄罗斯方块游戏，同时设置了一个AI（甚至都可以不用机器学习算法）</p><p>用到的编程语言和软件工具有：</p><p>（1）Python -- 实现俄罗斯方块逻辑和AI算法</p><p>AI算法基本思想就是，遍历当前可操作的俄罗斯方块和下一个可操作的俄罗斯方块(根据不同的策略，即选择不同的位置和旋转角度)下落到底部后组成的所有可能的未来场景</p><p>未来场景的优劣判断依据：</p><pre><code>1）可消除的行数；2）堆积后的俄罗斯方块内的虚洞数量；3）堆积后的俄罗斯方块内的小方块数量；4）堆积后的俄罗斯方块的最高点；5）堆积后的俄罗斯方块的高度(每一列都有一个高度)标准差；6）堆积后的俄罗斯方块的高度一阶前向差分；7）堆积后的俄罗斯方块的高度一阶前向差分的标准差；8）堆积后的俄罗斯方块的最高点和最低点之差。</code></pre><p>从这些未来场景中选择一个最优的，其对应的当前可操作的俄罗斯方块的行动策略即为当前解</p><h3 id="成绩-5">6.2. <a name='-1'></a>成绩：</h3><p>视频演示一边拖动源码一边游戏在自动运行，以显示不是手动操作的hh</p><p><iframe height=498 width=510 src='https://player.youku.com/embed/XNDg0MTAwMTY4MA==' frameborder=0 allowfullscreen></iframe></p><h2 id="手写数字识别gui">7. <a name='GUI'></a>手写数字识别GUI</h2><p>2020.11 - 2021.1</p><hr /><h3 id="描述-6">7.1. <a name='-1'></a>描述：</h3><p>不使用框架，实现手写数字识别GUI开发</p><p>用到的编程语言和软件工具有：</p><p>（1）Python --开发GUI界面（基于Qt5），涉及基本bp算法实现和正则化（BN，L2正则化，RMSProp）等优化算法，并实现pyqt界面及三个功能:mnist中抽取识别,上传图片识别,画板手写识别</p><h3 id="成绩-6">7.2. <a name='-1'></a>成绩：</h3><p><iframe height=498 width=510 src='https://player.youku.com/embed/XNDg0MTAwMDg3Mg==' frameborder=0 allowfullscreen></iframe></p><h2 id="马里奥diy版">8. <a name='DIY'></a>马里奥DIY版</h2><p>2018.4 - 2018.6</p><hr /><h3 id="描述-7">8.1. <a name='-1'></a>描述：</h3><p>DIY了一个马里奥，在原版的基础上改变了生命设定和地图场景：</p><p>生命上限可以通过吃蘑菇增加，并回复一部分血量，同时若身体是小人形态则变成大人形态。受击时形态不变化，扣相应的HP。</p><p>用到的编程语言和软件工具有：</p><p>（1）Gamemaker -- 开发游戏界面，绘制游戏地图及玩法逻辑实现</p><p>成绩：</p><p><strong>通关演示及简单功能演示</strong></p><p><iframe height=498 width=510 src='https://player.youku.com/embed/XNDg0MTAwMjQxMg==' frameborder=0 allowfullscreen></iframe></p><p><strong>若HP为0，则直接死亡</strong></p><p><iframe height=498 width=510 src='https://player.youku.com/embed/XNDg0MTAwNDE1Mg==' frameborder=0 allowfullscreen></iframe></p><h2 id="简易版魔塔">9. <a name='-1'></a>简易版魔塔</h2><p>2017.11 - 2018.1</p><hr /><p>###描述：</p><p>命令行界面，可操作的简易版魔塔</p><p>用到的编程语言和软件工具有：</p><p>（1）C++ -- 通过命令行和字符串绘制游戏地图及玩法逻辑实现</p><h3 id="成绩-7">9.1. <a name='-1'></a>成绩：</h3><p><strong>通关演示及简单功能演示</strong></p><iframe height="498" width="510" src="https://player.youku.com/embed/XNDg5NzU4NDgzMg==" frameborder="0" allowfullscreen></iframe><h2 id="疫情地图小程序">10. <a name='-1'></a>疫情地图小程序</h2><p>2020.6</p><hr /><h3 id="描述-8">10.1. <a name='-1'></a>描述：</h3><p>疫情期间做的一个疫情地图，分为国内、国外两个板块，每个板块分为当日累计疫情、当日新增疫情两个子板块，引用了开课吧的数据源，颜色越深说明感染人数越多。</p><p>用到的编程语言和软件工具有：</p><p>（1）html -- 引用开课吧数据源，尝试进行数字可视化</p><h3 id="成绩-8">10.2. <a name='-1'></a>成绩：</h3><iframe height="498" width="510" src="https://player.youku.com/embed/XNDg5NzI4NDU0MA==" frameborder="0" allowfullscreen></iframe><h1 id="硬件控制项目">硬件控制项目</h1><h2 id="玩具狗的多种开关方式式">11.<a name='-1'></a>玩具狗的多种开关方式式</h2><h2 id="gps欺骗">12. <a name='GPS'></a>GPS欺骗</h2><p>2019.9 - 2019.11</p><hr /><h3 id="描述-9">12.1. <a name='-1'></a>描述：</h3><p>在Linux环境下，应用GPS卫星定位的手机,通过HackRFOne发射欺骗信号，实现点到点欺骗或轨迹欺骗，可在1、2分钟内成功欺骗到指定位置在指定轨迹内依据给定的加速度、速度进行不间断运动。</p><p>用到的编程语言和软件工具有：</p><p>（1）硬件：HackRF One -- 带 TCXO 时钟模块和天线，用于发射GPS信号 (2)软件： | 软件 | 作用 | | ---- | ---- | | Google Earth |选中欺骗地点，勾画目标轨迹 | | SatGen | 目标轨迹并存储为运动路径 | |gps-sdr-sim | 采样数据文件，生成GPS数据源 | | Gnuradio |流程图式运行GPS欺骗的程序 | | hackrf-tools |通过hackrf_transfer函数，在命令行运行GPS欺骗 |</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Experience/gps%20mind.png" /></p><h3 id="成绩-9">12.2. <a name='-1'></a>成绩：</h3><p>实际手机位于广州大学城生活区某一定点静止不动，将定位欺骗至1千公里外的上海交大的操场跑道上变速跑步,全程精确度5m以内。</p><p><iframe height=498 width=510 src='https://player.youku.com/embed/XNDg0MTAwNDUyOA==' frameborder=0 allowfullscreen></iframe></p><p>2020.11 - 2021.1</p><hr /><h3 id="描述-10">12.3. <a name='-1'></a>描述：</h3><p>根据玩具电子狗，通过其电路图进行相应修改，可得到不同开关相应方式，除了下面视频外也已经实现磁控、小程序控制、蓝牙控制等方式</p><p>用到的硬件模块有：</p><ol type="1"><li>玩具电子狗 -- 具备基本行走，吠叫功能</li><li>电路板 -- 实现不同方式开关并焊接电路 （3) 蓝牙开关模块 --具有微信小程序控制系统</li></ol><h3 id="成绩-10">12.4. <a name='-1'></a>成绩：</h3><p><strong>键控开关方式</strong></p><iframe height="498" width="510" src="https://player.youku.com/embed/XNDg5NzI5MDU0MA==" frameborder="0" allowfullscreen></iframe><p><strong>温控开关方式</strong></p><iframe height="498" width="510" src="https://player.youku.com/embed/XNDg5NzI5MDcyMA==" frameborder="0" allowfullscreen></iframe><h2 id="基于arduino的音乐播放器">13.<a name='Arduino'></a>基于Arduino的音乐播放器</h2><p>2020.4 - 2020.6</p><hr /><h3 id="描述-11">13.1. <a name='-1'></a>描述：</h3><p>通过手机(串口)或电脑输入控制，实现了MP3的基本功能（曲目切换，多种播放模式，音量调节）。</p><p>用到的硬件模块有：</p><ol type="1"><li>Arduino -- 中央处理器</li><li>tf卡 -- 存储曲目 （3) 扬声器 -- 播放声音</li><li>LCD屏幕 -- 显示播放模式、曲目</li></ol><h3 id="成绩-11">13.2. <a name='-1'></a>成绩：</h3><iframe height="498" width="510" src="https://player.youku.com/embed/XNDg0MTAzMDQwOA==" frameborder="0" allowfullscreen></iframe>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;我的项目经历&quot;&gt;我的项目经历&lt;/h1&gt;
&lt;p&gt;目录：&lt;/p&gt;
&lt;!-- vscode-markdown-toc --&gt;
&lt;p&gt;&lt;strong&gt;一、软件编程项目&lt;/strong&gt; * 1. &lt;a
href=&quot;#&quot;&gt;无人机集群电磁仿真设计&lt;/a&gt; * 2. &lt;</summary>
      
    
    
    
    <category term="总结" scheme="https://serika-onoe.github.io/categories/%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="科研" scheme="https://serika-onoe.github.io/tags/%E7%A7%91%E7%A0%94/"/>
    
    <category term="项目" scheme="https://serika-onoe.github.io/tags/%E9%A1%B9%E7%9B%AE/"/>
    
  </entry>
  
  <entry>
    <title>Python爬虫爬取国家统计局数据</title>
    <link href="https://serika-onoe.github.io/2021/01/13/Python%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E5%9B%BD%E5%AE%B6%E7%BB%9F%E8%AE%A1%E5%B1%80%E6%95%B0%E6%8D%AE/"/>
    <id>https://serika-onoe.github.io/2021/01/13/Python%E7%88%AC%E8%99%AB%E7%88%AC%E5%8F%96%E5%9B%BD%E5%AE%B6%E7%BB%9F%E8%AE%A1%E5%B1%80%E6%95%B0%E6%8D%AE/</id>
    <published>2021-01-12T16:12:47.000Z</published>
    <updated>2022-12-12T14:35:22.494Z</updated>
    
    <content type="html"><![CDATA[<h1 id="python-爬虫爬取国家统计局数据">Python爬虫爬取国家统计局数据</h1><p><strong>本次实验以爬取“<ahref="https://data.stats.gov.cn/index.htm">国家统计局</a>”首页中的【上海市城乡居民收支基本情况】为例，国家统计局其他页面的爬取方法大同小异</strong></p><h2 id="爬虫基本流程">1.爬虫基本流程</h2><ol type="1"><li>发起请求：通过http/https库向目标站点发起请求，即发送一个request，请求可以包含额外的headers等信息，等待服务器响应</li><li>获取相应内容：如果服务器能正常响应，会得到一个response，response的内容便是所要获取的页面内容，类型可能有HTML，json字符串，二进制数据（如图片视频）等类型</li><li>解析内容：得到的内容可能是HTML，可以用正则表达式，网页解析库进行解析，可能是json，可以直接转为json对象，可能是二进制数据，可以做保存或者进一步的处理<strong>（本次实验得到的解析内容是json）</strong></li><li>保存数据：可以存为文本，也可以保存至数据库，或者特定格式的文件</li></ol><h2 id="打开网页并分析">2.打开网页并分析</h2><p>国家统计局的网站很奇怪，明明是https却会告警不安全，首次打开界面如下（本人使用的是谷歌浏览器）</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/1.jpg" /></p><p>点击“高级”-“继续前往”，方可进入首页</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/2.jpg" /></p><p>选择“季度数据”-“分省季度数据”</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/3.jpg" /></p><p>选择“人民生活”-“城乡收支情况”</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/4.jpg" /></p><p>地区修改为“上海市”</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/5.jpg" /></p><p>按下F12，进入浏览器调试模式</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/6.jpg" /></p><p>刷新重新获取网页信息，找到easyquery.htm?m=QueryData&amp;dbc...的文件。可以先选中"XHR"过滤条件，缩小查找范围</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/7.jpg" /></p><p>怎么确认这个文件就包含有我们要找的数据呢？点击“response”板块，向右拖动滑块可以看到表格数据可以一一对应（但数据并没有连续出现）</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/8.jpg" /></p><p><strong>注意：这里的data和strdata看上去一样，但实际格式不一样，data是int或double格式，strdata是str格式，这个表格有一些空数据行，字符串格式方便做判断，字符串转数字使用eval()即可</strong></p><h2 id="完整代码及解析">3.完整代码及解析</h2><p><strong>注：缺少的库可以在命令行使用pip命令安装，如缺少requests库，可以在命令行输入命令</strong></p><p><code>pip install requests</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib3 </span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用urllib3.disable_warnings()在关闭SSL认证（verify=False）情况下</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将requests请求禁用安全请求警告</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests    <span class="comment"># 使用Requests发送网络请求</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> time      <span class="comment"># 用来获取时间戳(计算当前时间，用于网页验证)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> json      <span class="comment"># 处理json文件</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  <span class="comment"># 处理数组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd  <span class="comment"># np.array()转换成pd.DataFrame格式，再使用to_excel()写入excel表格</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取毫秒级时间戳，用于网页验证</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTime</span>():</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="built_in">int</span>(<span class="built_in">round</span>(time.time() * <span class="number">1000</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据预处理，获取json列表中层层包裹的strdata元素（数据）</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getList</span>(<span class="params">length</span>):</span><br><span class="line"></span><br><span class="line">  <span class="type">List</span>=[]</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(length):</span><br><span class="line"></span><br><span class="line">​    temp = js[<span class="string">&#x27;returndata&#x27;</span>][<span class="string">&#x27;datanodes&#x27;</span>][i][<span class="string">&#x27;data&#x27;</span>][<span class="string">&#x27;strdata&#x27;</span>]</span><br><span class="line"></span><br><span class="line">​    <span class="comment"># 城乡居民收支列表中，原网站有同比增长数据为空，若直接使用eval()会报错，需要先判断</span></span><br><span class="line"></span><br><span class="line">​    <span class="keyword">if</span>(<span class="built_in">len</span>(temp)!=<span class="number">0</span>):</span><br><span class="line"></span><br><span class="line">​      <span class="comment"># eval()数字转字符串</span></span><br><span class="line"></span><br><span class="line">​      <span class="type">List</span>.append(<span class="built_in">eval</span>(temp))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="type">List</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 请求目标网址(链接?前面的东西)</span></span><br><span class="line"></span><br><span class="line">  url=<span class="string">&#x27;https://data.stats.gov.cn/easyquery.htm&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 请求头，User-Agent: 用来证明你是浏览器，满足一定格式即可，不一定和自己的浏览器一样</span></span><br><span class="line"></span><br><span class="line">  headers=&#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0(Windows;U;Windows NT6.1;en-US;rv:1.9.1.6) Geko/20091201 Firefox/3.5.6&#x27;</span>&#125;<span class="comment">#浏览器代理</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 构造参数键值对，具体数值从网页结构参数中获取</span></span><br><span class="line"></span><br><span class="line">  key=&#123;&#125;</span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;m&#x27;</span>]=<span class="string">&#x27;QueryData&#x27;</span></span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;dbcode&#x27;</span>]=<span class="string">&#x27;fsjd&#x27;</span></span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;rowcode&#x27;</span>]=<span class="string">&#x27;zb&#x27;</span></span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;colcode&#x27;</span>]=<span class="string">&#x27;sj&#x27;</span></span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;wds&#x27;</span>]=<span class="string">&#x27;[&#123;&quot;wdcode&quot;:&quot;reg&quot;,&quot;valuecode&quot;:&quot;310000&quot;&#125;]&#x27;</span></span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;k1&#x27;</span>]=<span class="built_in">str</span>(getTime()) </span><br><span class="line"></span><br><span class="line">  <span class="comment"># &quot;wdcode&quot;:&quot;reg&quot; 地区栏</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 上海 310000 </span></span><br><span class="line"></span><br><span class="line">  key[<span class="string">&#x27;dfwds&#x27;</span>]=<span class="string">&#x27;[&#123;&quot;wdcode&quot;:&quot;zb&quot;,&quot;valuecode&quot;:&quot;A0300&quot;&#125;,&#123;&quot;wdcode&quot;:&quot;sj&quot;,&quot;valuecode&quot;:&quot;LAST6&quot;&#125;]&#x27;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># &quot;wdcode&quot;:&quot;zb&quot; 选取左侧哪个条目,&quot;wdcode&quot;:&quot;sj&quot;选项框中选取&quot;最近6季度&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 禁用安全请求警告</span></span><br><span class="line"></span><br><span class="line">  requests.packages.urllib3.disable_warnings()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 发出请求，使用post方法，这里使用前面自定义的头部和参数</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># ！！！verify=False，国家统计局20年下半年改用https协议,若不加该代码无法通过SSL验证</span></span><br><span class="line"></span><br><span class="line">  r = requests.post(url, headers=headers, params=key,verify=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 使用json库中loads函数，将r.text字符串解析成dict字典格式存储于js中</span></span><br><span class="line"></span><br><span class="line">  js = json.loads(r.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># 得到所需数据的一维数组，利用np.array().reshape()整理为二维数组</span></span><br><span class="line"></span><br><span class="line">  length=<span class="built_in">len</span>(js[<span class="string">&#x27;returndata&#x27;</span>][<span class="string">&#x27;datanodes&#x27;</span>])</span><br><span class="line"></span><br><span class="line">  res=getList(length)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 总数据划分成6行的格式</span></span><br><span class="line"></span><br><span class="line">  array=np.array(res).reshape(<span class="built_in">len</span>(res)//<span class="number">6</span>,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment"># np.array()转换成pd.DataFrame格式，后续可使用to_excel()直接写入excel表格</span></span><br><span class="line"></span><br><span class="line">  df_shanghai=pd.DataFrame(array)</span><br><span class="line"></span><br><span class="line">  df_shanghai.columns=[<span class="string">&#x27;2020年第三季度&#x27;</span>,<span class="string">&#x27;2020年第二季度&#x27;</span>,<span class="string">&#x27;2020年第一季度&#x27;</span>,<span class="string">&#x27;2019年第四季度&#x27;</span>,</span><br><span class="line"></span><br><span class="line">​        <span class="string">&#x27;2019年第三季度&#x27;</span>,<span class="string">&#x27;2019年第二季度&#x27;</span>]</span><br><span class="line"></span><br><span class="line">  df_shanghai.index=[<span class="string">&#x27;居民人均可支配收入累计值(元)&#x27;</span>,</span><br><span class="line"></span><br><span class="line">​       <span class="string">&#x27;城镇居民人均可支配收入累计值(元)&#x27;</span>,</span><br><span class="line"></span><br><span class="line">​       <span class="string">&#x27;农村居民人均可支配收入累计值(元)&#x27;</span>,</span><br><span class="line"></span><br><span class="line">​       <span class="string">&#x27;居民人均消费支出累计值(元)&#x27;</span>,</span><br><span class="line"></span><br><span class="line">​        <span class="string">&#x27;城镇居民人均消费支出累计值(元)&#x27;</span>,</span><br><span class="line"></span><br><span class="line">​       <span class="string">&#x27;农村居民人均消费支出累计值(元)&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(df_shanghai)</span><br></pre></td></tr></table></figure><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/9.jpg" /></p><h2 id="部分代码说明">4.部分代码说明</h2><h3 id="数据提取">数据提取</h3><p>得到表格中的数据需要先分析提取到的js文件，打印内容如下：</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/10.jpg" /></p><p>将五层列表层层剥开，得到需要的strdata</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/11.jpg" /></p><h3 id="请求网站">请求网站</h3><p>请求目标网址(''?''前面的东西)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">url=<span class="string">&#x27;https://data.stats.gov.cn/easyquery.htm&#x27;</span></span><br></pre></td></tr></table></figure><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/12.jpg" /></p><p>请求头，User-Agent:用来证明你是浏览器，满足一定格式即可，不一定要和自己的浏览器一样</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">headers=&#123;<span class="string">&#x27;User-Agent&#x27;</span>:<span class="string">&#x27;Mozilla/5.0(Windows;U;Windows NT6.1;en-US;rv:1.9.1.6) Geko/20091201 Firefox/3.5.6&#x27;</span>&#125;<span class="comment">#浏览器代理</span></span><br></pre></td></tr></table></figure><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/13.jpg" /></p><p>构造参数键值对，下列参数会以 &amp; 连接，放在链接的''?''后面</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">key=&#123;&#125;</span><br><span class="line">key[<span class="string">&#x27;m&#x27;</span>]=<span class="string">&#x27;QueryData&#x27;</span></span><br><span class="line">key[<span class="string">&#x27;dbcode&#x27;</span>]=<span class="string">&#x27;fsjd&#x27;</span></span><br><span class="line">key[<span class="string">&#x27;rowcode&#x27;</span>]=<span class="string">&#x27;zb&#x27;</span></span><br><span class="line">key[<span class="string">&#x27;colcode&#x27;</span>]=<span class="string">&#x27;sj&#x27;</span></span><br><span class="line">key[<span class="string">&#x27;wds&#x27;</span>]=<span class="string">&#x27;[&#123;&quot;wdcode&quot;:&quot;reg&quot;,&quot;valuecode&quot;:&quot;310000&quot;&#125;]&#x27;</span></span><br><span class="line">key[<span class="string">&#x27;k1&#x27;</span>]=<span class="built_in">str</span>(getTime())  </span><br><span class="line">key[<span class="string">&#x27;dfwds&#x27;</span>]=<span class="string">&#x27;[&#123;&quot;wdcode&quot;:&quot;zb&quot;,&quot;valuecode&quot;:&quot;A0300&quot;&#125;,&#123;&quot;wdcode&quot;:&quot;sj&quot;,&quot;valuecode&quot;:&quot;LAST6&quot;&#125;]&#x27;</span></span><br></pre></td></tr></table></figure><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/14.jpg" /></p><p>部分参数可以从下图所示位置查看到，有些不显示的为默认，如果需要显示相同页面，需选取选项框中的相应选项</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/15.jpg" /></p><h2 id="数据保存到excel表格">5.数据保存到excel表格</h2><p>爬虫爬到的数据现以panda.dataframe格式存储，可以利用to_excel()函数，直接保存在excel表格中</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># write对象为该Excel工作簿，使用该方法保存多个工作表</span></span><br><span class="line">    write = pd.ExcelWriter(<span class="string">&#x27;F:/Ivory_Tower/norm/分省季度数据_城乡居民收支.xls&#x27;</span>) <span class="comment">#该路径自己设置即可，没有该文件的话会自行创建一个，存在的话写入会覆盖原内容</span></span><br><span class="line">    df_shanghai.to_excel(write,sheet_name=<span class="string">&#x27;上海&#x27;</span>)</span><br><span class="line">    <span class="comment">#如果爬多个省份的数据，可以写入多个工作表，且必须要加上save()保存</span></span><br><span class="line">    write.save()</span><br></pre></td></tr></table></figure><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/16.jpg" /></p><h2 id="表格优化可选">6.表格优化（可选）</h2><p>可以借助python代码，优化表格格式，如上图所示的结果不尽人意，至少还需要自动调整列宽。</p><p>这里本人采用xlwings库，需要先在命令行下载相应的库</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install xlwings</span><br><span class="line">pip install pywin32</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用xlwings库，利用python编辑整理Excel表格</span></span><br><span class="line"><span class="keyword">import</span> xlwings <span class="keyword">as</span> xw</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    app=xw.App(visible=<span class="literal">False</span>,add_book=<span class="literal">False</span>) <span class="comment">#过程不可见，不添加新工作表</span></span><br><span class="line">    wb=app.books.<span class="built_in">open</span>(<span class="string">r&#x27;F:/Ivory_Tower/norm/分省季度数据_城乡居民收支.xls&#x27;</span>)</span><br><span class="line">    <span class="comment"># wb就是新建的工作簿(workbook)</span></span><br><span class="line">    <span class="comment"># 对8个工作表，分别进行操作</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>): </span><br><span class="line">        rng=wb.sheets[i].<span class="built_in">range</span>(<span class="string">&#x27;A1:H20&#x27;</span>)      <span class="comment"># 选中这些单元格</span></span><br><span class="line">        rng.api.HorizontalAlignment = -<span class="number">4108</span>   <span class="comment"># 文字水平方向居中</span></span><br><span class="line">        rng.autofit()                         <span class="comment"># 自动调整行高列宽</span></span><br><span class="line">    wb.save()</span><br><span class="line">    wb.close()</span><br><span class="line">    app.quit()</span><br></pre></td></tr></table></figure><p>运行代码，即可得到以下效果（后续多爬了其他一些省份，在key处修改相应参数即可）</p><p><imgsrc="https://github.com/serika-onoe/web-img/raw/main/Python_crawler/17.jpg" /></p><h2 id="参考资料">7.参考资料</h2><p>史上超详细python爬取国家统计局数据：https://blog.csdn.net/qq_41988893/article/details/103017854</p><p>如果报其他各种各样莫名其妙的错，可以评论或私信询问哦~</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;python-爬虫爬取国家统计局数据&quot;&gt;Python
爬虫爬取国家统计局数据&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;本次实验以爬取“&lt;a
href=&quot;https://data.stats.gov.cn/index.htm&quot;&gt;国家统计局&lt;/a&gt;”首页中的【上海市城乡</summary>
      
    
    
    
    <category term="项目详情" scheme="https://serika-onoe.github.io/categories/%E9%A1%B9%E7%9B%AE%E8%AF%A6%E6%83%85/"/>
    
    
    <category term="Python" scheme="https://serika-onoe.github.io/tags/Python/"/>
    
    <category term="爬虫" scheme="https://serika-onoe.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>游戏内抽卡机制总结</title>
    <link href="https://serika-onoe.github.io/2020/09/06/%E6%B8%B8%E6%88%8F%E5%86%85%E6%8A%BD%E5%8D%A1%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93/"/>
    <id>https://serika-onoe.github.io/2020/09/06/%E6%B8%B8%E6%88%8F%E5%86%85%E6%8A%BD%E5%8D%A1%E6%9C%BA%E5%88%B6%E6%80%BB%E7%BB%93/</id>
    <published>2020-09-06T03:00:00.000Z</published>
    <updated>2022-12-11T03:55:10.746Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面">写在前面</h2><p>卡牌类游戏很好地搭载了二次元的核心元素——“角色”，抽卡活动对库存货币消耗和新增货币均有很大的正面作用，且参与了抽卡活动的玩家也占了充值用户相当高的比例。抽卡活动确实是当前非常重要的付费点</p><p>但本人深知“玄不改非，氪能改命”，而因为是轻度玩家并不想充钱变强，所以卡牌游戏玩的并不多，阴阳师、明日方舟、剑与远征等都略有接触，而每个游戏都有着属于自己的抽卡概率。</p><p>像阴阳师的SSR大体在1%左右，而剑与远征的概率在4.8%左右，但为了用户体验，最重要的是不能让时间玩家拉开RMB玩家的差距，不能违反大R碾压小R的原则，所以所谓随机基本都是伪随机。</p><p>纯干货，为了不影响阅读就不放图片了，下面介绍一些常见的抽卡机制。</p><h3 id="保底机制">①保底机制</h3><p>这是最简单，也最普遍的一种机制，如《王者荣耀》，购买次数到达361次时，荣耀水晶产出概率为100%。《剑与远征》两个保底机制，30抽必出紫卡，在同卡池内累计抽30次即可获得出一张紫卡英雄不论是单抽还是连抽，只要数量达到即必出紫卡。</p><p>还有一个保底机制就是10连抽必出一个稀有或者精英级别的英雄，和30抽不一样的地方在于只能是适用于十连抽而不能适用于十次单抽。</p><p>保底机制保证了玩家的最终体验</p><h3 id="玄学抽奖法">②玄学抽奖法</h3><p>在一些抽卡游戏里是用一定作用的，可能由于游戏开发者在写抽卡的程序时，有时候会引用其他数据，然后增加一定算法，来决定抽到哪一张卡，这就是玩家玄学的由来。</p><p>如果引用的数据是当前系统时间，那么有可能出现“凌晨某个时间点抽卡中奖率高，或者每小时的前十分钟中奖率高”</p><p>虽说结果都在你抽卡的那一刻，在服务器就决定好，这就与抽卡画出什么图案、使用哪种方法无关，但游戏厂商还是乐意留下一个玩家主导的过程，让玩家相信是抽卡过程影响抽卡结果，对抽卡这一过程充满仪式感。</p><h3id="概率递增不知道业内是不是叫水位">③概率递增（不知道业内是不是叫水位）</h3><p>概率递增法，是指抽卡时，抽卡次数越多，爆率越高的抽卡方法。如果在还没累积到这个数值前已经抽到，那么就将概率归零。</p><p>可以让玩家的游戏体验保持在一个比较均衡的位置。</p><h3 id="奖池划分">④奖池划分</h3><p>这种抽卡方法比较复杂，在一些频繁出新卡的游戏里比较多。</p><p>当玩家抽取时候，会先判定玩家进入哪个奖池（R,SR,SSR），然后再判定玩家在这个奖池里抽到哪一张卡。如果官方加入一张新卡，会单出一个奖池，暗中去掉一张旧卡，玩家不会太过关注旧卡的出卡率，也乐意多抽出新卡。</p><h3 id="剧本抽卡">⑤剧本抽卡</h3><p>《空当接龙》所有的牌组都已经写好，每次开始游戏，就从牌组剧本中挑选一个。</p><p>《斗地主》游戏官方会特意编写出多连对，多飞机，多炸弹的牌组，随机发牌很可能出现散牌。</p><h3 id="氪金区分抽卡">⑥氪金区分抽卡</h3><p>原来的游戏是充值多少送一次抽奖，且一般都能得到非常珍贵的游戏道具。现在会暗中增加一个数据栏，计算玩家充值的数量，划分等级调整概率，来提高氪金玩家的游戏体验。</p><p>如果某种货币既可以从游戏内肝到，也可以选择充值得到，那么官方可以暗中设定一个状态栏，将活动肝到的和充值得到的区别开，每次抽奖，都会识别这次抽奖所使用的钻石是哪种类型的钻石。如果使用的过程中两种同时使用，可能默认都是充值得到的，这时概率会比用肝到的大。</p><h3 id="与抽卡促销的其他玩法">⑦与抽卡促销的其他玩法</h3><p>常见的表现方式是进入游戏，就给玩家一笔足够首抽的钱，引导玩家进行抽奖然后获得珍贵道具。</p><p>或者抽奖时，系统突然提醒你：你获得了一个购买稀有道具的机会，并附带增加时间限制。</p><p>还有一种方法是根据新玩家的道具需求，调整不同物品的爆率。比如收集套装正缺那一个部件，很可能抽奖的时候就爆出来。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;写在前面&quot;&gt;写在前面&lt;/h2&gt;
&lt;p&gt;卡牌类游戏很好地搭载了二次元的核心元素——“角色”，抽卡活动对库存货币消耗和新增货币均有很大的正面作用，且参与了抽卡活动的玩家也占了充值用户相当高的比例。抽卡活动确实是当前非常重要的付费点&lt;/p&gt;
&lt;p&gt;但本人深知“玄不改</summary>
      
    
    
    
    <category term="游戏" scheme="https://serika-onoe.github.io/categories/%E6%B8%B8%E6%88%8F/"/>
    
    
    <category term="二次元" scheme="https://serika-onoe.github.io/tags/%E4%BA%8C%E6%AC%A1%E5%85%83/"/>
    
    <category term="抽卡" scheme="https://serika-onoe.github.io/tags/%E6%8A%BD%E5%8D%A1/"/>
    
    <category term="游戏" scheme="https://serika-onoe.github.io/tags/%E6%B8%B8%E6%88%8F/"/>
    
  </entry>
  
  <entry>
    <title>我的第一篇博客</title>
    <link href="https://serika-onoe.github.io/2020/02/11/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
    <id>https://serika-onoe.github.io/2020/02/11/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</id>
    <published>2020-02-11T04:14:00.000Z</published>
    <updated>2022-12-12T14:40:38.284Z</updated>
    
    <content type="html"><![CDATA[<h1 id="为什么要写博客">为什么要写博客</h1><p>记录博客是酝酿很久的想法，相信多数人作出这一决定也都经历了较长时间的拖延症hh。不过除了习惯性偷懒，也有出于对自身技术水平不自信的考量，毕竟大牛是极少数，多数人不过是在平均线上徘徊。</p><p>不过大脑做决定并不是纯粹理性的，反而主要凭感性。立下了靶子，定下来方向，理性思维才会积极地把行为合理化。</p><p>对为什么突发奇想开始记录博客，个人总结了如下动机：</p><ul><li><p>无论课内外，本人都已养成了动笔前先查阅大量资料的习惯。心里对那些具有开源精神的大牛们、前辈们充满敬意和感激。很多时候，一个简洁清晰的结论、一行高度概括的代码，单靠自己的探索往往要事倍功半，甚至还可能因为其在知识盲区(UnknownUnknown)而作不必要的苦恼，被前辈们留下的博客文章中不经意地一语道破，这样的瞬间简直不要太多。</p></li><li><p>从一个纯小白进化到现在一个在很多领域都有些入门经验的....小白来说，也很希望把当时掉进去的坑补上，最起码在前面做个警示，新人在环境搭建阶段没必要走弯路，把重心放在解决需求的程序调试阶段，实现更高的自我提升效率。</p></li><li><p>俗话说得好：“好记性不如烂笔头。”之前看过一本讲如何高效记笔记的书，但纸面的笔记也常常无法翻阅。加上现在经常用手机浏览很多碎片化的知识点，得不到有效的整理，博客的存在比起私人笔记，也有种民主监督的意味在里头，避免个人认知偏差和局限。</p></li><li><p>还有《暗时间》，让我受益匪浅，学习的时候我也会经常想象如何把知识向一个小白讲解，而博客也相当于把这个过程实例化，可视化。</p></li></ul><h1 id="博客记录什么">博客记录什么</h1><p>大学期间，课内学的很多是原理层面的东西，课外兴趣广泛，为避免犯蜻蜓点水般浅尝辄止的毛病，我总结所学以下几个方面的知识技术，抽空进行记录：</p><ul><li>编程语言类：C、Python、JAVA等</li><li>软件安装类: Android Studio，WordPress等</li><li>音频编辑类：pr，ps，au等等</li></ul><p>博客更新频率尽量保持在一周一两次，在此先作个纪念，日后若需要再加更改。</p><p>有诗云:“青山一道同云雨，明月何曾是两乡。”</p><p>愿与诸君共勉。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;为什么要写博客&quot;&gt;为什么要写博客&lt;/h1&gt;
&lt;p&gt;记录博客是酝酿很久的想法，相信多数人作出这一决定也都经历了较长时间的拖延症hh。不过除了习惯性偷懒，也有出于对自身技术水平不自信的考量，毕竟大牛是极少数，多数人不过是在平均线上徘徊。&lt;/p&gt;
&lt;p&gt;不过大脑做决</summary>
      
    
    
    
    <category term="总结" scheme="https://serika-onoe.github.io/categories/%E6%80%BB%E7%BB%93/"/>
    
    
    <category term="介绍" scheme="https://serika-onoe.github.io/tags/%E4%BB%8B%E7%BB%8D/"/>
    
  </entry>
  
</feed>
