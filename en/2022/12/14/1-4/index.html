<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Kung's Blog | Kung's Blog</title><meta name="author" content="Richard KUNG"><meta name="copyright" content="Richard KUNG"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Introduction to Transformer Tutorial Series The development of large models is gradually evolving from single-modal data input to multi-modal data input. The joint training and learning of text, v">
<meta property="og:type" content="article">
<meta property="og:title" content="Kung&#39;s Blog">
<meta property="og:url" content="https://serika-onoe.github.io/2022/12/14/1-4/index.html">
<meta property="og:site_name" content="Kung&#39;s Blog">
<meta property="og:description" content="Introduction to Transformer Tutorial Series The development of large models is gradually evolving from single-modal data input to multi-modal data input. The joint training and learning of text, v">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg">
<meta property="article:published_time" content="2022-12-14T01:42:13.000Z">
<meta property="article:modified_time" content="2023-02-23T07:39:41.217Z">
<meta property="article:author" content="Richard KUNG">
<meta property="article:tag" content="research">
<meta property="article:tag" content="transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://serika-onoe.github.io/2022/12/14/1-4/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"W5J7GO3HT8","apiKey":"53b7d5d068605c78d1a20d7f3671629a","indexName":"test_serika","hits":{"per_page":3},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Kung\'s Blog',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-02-23 15:39:41'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/CodeByZach/pace/themes/green/pace-theme-flash.css"><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Kung's Blog" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page" href="/../"><i class="fa-fw fas fa-language"></i><span> 中文</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Kung's Blog</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Links</span></a></div><div class="menus_item"><a class="site-page" href="/../"><i class="fa-fw fas fa-language"></i><span> 中文</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">No title</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-12-14T01:42:13.000Z" title="Created 2022-12-14 09:42:13">2022-12-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-02-23T07:39:41.217Z" title="Updated 2023-02-23 15:39:41">2023-02-23</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/notes-tutorials/">notes, tutorials</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="introduction-to-transformer-tutorial-series">Introduction to
Transformer Tutorial Series</h1>
<p>The development of large models is gradually evolving from
single-modal data input to multi-modal data input. The joint training
and learning of text, voice, image, video and other modalities will
effectively complement each other, which will help to improve the effect
and generalization ability of the model and lay a more solid step
towards general artificial intelligence. When it comes to the
multi-modal algorithm model, we have to mention the famous
Transformer.</p>
<p>In 2017, ** Google Research Team ** a paper ** 《Attention Is All You
Need》 ** was published, and Transformer was born, and soon became a **
Natural Language Processing (NLP) ** benchmark model in the field, such
as machine translation and subsequent NLP models such as BERT and GPT
series. Transformer has also been applied to the ** Computer Vision (CV)
** field of image classification, object detection, image generation and
video processing tasks, such as DERT, ViT and so on. Transforme has also
been used ** Speech Domain (ASR) ** for tasks such as speech
recognition, speech synthesis, speech enhancement, and music generation.
In addition, the scenes composed ** Multimodality ** of NLP, vision and
speech are also the hot directions of Transformer applications in recent
years, such as visual question answering, visual common sense reasoning,
speech-to-text translation and text-to-image generation.</p>
<p>Readers who ** This article focuses on the two technical foundations
before Transformer: the Encoder-Decoder framework and the Attention
mechanism. ** have mastered the basics of Encoder-Decoder and Attention
can read the subsequent chapters directly.</p>
<blockquote>
<p>There are four articles in the Transformer series of this blog, and
the navigation is as follows:</p>
</blockquote>
<ul>
<li><p>[Transformers for Beginners: Encoder-Decoder Architecture
Demystified (1/4) ** This article **]
(https://serika-onoe.github.io/2022/12/14/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%88%E5%89%8D%E8%A8%80%EF%BC%89/)</p></li>
<li><p>[Transformers for Beginners: Peeling Back the Layers of Inner
Workings (2/4)]
(https://serika-onoe.github.io/2023/01/11/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%881%EF%BC%89/)</p></li>
<li><p>[Transformers for Beginners: A Step-by-Step PyTorch
Implementation Guide (3/4)]
(https://serika-onoe.github.io/2023/01/31/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%882%EF%BC%89/)</p></li>
<li><p>[Transformers for Beginners: The Ultimate Code Challenge (4/4)]
(https://serika-onoe.github.io/2023/02/05/Transformer%20%E9%9B%B6%E5%9F%BA%E7%A1%80%E8%A7%A3%E6%9E%90%E6%95%99%E7%A8%8B%EF%BC%883%EF%BC%89/)</p></li>
</ul>
<h1 id="foreword">Foreword</h1>
<blockquote>
<p>We propose a new simple network architecture, the ** Transformer
<strong>, based solely on </strong> attention mechanisms **, dispensing
with recurrence and convolutions entirely.</p>
<ul>
<li>Reference: Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I. (2017). *<br></li>
<li>Attention is all you need. *</li>
</ul>
</blockquote>
<p>Combined with the initial definition of the original paper, we can
simply divide the relationship among Encoder-Decoder, Attention and
Transformer:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;">Concept</th>
<th style="text-align: center;">说明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">Encoder-Decoder</td>
<td style="text-align: center;">A Framework for Solving
Sequence-Sequence Problems</td>
</tr>
<tr class="even">
<td style="text-align: center;">Attention</td>
<td style="text-align: center;">Mechanism to improve Encoder-Decoder
output results</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;">Transformer</td>
</tr>
</tbody>
</table>
<h1 id="encoder-decoder-framework">Encoder-Decoder framework</h1>
<h2 id="introduction">Introduction</h2>
<p>Typical problems in the field of natural language processing (NLP)
can be reduced to processing a sentence pair &lt;Source,Target&gt;, such
as:</p>
<ul>
<li>Text summary, Source is an article, Target is a general description
of a few sentences</li>
<li>For text translation, Source is the sentence to be translated, and
Target is the translated sentence</li>
<li>Question answering system, Source is a question, Target is an
answer</li>
</ul>
<p>Our goal is to give the input sentence <code>Source</code>, through
the Encoder-Decoder framework, and finally generate the target sentence
<code>Target</code>. Source and Target are made up of their own
sequences of words. Among them, Encoder-Decoder is a framework for
dealing with the sequence-sequence problem. The encoder (
<code>Encoder</code>) inputs a sequence and outputs an encoding, which
the decoder ( <code>Decoder</code>) uses to generate an output
sequence.</p>
<p>The Encoder-Decoder framework is not only widely used in the field of
text, but also in speech recognition, image processing and other fields.
The difference is that the RNN model is usually used in the Encoder part
of text processing and speech recognition, and the CNN model is usually
used in the Encoder of image processing. By way of example of a pair of
&lt;Source,Target&gt; sequence:</p>
<ul>
<li>For speech recognition, Source is a voice stream, and Target is the
corresponding text information.</li>
<li>Image description, Source is an image, and Target is the description
of the image content.</li>
</ul>
<h2 id="principle">Principle</h2>
<p>As a more specific example, we take the translation of a given input
sentence $X $as an example, through the Encoder-Decoder framework, and
finally generate the target sentence $Y $. Where $X $and $Y $are
sequences of words, respectively</p>
<ul>
<li>The original sentence <span class="math inline">\(X = (x_1, x_2,
\cdots, x_m)\)</span></li>
<li>Target sentence of translation</li>
</ul>
<p>The Encoder's task is to encode the input sentence $X $and transform
the input sentence into an intermediate semantic representation $C
$through a nonlinear transformation:</p>
<p><span class="math display">\[
C = F(x_1, x_2, \cdots, x_m)
\]</span></p>
<p>The Decoder task is to generate the word $y _ I $to be generated at
time I based on the intermediate semantic representation $C $of the
sentence $X $and the previously generated history information $y _ 1, y
_ 2,  cdots, y _ { i-1 } $.</p>
<p><span class="math display">\[y_i = G(C, y_1, y_2, \cdots, y_{i-1})
\]</span></p>
<p>Each $y _ I $is generated in turn, and eventually it looks like the
entire system generates the target sentence $Y $based on the input
sentence $X $.</p>
<p>Encoder-Decoder is a general computing framework. You can choose what
model Encoder and Decoder use. <code>(因此这可以是创新点)</code></p>
<p><img
src="https://github.com/serika-onoe/web-img/raw/main/Transformer/encoder-decoder.jpg"
title="Encoder-Decoder架构图" /></p>
<center>
<code>Figure 1: Encoder-Decoder architecture diagram</code>
</center>
<h2 id="classical-decoder-formalism-and-its-problems">Classical Decoder
Formalism and Its Problems</h2>
The classical Decoder has two forms. Corresponding to two papers: **
[Paper 1] ** <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.1078">Kyunghyun Cho,
Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,
Holger Schwenk, Yoshua Bengio: “Learning Phrase Representations using
RNN Encoder-Decoder for Statistical Machine Translation”, 2014;
[http://arxiv.org/abs/1406.1078 arXiv:1406.1078].</a> <img
src="https://img-blog.csdnimg.cn/20200322154916564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" />
<center>
`Figure 2: Semantic encoding of the input sequence C equally affects the
output sequence'
</center>
<p><br> Paper 1 points out that both Ecoder and Decoder use RNN. Because
the semantic code ${ C } $contains the information of the whole input
sequence, the semantic code ${ C } $should be input when calculating the
output $y _ t $at each time, that is, the input information ${ C] $is
introduced at each step of decoding. The following is expressed by the
formula:</p>
<ul>
<li>$H _ { <t> } $of the internal state at time $t $in the Decoder
is:</li>
</ul>
<p><span
class="math display">\[h_{&lt;t&gt;}=f(h_{t-1},y_{t-1},C)\]</span></p>
<ul>
<li>At <span class="math inline">\(t\)</span> moment, the output
probability is</li>
</ul>
<p><span
class="math display">\[p(y_t|y_{t-1},y_{t-2},\cdots,y_{1},C)=g(h_{&lt;t&gt;},y_{t−1},C)\]</span></p>
<p><span class="math inline">\(h_t\)</span> Is the value of the hidden
layer at the current time $t $, and $y _ { T-1 } $is the predicted
output at the previous time.</p>
** [Paper 2] **: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.3215">Ilya
Sutskever, Oriol Vinyals, Quoc V. Le: “Sequence to Sequence Learning
with Neural Networks”, 2014; [http://arxiv.org/abs/1409.3215
arXiv:1409.3215].</a> <img
src="https://img-blog.csdnimg.cn/20200322162508369.png" />
<center>
<code>Figure 3: Only introduce semantic encoding C at the initial input of the Decoder</code>
</center>
<p><br/><br/> The method of Paper 2 is relatively simple. It only
introduces the semantic code ${ C } $at the initial input of the
Decoder, and takes the semantic code ${ C } $as the initial value of the
hidden layer state value $H _ { &lt;0&gt; } $. The formula is as
follows:</p>
<p><span class="math display">\[h_{&lt;0&gt;}=C\]</span></p>
<p><span
class="math display">\[h_{&lt;t&gt;}=f(h_{&lt;t-1&gt;},y_{t-1})\]</span></p>
<p><span
class="math display">\[p(y_t)=g(h_{&lt;t&gt;},y_{t−1})\]</span></p>
<p>These two Encoder-Decoder structures have at least the following
problems:</p>
<ol type="1">
<li><p>If it is decoded according to Paper 1, it means that any word in
the input sequence $X $has the same influence on the generation of a
target word $y _ I $ (in fact, if the Encoder is RNN, in theory, the
later the word is input, the greater the influence is, which is not
equally weighted, so in this case, sometimes the accuracy of reverse
input may be higher).</p></li>
<li><p>If decoded according to paper 2, it means that all the
information of the input sequence is compressed in a semantic code $C $.
If the input sequence is so short that $C $contains insufficient
information, the resulting output sequence may not be accurate enough.
If the input sequence is very long, Decoder may lose important
information when generating the output sequence, and the gradient may
disappear.</p></li>
</ol>
<h1 id="simple-example-of-a-problem"># Simple example of a problem</h1>
<p>In contrast to the Attention model, the original Encoder-Decoder
model can be seen as a model of inattention. Why is it not focused?
Please observe the generation process of each word in the Target
sentence as follows:</p>
<p><span class="math display">\[y_i = G(C, y_1, y_2, \cdots,
y_{i-1})\]</span> Substitute the specific value, that is, <span
class="math display">\[ y_1 = G(C)\]</span> <span
class="math display">\[ y_2 = G(C, y_1)\]</span> <span
class="math display">\[ y_3 = G(C, y_1, y_2)
\]</span></p>
<p>Where $f $is Decoder's nonlinear transformation function. It can be
seen from here that when generating the words of the target sentence, no
matter which word is generated, the semantic encoding $C $of the input
sentence Source used by them is the same, and there is no
difference.</p>
<p>The Encoder-Decoder framework that uses machine translation to
explain this model is easier to understand. For example, the input is an
English sentence: 'Tom chase Jerry'. The Encoder-Decoder framework
gradually generates Chinese words: "Tom", "chase", "Jerry".</p>
<p>When translating the Chinese word "Jerry", each English word in the
model has the same contribution to the target word "Jerry". Obviously,
this is not reasonable. Obviously, "Jerry" is more important to
translate into "Jerry," but the model does not reflect this.</p>
<p>Since only one $C $can't be introduced, let's use multiple $C $and
let different semantic encodings $C _ I $correspond to different words
of the target sentence. At this point, we introduced the'Attention
'mechanism to improve the output of Encoder-Decoder.</p>
<h1 id="attention-mechanism">Attention mechanism</h1>
<h1 id="introduction-1"># Introduction</h1>
<p>To summarize, in the Encoder-Decoder model of text translation, the
encoder outputs a sequence of vectors, and the decoder needs to generate
words in the target language step by step. At each step, the decoder
needs to predict the next word based on the previously generated word
and the output of the encoder, and the decoder lacks a focus on the
output of the encoder. In order for the decoder to make better use of
the encoder's output, we can use the Attention mechanism.</p>
<p>The Attention mechanism allows the decoder to focus on certain parts
of the encoder's output at each step, so that the decoder can more
accurately generate words in the target language. The encoder can also
use the Attention mechanism. In each layer of the encoder, it uses
self-attention mechanisms to pay attention to different parts of the
input sequence.</p>
<h1 id="principle-1"># Principle</h1>
<img
src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" />
<center>
<code>Figure 4: Encoder-Decoder framework with Attention mechanism</code>
</center>
<p><br/><br/> The figure above is the Encoder-Decoder framework that
introduces the Attention mechanism. We can see at a glance that it no
longer has a single semantic encoding $C $, but has multiple encodings
such as $C _ 1, C _ 2, and C _ 3 $. When we predict $Y _ 1 $, maybe the
attention of $Y _ 1 $is focused on $C _ 1 $, so we use $C _ 1 as the
semantic code. When we predict $Y _ 2 $, the attention of $Y-2 $is
focused on $C-2 $, so we use $C-2 as the semantic code, and so on.</p>
<h1 id="question-1-how-should-attention-be-allocated"># # Question 1:
How should attention be allocated?</h1>
<blockquote>
<p>How do you tell where you should focus on the input sequence each
time you decode? How to calculate $C1, C2.. Cn $?</p>
</blockquote>
<p>We can assume that each word in the input sequence should have
different * * attention * * to the translation of "Jerry", such as when
translating "Jerry": <span class="math display">\[
(Tom, 0.3), \; (Chase, 0.2), \; (Jerry, 0.5)\]</span> The probability of
each English word represents the amount of attention allocated by the
attention allocation model to different English words when translating
the current word "Jerry". This is helpful to translate the target word
correctly. Because for each word of input, the different positions of
the output introduce new information and magnify the probability of
attention for the corresponding part of the input.</p>
<p>The premise of this is that each word in the target sentence should
learn to assign the attention probability information of each word in
the corresponding source sentence. That is, when each word $Y _ I $is
generated, the probability of the corresponding related word in the
source sentence is higher than that of the unrelated word.</p>
<blockquote>
<p>Personal understanding: Attention means that the model learns to pay
more attention to the word related to the input sequence when outputting
the next word, thus replacing the same intermediate semantic
representation $C $with $C _ I $that changes according to the current
generated word. (* * This is the key to the Attention mechanism * *)</p>
</blockquote>
<p>The specific implementation process is as follows:</p>
<p>Please observe the generation process of each word in the target
sentence Target. Compared with the original model, $C $becomes $C _ I
$:</p>
<p><span class="math display">\[ y_i = G(C_i, y_1, y_2, \cdots,
y_{i-1})\]</span> Substitute the specific value, that is, <span
class="math display">\[ y_1 = G(C_1)\]</span> <span
class="math display">\[y_2 = G(C_2, y_1)\]</span> <span
class="math display">\[y_3 = G(C_3, y_1, y_2)\]</span></p>
<p>Then for the previous example <code>“汤姆追逐杰瑞”</code>, $C _ I
$can be expressed as:</p>
<p><img
src="https://img-blog.csdnimg.cn/20200322173132643.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1RpbmsxOTk1,size_16,color_FFFFFF,t_70" /></p>
<ul>
<li><span class="math inline">\(f2\)</span> The function represents some
kind of transformation function of the Encoder to the input English
word. If the Encoder uses the RNN model, the result of this $F2
$function is often the state value of the hidden layer node after
inputting $X _ I $at a certain time.</li>
<li><span class="math inline">\(g\)</span> The function represents the
transformation function that the Encoder synthesizes the intermediate
semantic representation of the entire sentence according to the
intermediate representation of the word. In general, the $G $function is
a weighted summation of the constituent elements, that is, the following
formula: <span class="math display">\[
C_i = ∑_{j=1}^{Lx}a_{ij}h_j
\]</span><br />
</li>
<li><ul>
<li><p><span class="math inline">\(L_x\)</span> Represents the length of
the input sentence Source</p></li>
<li><p><span class="math inline">\(a_{ij}\)</span> Represents the
attention allocation coefficient of the jth word in the Source input
sentence when the ith word is output by the Target</p></li>
<li><p><span class="math inline">\(h_j\)</span> is the semantic encoding
of the jth word in the Source input sentence.</p>
<p>** Suppose $C _ I $refers to " $C _ { Tom } $" in the example above.
**</p></li>
<li><p><span class="math inline">\(L_x=3\)</span></p></li>
<li><p><span class="math inline">\(h_1=f2(&#39;Tom&#39;),
h_2=f2(&#39;Chase&#39;), h_3=f2(&#39;Jerry&#39;)\)</span></p></li>
<li><p><span class="math inline">\(a_{11}=0.6, a_{12}=0.2,
a_{13}=0.2\)</span></p></li>
</ul>
It can be seen that we calculate $C _ I $through a weighted summation
function, that is, the probability allocation of attention corresponding
to different decoding positions. In this example, this function is
written as $G $function, as shown in the following figure:</li>
</ul>
<img
src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention2.jpg" />
<center>
<code>Figure 5: G function process visualization</code>
</center>
<h3
id="question-2-on-the-calculation-of-the-probability-of-specific-attention">Question
2: On the calculation of the probability of specific attention</h3>
<blockquote>
<p>When generating a word in the target sentence, how do we know what
the calculated probability of attention for that word is? &gt; &gt; For
example, when generating the Chinese translation word "Tom", the model
generates the probability distribution of each word in the input
sentence Source corresponding to "Tom": $ $ (Tom, 0.6) (Chase, 0.2)
(Jerry, 0.2) $ $How is this distribution obtained?</p>
</blockquote>
<p>For the convenience of illustration, we assume that the initial
Encoder-Decoder framework is refined. Both Encoder and Decoder adopt the
RNN model, which is a common model configuration. The architecture
diagram is as follows.</p>
<img
src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention3.jpg" />
<center>
<code>Figure 6: Initial Encoder-Decoder Common Framework</code>
</center>
<p><br/><br/></p>
<p>For Decoder using RNN, at time $I $, if we want to generate $y _ I
$words, we can know the output value $H _ { i-1} $of the hidden node at
the time <span class="math inline">\(i-1\)</span> before Target
generates $y _ <span class="math inline">\(i-1\)</span> I $. Our goal is
to calculate the probability distribution of attention allocation to $y
_ I $for the words "Tom", "Chase" and "Jerry" in the input sentence when
$y _ I $is generated. Then the hidden node state $H _ { i-1} $at the
time of the Target output sentence <span
class="math inline">\(i-1\)</span> can be compared ** The alignment
likelihood corresponding to the target word $y _ I $and each input word
is obtained by the function $F (H _ J, H _ { i-1}) $ ** with the RNN
hidden node state $H _ J $corresponding to each word in the input
sentence Source, that is, (This $F $function may take different
approaches in different papers. ** Then the output of the function $F
$is normalized by Softmax to obtain the value of the probability
distribution of attention allocation that conforms to the value interval
of the probability distribution. **). For example
<code>“汤姆追逐杰瑞”</code>, an example can be expressed as follows.</p>
<img
src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention4.jpg" />
<center>
<code>Figure 7: Attention allocation coefficient calculation process for "Tom" in the "Tom Chasing Jerry" example</code>
</center>
<p><br/><br/> Note that what we are using here is ** Soft Attention **
that all the data will be noticed and the corresponding attention weight
will be calculated, and the filter condition will not be set. Another
way ** Hard Attention ** is to filter out some unqualified attention
after generating the attention weight, so that its attention weight is
0, which can be understood as no longer paying attention to these
unqualified parts.</p>
<h2 id="essence">Essence</h2>
<p>If the Attention mechanism is separated from the Encoder-Decoder
framework in the above example and further abstracted, it is easier to
understand the essence of the Attention mechanism.</p>
<p>** [Paper 3] **: [Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin:
“Attention Is All You Need”, 2017; [Http://arxiv.org/abs/1706.03762
arXiv: 1706.03762].] (https://arxiv.org/abs/1706.03762) This paper
discusses the use of Query, How the Key and Value vectors calculate
Attention.</p>
<blockquote>
<p>What do the Q, K, and V vectors represent, respectively, and why are
these three vectors used?</p>
</blockquote>
<p>Attention is simply a weighted average of some values, as mentioned
in the "Tom Chases Jerry" example:</p>
<p><span class="math display">\[
C_i = ∑_{j=1}^{Lx}a_{ij}h_j
\]</span></p>
<ul>
<li><span class="math inline">\(L_x\)</span> Represents the length of
the input sentence Source</li>
<li><span class="math inline">\(a_{ij}\)</span> Represents the attention
allocation coefficient of the jth word in the Source input sentence when
the ith word is output by the Target</li>
<li><span class="math inline">\(h_j\)</span> is the semantic encoding of
the jth word in the Source input sentence.</li>
</ul>
<p>The key idea of Transformer is the Attention mechanism. In contrast
to recurrent networks such as RNN/LSTM, which must look at each word one
by one (and thus have long-term memory problems), Transformer not only
considers input from one direction, but also sees all the tokens in the
input at the same time as a whole to determine what each word should
"notice". ** That is to say, we expect that after an input Source is
given, the output can see all the input information at the same time,
and then each output position chooses its own attention point as the
output according to different weights. **</p>
<p>This is similar to the basic principle of search engine or
recommendation system, take Google as an example:</p>
<ul>
<li>Query given by the user</li>
<li>There are various article titles (Key) and articles themselves
(Value) in the Google background.</li>
</ul>
<p>By defining and calculating the correlation between the Query and the
article title Key, the user can find the best matching article Value. Of
course, the goal of Attention is not to get Value through Query, but to
get the weighted sum of Value through Query. We may therefore consider
that,</p>
<img
src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention_bz1.jpg" />
<center>
<code>Figure 8: The essential idea of the Attention mechanism</code>
</center>
<p><br/><br/> So we can look at the Attention mechanism in this way: our
input and output are recorded as sentence pairs &lt;Source,Target&gt;.
Imagine that the elements in Source are composed of a series of
&lt;Key,Value&gt; data pairs. At this time, given an element Query in
Target, the weight coefficient of Value corresponding to each Key is
obtained by calculating the similarity or correlation between Query and
each Key, and then the Value is weighted and summed. The final Attention
value is obtained. Therefore ** Essentially, the Attention mechanism is
a weighted sum of the Value values of the elements in Source, while
Query and Key are used to calculate the weight coefficients of the
corresponding Values. **, the essential idea can be rewritten as the
following formula:</p>
<p><img src="https://img-blog.csdnimg.cn/20200322195829296.png" /></p>
<ul>
<li><span class="math inline">\(L_x=||Source||\)</span> Represents the
length of the Source</li>
</ul>
<p>Key and Value are strongly related, so they can also be the same
thing, such as Google search, which can theoretically search directly
for the article itself without listing the title of the article. In the
translation example of "Tom Chasing Jerry", Key and Value point to the
same thing: the semantic code $C _ I $for each word in the input
sentence.</p>
<p>Of course, it is still true to conceptually understand Attention as
selectively screening out a small amount of important information from a
large amount of information, focusing on these important information and
ignoring most of the unimportant information. The process of focusing is
reflected in the calculation of the weight coefficient. The larger the
weight is, the more focused it is on the corresponding Value value, that
is, the weight represents the importance of information, and Value is
the corresponding information.</p>
<p>As mentioned just now, "What Attention actually does is the retrieval
operation in the database", so we can also use addressing to simulate
the retrieval process: we regard the Attention mechanism as a kind of
Soft addressing: Source can be regarded as the content stored in the
memory. The element consists of the address Key and the value Value.
Currently, there is a query of $Key = Query $to retrieve the
corresponding Value value in the memory, that is, the Attention value.
The address is carried out through that similarity comparison of the
address of the Query and the address of the element Key in the memory.
The reason why the address is soft addressing is that the content can be
taken out from each Key address instead of finding out only one content
from the storage content like general addressing, and the importance of
the content taken out is determine according to the similarity between
the Query and the Key, Value is then weighted and summed so that the
final Value value, the Attention value, can be extracted.</p>
<p>It suddenly occurred to me that we can use everyday examples to make
an analogy, such as the decryption type of secret room escape game.
Players need to find clues or props in different places according to the
tips like detectives. Each clue may be related to the final escape
method (of course, the more clues or props they get in the later stage,
the higher the weight), and finally use reasoning reasonably to
escape.</p>
<h2 id="calculation-process">Calculation process</h2>
<p>If most of the current methods are abstracted, the specific
calculation process of Attention mechanism can be summarized into two
processes:</p>
<ol type="1">
<li>Calculate the weight coefficient based on Query and Key
<ul>
<li>Calculate the similarity or correlation between the two according to
Query and Key</li>
<li>Normalizing the original score of the similarity to obtain</li>
</ul></li>
<li>Value is weighted and summed according to the weight
coefficient.</li>
</ol>
<p>We can also break down the first process into two stages. Therefore,
the three stages are</p>
<ul>
<li>Stage 1: Query calculates the similarity with each Key to get the
similarity score $S1, S2, S3,  cdots, s _ I $</li>
<li>Stage 2: convert the scores of $S1, S2, S3,  cdots, s _ I $into a
probability distribution between [0,1] by softmax</li>
<li>Stage 3: Take $ [A1, A2, a3,  cdots, a _ n] $as the weight matrix to
perform weighted summation on the Value to obtain the final Attention
value.</li>
</ul>
<p>As shown in the figure below,</p>
<img
src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention_bz2.jpg" />
<center>
`Figure 9: Illustrating the computation of the Attention mechanism'
</center>
<p><br/><br/></p>
<h3 id="phase-1">Phase 1</h3>
<p>In Stage 1, different functions and calculation mechanisms can be
introduced to calculate the similarity or correlation between Query and
a $Key _ I $. The most common methods include: calculating the vector
dot product of the two, calculating the vector Cosine similarity of the
two, or introducing an additional neural network to evaluate:</p>
<p><img src="https://img-blog.csdnimg.cn/20200322200849586.png" /></p>
<h3 id="phase-2">Phase 2</h3>
<p>The value range of the result value of the score generated in the
stage 1 is different according to different specific generation methods.
Therefore, the calculation method similar to SoftMax is introduced in
the stage 2 to convert the value of the score in the first stage. On the
one hand, normalization can be carried out, and the original calculated
score can be arranged into a probability distribution in which the sum
of all element weights is 1; On the other hand, it can also highlight
the weight of important elements through the internal mechanism of
SoftMax. That is, the following formula is generally used for
calculation:</p>
<p><img src="https://img-blog.csdnimg.cn/20200322205913297.png" /></p>
<h3 id="phase-3">Phase 3</h3>
<p>The calculation result $a _ I $in Stage 2 is the weight coefficient
corresponding to $Value _ I $, and then the Attention value is obtained
by weighted summation in Stage 3:</p>
<p><img src="https://img-blog.csdnimg.cn/20200322210027426.png" /></p>
<p>Through the calculation of the above three stages, the Attention
value for Query can be obtained. At present, most of the specific
attention mechanism calculation methods are in line with the above
three-stage abstract calculation process.</p>
<h2 id="advantages-and-disadvantages">Advantages and disadvantages</h2>
<p>Attention is a common technique for solving long sequence processing
problems, but is it really invulnerable?</p>
<p>** Advantage **</p>
<ol type="1">
<li><p>** Computable in parallel: ** Attention mechanism solves the
problem that RNN can not compute in parallel. It needs to be explained
here that when we train the seq2seq model of the Attention mechanism,
the decoder does not predict a word and then use it as the next input,
but supervised training: we already have the target data, so it can be
input in parallel. Each output of the decoder can be calculated in
parallel, but there is no target data in the actual prediction. At this
time, the predicted value based on the previous time node needs to be
used as the input of the next time node decoder. So the time saved is
the training time.</p></li>
<li><p>** Improve long sequence processing capability: ** Attention
mechanism can help the model to choose the important parts when dealing
with long sequences, thus improving the accuracy of the model.</p></li>
<li><p>** Model weight visualization: ** Attention mechanism can get the
importance of each element in the input sequence to the output result by
calculating the weight coefficient. Visualizing the weight coefficient
is more helpful for us to understand what the model has learned than
previous methods.</p></li>
</ol>
<p>** Shortcoming **</p>
<p>1.** The Encoder section does not implement parallel operations: **
The Encoder part still uses RNN and LSTM, which are encoded in order,
and are not perfect.</p>
<p>2.** High computational complexity: ** Attention mechanism usually
needs to traverse the whole input sequence, which is computationally
expensive, especially when dealing with long sequences.</p>
<h3 id="improvements-self-attention">Improvements: Self Attention</h3>
<p>In order to improve the above two shortcomings, a more perfect
Self-Attention has emerged.</p>
<p>In the Encoder-Decoder framework for general tasks, the input Source
is different from the output Target. For example, for English-Chinese
machine translation, Source is an English sentence, and Target is the
corresponding translated Chinese sentence. The Attention mechanism
occurs between the element of the Target and all elements in the Source.
Self Attention, as its name implies, refers not to the Attention
mechanism between Target and Source, but to the Attention mechanism
between elements within Source or between elements within Target. It can
also be understood as the attention calculation mechanism in the special
case of $Target = Source $. The specific calculation process is the
same, except that the calculation object has changed, which is
equivalent to $Query = Key = Value $. The calculation process is the
same as that of attention, so the details of the calculation process
will not be repeated here.</p>
<img
src="https://github.com/serika-onoe/web-img/raw/main/Transformer/self_attention.jpg" />
<center>
"Figure 9: Self-attention can help the model understand the meaning of
pronouns"
</center>
<p><br/><br/> The figure above is an example of self-attention. We want
to know the its in this sentence, what it refers to in this sentence,
and which words are related to it, so we can use its as Query, and then
use this sentence as Key and Value to calculate the value of attention,
and find the word that is most related to its in this sentence. Through
self-attention, we find that its is most related to Law and application
in this sentence, and the meaning of the sentence is also very
consistent through our analysis.</p>
<p>After introducing Self Attention in this way, it will be easier to
capture the long-distance dependency in the sentence, because if it is
RNN or LSTM, it needs to be calculated in sequence. For the
long-distance interdependent features, it takes several time steps to
accumulate information to link the two, and the farther the distance is,
the less likely it is to capture effectively. However, Self Attention
will directly link any two words in the sentence through a calculation
step in the calculation process, so the distance between long-distance
dependency features is greatly shortened.</p>
<p>In addition, Self Attention has made other improvements, mainly as
follows:</p>
<ol type="1">
<li><p>The context is better understood by taking into account the
relationship between each element and the other elements in the input
sequence.</p></li>
<li><p>By adding multi-head mechanism to improve the generalization
ability of the model, the parallelism of Encoder calculation is
increased.</p></li>
<li><p>Position coding is added to ensure that the relative order of the
sequence elements has an effect on the result.</p></li>
</ol>
<p>In this section, I'll explain in detail how Self Attention works and
how it applies to Encoder in Transformer in the next article.</p>
<h1 id="concluding-remarks">Concluding remarks</h1>
<p>In this article, we have learned about the concepts of
Encoder-Decoder and Attention, which are useful in natural language
processing. However, we hope that these models can be used not only to
process natural language, but also in more areas. For example, my
ultimate goal is to use Transformer to predict time series data, so the
technical principle of Transformer will be the focus of my next article.
Let's wait and see how it expands on the basis of Encoder-decoder and
Attention!</p>
<h1 id="reference-link">Reference link</h1>
<h2 id="refer-to-the-website">Refer to the website</h2>
<ol type="1">
<li><p>Official Tensorflow tutorial: Neural machine translation with a
Transformer and Keras:</p>
<p><a
target="_blank" rel="noopener" href="https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb">https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb</a></p></li>
<li><p>Detailed explanation of the smallest white Attention in
history:</p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/Tink1995/article/details/105012972">https://blog.csdn.net/Tink1995/article/details/105012972</a></p></li>
<li><p>The Most Complete Transformer Interview Question Series in
History (1): Soul 20 Questions to Help You Get Transformer
Completely-Dry Goods! :</p>
<p><a
target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/148656446">https://zhuanlan.zhihu.com/p/148656446</a></p></li>
<li><p>Encoder-Decoder Models for Natural Language Processing</p>
<p><a
target="_blank" rel="noopener" href="https://www.baeldung.com/cs/nlp-encoder-decoder-models">https://www.baeldung.com/cs/nlp-encoder-decoder-models</a></p></li>
<li><p>ChatGPT3：</p>
<p><a
target="_blank" rel="noopener" href="https://chat.openai.com/chat">https://chat.openai.com/chat</a></p></li>
<li><p>Attention Model in Natural Language Processing: What and Why
[I]:</p>
<p><a
target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247484196&amp;idx=1&amp;sn=efa6b79d24b138ff79f33ec3426c79e2&amp;chksm=ebb43bf0dcc3b2e6e69ff3b7d7cabf28744e276ee08ccfcfc1dc2c3f0bf52a122c9bd77ff319&amp;scene=21#wechat_redirect">https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247484196&amp;idx=1&amp;sn=efa6b79d24b138ff79f33ec3426c79e2&amp;chksm=ebb43bf0dcc3b2e6e69ff3b7d7cabf28744e276ee08ccfcfc1dc2c3f0bf52a122c9bd77ff319&amp;scene=21#wechat_redirect</a></p></li>
<li><p>Query, Key and Value in Attention mechanism</p>
<p><a
target="_blank" rel="noopener" href="https://lih-verma.medium.com/query-key-and-value-in-attention-mechanism-3c3c6a2d4085">https://lih-verma.medium.com/query-key-and-value-in-attention-mechanism-3c3c6a2d4085</a></p></li>
<li><p>How to Understand Query, Key and Value in Transformer</p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/yangyehuisw/article/details/116207892">https://blog.csdn.net/yangyehuisw/article/details/116207892</a></p></li>
<li><p>Attention Mechanism in Deep Learning (2017 Edition)</p>
<p><a
target="_blank" rel="noopener" href="https://blog.csdn.net/malefactor/article/details/78767781">https://blog.csdn.net/malefactor/article/details/78767781</a></p></li>
<li><p>Transformer opens a door to general artificial intelligence?</p>
<p><a
target="_blank" rel="noopener" href="https://maimai.cn/article/detail?fid=1736575591&amp;efid=G3Qsw_4kvsfAXWUIsSeeig">https://maimai.cn/article/detail?fid=1736575591&amp;efid=G3Qsw_4kvsfAXWUIsSeeig</a></p></li>
</ol>
<h2 id="references">References</h2>
<p>** [Paper 1] **: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1406.1078">Kyunghyun
Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi
Bougares, Holger Schwenk, Yoshua Bengio: “Learning Phrase
Representations using RNN Encoder-Decoder for Statistical Machine
Translation”, 2014; [http://arxiv.org/abs/1406.1078
arXiv:1406.1078].</a></p>
<p>** [Paper 2] **: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1409.3215">Ilya
Sutskever, Oriol Vinyals, Quoc V. Le: “Sequence to Sequence Learning
with Neural Networks”, 2014; [http://arxiv.org/abs/1409.3215
arXiv:1409.3215].</a></p>
<p>** [Paper 3] **: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Ashish
Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
N. Gomez, Lukasz Kaiser, Illia Polosukhin: “Attention Is All You Need”,
2017; [http://arxiv.org/abs/1706.03762 arXiv:1706.03762].</a></p>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/research/">research</a><a class="post-meta__tags" href="/tags/transformer/">transformer</a></div><div class="post_share"><div class="social-share" data-image="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/12/14/Transformer%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%8E%A2/"><img class="prev-cover" src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Transformer模型初探</div></div></a></div><div class="next-post pull-right"><a href="/2022/12/14/Transformer%20Tutorial%20for%20Beginners%20A%20Comprehensive%20Guide/"><img class="next-cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info"></div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2022/12/14/Transformer%20Tutorial%20for%20Beginners%20A%20Comprehensive%20Guide/" title=""><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-14</div><div class="title"></div></div></a></div><div><a href="/2022/12/09/My%20projects/" title="My Projects"><img class="cover" src="https://github.com/serika-onoe/web-img/raw/main/Experience/uav3(1).png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-09</div><div class="title">My Projects</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Richard KUNG</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">9</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">19</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/serika-onoe" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:zgong313@connect.hkust-gz.edu.cn" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Best wishes for the New Year！</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#introduction-to-transformer-tutorial-series"><span class="toc-number">1.</span> <span class="toc-text">Introduction to
Transformer Tutorial Series</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#foreword"><span class="toc-number">2.</span> <span class="toc-text">Foreword</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#encoder-decoder-framework"><span class="toc-number">3.</span> <span class="toc-text">Encoder-Decoder framework</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#introduction"><span class="toc-number">3.1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#principle"><span class="toc-number">3.2.</span> <span class="toc-text">Principle</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#classical-decoder-formalism-and-its-problems"><span class="toc-number">3.3.</span> <span class="toc-text">Classical Decoder
Formalism and Its Problems</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#simple-example-of-a-problem"><span class="toc-number">4.</span> <span class="toc-text"># Simple example of a problem</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#attention-mechanism"><span class="toc-number">5.</span> <span class="toc-text">Attention mechanism</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#introduction-1"><span class="toc-number">6.</span> <span class="toc-text"># Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#principle-1"><span class="toc-number">7.</span> <span class="toc-text"># Principle</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#question-1-how-should-attention-be-allocated"><span class="toc-number">8.</span> <span class="toc-text"># # Question 1:
How should attention be allocated?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#question-2-on-the-calculation-of-the-probability-of-specific-attention"><span class="toc-number">8.0.1.</span> <span class="toc-text">Question
2: On the calculation of the probability of specific attention</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#essence"><span class="toc-number">8.1.</span> <span class="toc-text">Essence</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#calculation-process"><span class="toc-number">8.2.</span> <span class="toc-text">Calculation process</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#phase-1"><span class="toc-number">8.2.1.</span> <span class="toc-text">Phase 1</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#phase-2"><span class="toc-number">8.2.2.</span> <span class="toc-text">Phase 2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#phase-3"><span class="toc-number">8.2.3.</span> <span class="toc-text">Phase 3</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#advantages-and-disadvantages"><span class="toc-number">8.3.</span> <span class="toc-text">Advantages and disadvantages</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#improvements-self-attention"><span class="toc-number">8.3.1.</span> <span class="toc-text">Improvements: Self Attention</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#concluding-remarks"><span class="toc-number">9.</span> <span class="toc-text">Concluding remarks</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#reference-link"><span class="toc-number">10.</span> <span class="toc-text">Reference link</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#refer-to-the-website"><span class="toc-number">10.1.</span> <span class="toc-text">Refer to the website</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#references"><span class="toc-number">10.2.</span> <span class="toc-text">References</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/12/20/CS231n-Assignment-1/" title="CS231n Assignment 1 (Updating)"><img src="https://github.com/serika-onoe/web-img/raw/main/CS231N/Assignment1/cs231n_assignment1.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="CS231n Assignment 1 (Updating)"/></a><div class="content"><a class="title" href="/2022/12/20/CS231n-Assignment-1/" title="CS231n Assignment 1 (Updating)">CS231n Assignment 1 (Updating)</a><time datetime="2022-12-20T14:49:13.000Z" title="Created 2022-12-20 22:49:13">2022-12-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/14/Transformer%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%8E%A2/" title="Transformer模型初探"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Transformer模型初探"/></a><div class="content"><a class="title" href="/2022/12/14/Transformer%E6%A8%A1%E5%9E%8B%E5%88%9D%E6%8E%A2/" title="Transformer模型初探">Transformer模型初探</a><time datetime="2022-12-14T01:42:13.000Z" title="Created 2022-12-14 09:42:13">2022-12-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/14/1-4/" title="No title"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="No title"/></a><div class="content"><a class="title" href="/2022/12/14/1-4/" title="No title">No title</a><time datetime="2022-12-14T01:42:13.000Z" title="Created 2022-12-14 09:42:13">2022-12-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/14/Transformer%20Tutorial%20for%20Beginners%20A%20Comprehensive%20Guide/" title="No title"><img src="https://github.com/serika-onoe/web-img/raw/main/Transformer/attention.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="No title"/></a><div class="content"><a class="title" href="/2022/12/14/Transformer%20Tutorial%20for%20Beginners%20A%20Comprehensive%20Guide/" title="No title">No title</a><time datetime="2022-12-14T01:42:13.000Z" title="Created 2022-12-14 09:42:13">2022-12-14</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/12/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/" title="强化学习初探"><img src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="强化学习初探"/></a><div class="content"><a class="title" href="/2022/12/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/" title="强化学习初探">强化学习初探</a><time datetime="2022-12-12T15:17:01.000Z" title="Created 2022-12-12 23:17:01">2022-12-12</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Richard KUNG</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr/><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/algoliasearch/dist/algoliasearch-lite.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js/dist/instantsearch.production.min.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://rbmbrain.me/',
      region: 'ap-east-1',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script defer src="https://cdn.jsdelivr.net/gh/CodeByZach/pace/pace.min.js"></script><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>